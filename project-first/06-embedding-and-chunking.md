# ç¬¬å…­ç« ï¼šåµŒå…¥å‘é‡èˆ‡åˆ†å¡ŠåŸç† â€” RAG çš„ã€Œç·¨è­¯å™¨æ ¸å¿ƒã€

## å­¸ç¿’ç›®æ¨™

è®€å®Œæœ¬ç« ï¼Œä½ å°‡èƒ½å¤ ï¼š
- è§£é‡‹åµŒå…¥å‘é‡ï¼ˆEmbeddingï¼‰çš„ç›´è¦ºæ¦‚å¿µå’Œæ•¸å­¸æ„æ¶µ
- ç†è§£ MÃ—N â†’ M+N çš„æ¶æ§‹å„ªå‹¢å¦‚ä½•å°æ‡‰åˆ° RAG çš„è§£è€¦è¨­è¨ˆ
- èªªæ˜å››ç¨® Chunking ç­–ç•¥çš„é©ç”¨å ´æ™¯å’Œå„ªç¼ºé»
- ç†è§£ç‚ºä»€éº¼ Chunking å’Œ Embedding ç­–ç•¥æ˜¯é‡è¦çš„æ²»ç†æ±ºç­–

---

## 6.1 åµŒå…¥å‘é‡ï¼šè®“æ–‡å­—æœ‰ã€Œè·é›¢ã€

### å¾é—œéµå­—æœå°‹åˆ°èªæ„æœå°‹

å‚³çµ±é—œéµå­—æœå°‹çš„å•é¡Œï¼š

```
æŸ¥è©¢ï¼šã€Œå“¡å·¥è«‹å‡è¦å®šã€
æ‰¾åˆ°ï¼šåŒ…å«ã€Œè«‹å‡ã€ã€Œè¦å®šã€çš„æ–‡ä»¶ âœ“
æ‰¾ä¸åˆ°ï¼šå«æœ‰ã€Œä¼‘å‡æ”¿ç­–ã€ã€Œç¼ºå‹¤ç®¡ç†ã€çš„æ–‡ä»¶ âœ—ï¼ˆèªæ„ç›¸åŒï¼Œè©å½™ä¸åŒï¼‰
```

**åµŒå…¥å‘é‡ï¼ˆEmbeddingï¼‰** çš„è§£æ³•ï¼š  
æŠŠæ–‡å­—è½‰æ›æˆé«˜ç¶­å‘é‡ç©ºé–“ä¸­çš„åº§æ¨™ï¼Œ  
èªæ„ç›¸è¿‘çš„æ–‡å­—åœ¨ç©ºé–“ä¸­çš„è·é›¢ä¹Ÿè¿‘ã€‚

```python
# åµŒå…¥å‘é‡çš„ç›´è¦ºç¤ºæ„ï¼ˆå¯¦éš›æ˜¯ 1536 ç¶­å‘é‡ï¼‰

"å“¡å·¥è«‹å‡è¦å®š" â†’ [0.23, -0.71, 0.45, ...]  # 1536 ç¶­å‘é‡
"ä¼‘å‡æ”¿ç­–èªªæ˜" â†’ [0.25, -0.68, 0.43, ...]  # èªæ„ç›¸è¿‘ â†’ å‘é‡ç›¸è¿‘
"è²¡å‹™å ±å‘Š Q4"  â†’ [-0.82, 0.12, -0.55, ...] # èªæ„ä¸åŒ â†’ å‘é‡ç›¸é 
```

### èª¿ç”¨ OpenAI Embedding API

```python
# æª”æ¡ˆï¼šsrc/ingestion/embedder.py

import openai
from tenacity import retry, stop_after_attempt, wait_exponential

class OpenAIEmbedder:
    """
    ä½¿ç”¨ text-embedding-3-large å°‡æ–‡å­—è½‰æ›æˆå‘é‡ã€‚
    ç”± ADR-001 æ±ºå®šä½¿ç”¨æ­¤æ¨¡å‹ã€‚
    """

    MODEL = "text-embedding-3-large"  # Constitution INV-6ï¼šä¸å¯åœ¨ code ä¸­ç¡¬æ”¹

    def __init__(self):
        self.client = openai.OpenAI()

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(min=1, max=10),
    )
    def embed(self, text: str) -> list[float]:
        """
        å°‡å–®ä¸€æ–‡å­—åµŒå…¥ç‚ºå‘é‡ã€‚

        Precondition: len(text.strip()) > 0ï¼ˆä¸åµŒå…¥ç©ºæ–‡å­—ï¼‰
        Postcondition: len(result) == 1536ï¼ˆtext-embedding-3-large çš„ç¶­åº¦ï¼‰
        """
        if not text.strip():
            raise ValueError("ä¸å¾—åµŒå…¥ç©ºæ–‡å­—ï¼ˆé•å INV-1ï¼‰")

        response = self.client.embeddings.create(
            model=self.MODEL,
            input=text,
        )
        vector = response.data[0].embedding
        assert len(vector) == 1536, f"é æœŸ 1536 ç¶­ï¼Œå¾—åˆ° {len(vector)} ç¶­"
        return vector

    def embed_batch(self, texts: list[str]) -> list[list[float]]:
        """
        æ‰¹æ¬¡åµŒå…¥ï¼ˆæœ€å¤š 100 å€‹ï¼‰ã€‚
        ä½¿ç”¨æ‰¹æ¬¡ API é™ä½å‘¼å«æ¬¡æ•¸ï¼Œç¯€çœ token ç”¨é‡ã€‚
        """
        # éæ¿¾ç©ºç™½æ–‡å­—
        valid_texts = [t for t in texts if t.strip()]
        if len(valid_texts) != len(texts):
            raise ValueError(f"æ‰¹æ¬¡ä¸­æœ‰ {len(texts) - len(valid_texts)} å€‹ç©ºç™½æ–‡å­—")

        if len(texts) > 100:
            raise ValueError("å–®æ¬¡æ‰¹æ¬¡æœ€å¤š 100 å€‹æ–‡å­—ï¼ˆAPI é™åˆ¶ï¼‰")

        response = self.client.embeddings.create(
            model=self.MODEL,
            input=valid_texts,
        )
        return [item.embedding for item in response.data]
```

---

## 6.2 MÃ—N â†’ M+Nï¼šRAG çš„è§£è€¦æ¶æ§‹å„ªå‹¢

### ç‚ºä»€éº¼è¦æŠŠã€ŒåµŒå…¥ã€å’Œã€Œç”Ÿæˆã€åˆ†é–‹

é€™å€‹å•é¡Œå’Œç·¨è­¯å™¨ç‚ºä»€éº¼éœ€è¦ IRï¼ˆä¸­é–“è¡¨ç¤ºï¼‰çš„é“ç†å®Œå…¨ç›¸åŒï¼š

**æ²’æœ‰ Embedding Layerï¼ˆç›´æ¥ fine-tuneï¼‰æ™‚ï¼š**
æ¯ç¨®çŸ¥è­˜ä¾†æº Ã— æ¯å€‹ LLM = M Ã— N å€‹ fine-tune å·¥ç¨‹

```
HR æ–‡ä»¶     Ã— GPT-4o    = fine-tune 1
HR æ–‡ä»¶     Ã— Claude    = fine-tune 2
Legal æ–‡ä»¶  Ã— GPT-4o    = fine-tune 3
Legal æ–‡ä»¶  Ã— Claude    = fine-tune 4
...
5 ç¨®ä¾†æº Ã— 4 å€‹ LLM = 20 å€‹ fine-tuneï¼ˆæ˜‚è²´ä¸”é›£ä»¥ç¶­è­·ï¼‰
```

**æœ‰ Embedding Layerï¼ˆRAG æ¶æ§‹ï¼‰æ™‚ï¼š**
åªéœ€è¦ M å€‹åµŒå…¥ç®¡ç·š + N å€‹ LLM æ¥å£

```
HR æ–‡ä»¶    â†’ [Embedding] â†’ Vector DB
Legal æ–‡ä»¶ â†’ [Embedding] â†’ Vector DB
...                              â†“
                          èªæ„æœå°‹
                              â†“
                          GPT-4o / Claude / Geminiï¼ˆå¯åˆ‡æ›ï¼‰
5 ç¨®ä¾†æº + 4 å€‹ LLM = 9 å€‹æ¨¡çµ„ï¼ˆä¸”å¯ç¨ç«‹æ›¿æ›ï¼‰
```

> **æ ¸å¿ƒæ´è¦‹**ï¼šVector DB æ˜¯ RAG çš„ã€Œä¸­é–“è¡¨ç¤ºå±¤ï¼ˆIRï¼‰ã€ã€‚  
> å®ƒè®“ã€Œç†è§£æ–‡ä»¶ï¼ˆåµŒå…¥ï¼‰ã€å’Œã€Œç”Ÿæˆç­”æ¡ˆï¼ˆLLMï¼‰ã€å„è‡ªç¨ç«‹æ¼”åŒ–ã€‚  
> æ›´æ› LLM ä¸éœ€è¦é‡æ–°åµŒå…¥ï¼Œæ›´æ›åµŒå…¥æ¨¡å‹ä¸éœ€è¦æ”¹ LLM çš„å‘¼å«æ–¹å¼ã€‚

---

## 6.3 å››ç¨® Chunking ç­–ç•¥

Chunkingï¼ˆæ–‡ä»¶åˆ†å¡Šï¼‰æ˜¯ RAG çš„æ ¸å¿ƒå·¥ç¨‹å•é¡Œï¼š  
å¡Šå¤ªå¤§ â†’ LLM context çˆ†æ»¿ï¼Œç„¡æ³•æ¯”è¼ƒå¤šä»½æ–‡ä»¶ï¼›  
å¡Šå¤ªå° â†’ èªæ„æ–·è£‚ï¼Œç„¡æ³•å›ç­”éœ€è¦ä¸Šä¸‹æ–‡çš„å•é¡Œã€‚

### ç­–ç•¥ä¸€ï¼šå›ºå®šå¤§å°åˆ†å¡Šï¼ˆFixed-size Chunkingï¼‰

```python
def fixed_size_chunk(text: str, size: int = 512, overlap: int = 64) -> list[str]:
    """
    æœ€ç°¡å–®çš„åˆ†å¡Šç­–ç•¥ï¼šæ¯éš” size å€‹ token åˆ‡ä¸€åˆ€ï¼Œé‡ç–Š overlap å€‹ tokenã€‚
    
    âœ… å„ªé»ï¼šç°¡å–®ã€å¿«é€Ÿã€å¯é æ¸¬
    âŒ ç¼ºé»ï¼šå¸¸åœ¨å¥å­æˆ–æ®µè½ä¸­é–“åˆ‡æ–·ï¼Œèªæ„ç ´ç¢
    é©ç”¨ï¼šå¿«é€ŸåŸå‹ã€çµæ§‹éå¸¸è¦æ•´çš„æ–‡ä»¶ï¼ˆå¦‚ CSV è½‰æ–‡å­—ï¼‰
    """
    tokens = tokenize(text)
    chunks = []
    start = 0
    while start < len(tokens):
        end = min(start + size, len(tokens))
        chunk = detokenize(tokens[start:end])
        chunks.append(chunk)
        start += size - overlap
    return chunks
```

### ç­–ç•¥äºŒï¼šéè¿´åˆ†å¡Šï¼ˆRecursive Chunkingï¼‰â€” æœ¬å°ˆæ¡ˆæ¡ç”¨

```python
# æª”æ¡ˆï¼šsrc/ingestion/chunker.py

class RecursiveChunker:
    """
    éè¿´åˆ†å¡Šç­–ç•¥ï¼ˆADR-002 æ±ºå®šæ¡ç”¨æ­¤ç­–ç•¥ï¼‰ã€‚

    å„ªå…ˆåœ¨è‡ªç„¶é‚Šç•Œåˆ‡åˆ†ï¼Œä¾åºå˜—è©¦ï¼š
    æ®µè½ï¼ˆ\n\nï¼‰â†’ å¥å­ï¼ˆã€‚ï¼ï¼Ÿï¼‰â†’ è©ï¼ˆï¼Œï¼‰â†’ å­—å…ƒ

    âœ… å„ªé»ï¼šå°Šé‡æ–‡ä»¶çš„è‡ªç„¶çµæ§‹ï¼Œèªæ„å®Œæ•´
    âœ… å„ªé»ï¼šé€Ÿåº¦å¿«ï¼ˆæ¯”èªæ„åˆ†å¡Šå¿« 3 å€ï¼‰
    âŒ ç¼ºé»ï¼šä¸åŒæ–‡ä»¶çš„ chunk å¤§å°ä¸å‡
    é©ç”¨ï¼šå¤§å¤šæ•¸ä¼æ¥­æ–‡ä»¶ï¼ˆWordã€PDFã€Markdownï¼‰
    """

    SEPARATORS = ["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", " ", ""]

    def __init__(self, target_size: int = 600, overlap: int = 100):
        self.target_size = target_size
        self.overlap = overlap

    def split(self, text: str, metadata: dict = None) -> list["Chunk"]:
        chunks = self._recursive_split(text, self.SEPARATORS)

        result = []
        for i, chunk_text in enumerate(chunks):
            chunk_metadata = {
                **(metadata or {}),
                "chunk_index": i,
                "total_chunks": len(chunks),
            }
            result.append(Chunk(text=chunk_text, metadata=chunk_metadata))

        return result

    def _recursive_split(self, text: str, separators: list[str]) -> list[str]:
        """éè¿´åœ°ç”¨åˆ†éš”ç¬¦åˆ‡åˆ†ï¼Œç›´åˆ°æ‰€æœ‰å¡Šéƒ½ç¬¦åˆç›®æ¨™å¤§å°"""
        if not separators:
            # æœ€å¾Œæ‰‹æ®µï¼šç›´æ¥æŒ‰å­—å…ƒåˆ‡
            return [text[i:i+self.target_size]
                    for i in range(0, len(text), self.target_size - self.overlap)]

        separator = separators[0]
        splits = text.split(separator) if separator else list(text)

        chunks = []
        current = ""
        for split in splits:
            if len(tokenize(current + separator + split)) <= self.target_size:
                current += separator + split if current else split
            else:
                if current:
                    chunks.append(current)
                # å¦‚æœå–®å€‹ split æœ¬èº«è¶…é target_sizeï¼Œéè¿´è™•ç†
                if len(tokenize(split)) > self.target_size:
                    chunks.extend(self._recursive_split(split, separators[1:]))
                    current = ""
                else:
                    current = split

        if current:
            chunks.append(current)

        return self._add_overlap(chunks)

    def _add_overlap(self, chunks: list[str]) -> list[str]:
        """åœ¨ç›¸é„° chunks ä¹‹é–“åŠ å…¥é‡ç–Šï¼Œä¿ç•™è·¨å¡Šä¸Šä¸‹æ–‡"""
        if len(chunks) <= 1:
            return chunks

        result = [chunks[0]]
        for i in range(1, len(chunks)):
            # æŠŠå‰ä¸€å€‹ chunk çš„æœ€å¾Œ overlap å€‹ token åŠ åˆ°ç•¶å‰ chunk çš„é–‹é ­
            prev_tail = get_last_n_tokens(chunks[i-1], self.overlap)
            result.append(prev_tail + " " + chunks[i])
        return result
```

### ç­–ç•¥ä¸‰ï¼šèªæ„åˆ†å¡Šï¼ˆSemantic Chunkingï¼‰

```python
# æ¦‚å¿µç¤ºä¾‹ï¼šä½¿ç”¨ NLP æ¨¡å‹åµæ¸¬æ®µè½é‚Šç•Œ

class SemanticChunker:
    """
    èªæ„æ„ŸçŸ¥çš„åˆ†å¡Šç­–ç•¥ã€‚
    
    ä½¿ç”¨åµŒå…¥å‘é‡æ¯”è¼ƒç›¸é„°å¥å­çš„èªæ„ç›¸ä¼¼åº¦ï¼Œ
    ç•¶ç›¸ä¼¼åº¦çªç„¶ä¸‹é™æ™‚ï¼Œè¦–ç‚ºæ®µè½é‚Šç•Œä¸¦åˆ‡åˆ†ã€‚
    
    âœ… å„ªé»ï¼šæœ€å¥½çš„èªæ„å®Œæ•´æ€§
    âŒ ç¼ºé»ï¼šè™•ç†é€Ÿåº¦æ…¢ï¼ˆæ¯å¥éƒ½è¦å‘¼å« Embedding APIï¼‰
    âŒ ç¼ºé»ï¼šæˆæœ¬é«˜ï¼ˆå‘¼å« Embedding API æ¬¡æ•¸å¤šï¼‰
    é©ç”¨ï¼šé«˜åƒ¹å€¼æ–‡ä»¶ï¼ˆæ³•å¾‹åˆç´„ã€å­¸è¡“è«–æ–‡ï¼‰
    """

    def split(self, text: str, threshold: float = 0.5) -> list[str]:
        sentences = self._split_to_sentences(text)
        sentence_vectors = self.embedder.embed_batch(sentences)

        # è¨ˆç®—ç›¸é„°å¥å­çš„é¤˜å¼¦ç›¸ä¼¼åº¦
        similarities = []
        for i in range(len(sentence_vectors) - 1):
            sim = cosine_similarity(sentence_vectors[i], sentence_vectors[i+1])
            similarities.append(sim)

        # ç›¸ä¼¼åº¦ < threshold è™•è¦–ç‚ºæ®µè½é‚Šç•Œ
        breakpoints = [i+1 for i, sim in enumerate(similarities) if sim < threshold]

        return self._merge_by_breakpoints(sentences, breakpoints)
```

### ç­–ç•¥å››ï¼šæ–‡ä»¶çµæ§‹æ„ŸçŸ¥åˆ†å¡Šï¼ˆDocument-Aware Chunkingï¼‰

```python
class DocumentAwareChunker:
    """
    åˆ©ç”¨æ–‡ä»¶çµæ§‹ï¼ˆæ¨™é¡Œã€ç« ç¯€ï¼‰çš„åˆ†å¡Šç­–ç•¥ã€‚
    
    è§£æ Markdown / Word çš„æ¨™é¡Œå±¤ç´šï¼Œ
    è®“æ¯å€‹ chunk å®Œæ•´åŒ…å«ä¸€å€‹é‚è¼¯ç« ç¯€ã€‚
    
    âœ… å„ªé»ï¼šä¿æŒæ–‡ä»¶çš„é‚è¼¯å®Œæ•´æ€§
    âœ… å„ªé»ï¼šchunk å¤©ç„¶å…·æœ‰èªæ„æ¨™é¡Œï¼Œå¯ä½œç‚º metadata
    âŒ ç¼ºé»ï¼šä¾è³´æ–‡ä»¶æœ‰è‰¯å¥½çš„æ¨™é¡Œçµæ§‹
    é©ç”¨ï¼šæŠ€è¡“æ–‡ä»¶ã€è¦æ ¼æ›¸ã€æœ‰ç›®éŒ„çš„ Word æ–‡ä»¶
    """

    def split_markdown(self, text: str) -> list["Chunk"]:
        """æŒ‰ Markdown æ¨™é¡Œåˆ‡åˆ†"""
        sections = re.split(r'\n(#{1,3} .+)\n', text)
        chunks = []
        current_title = "ï¼ˆç„¡æ¨™é¡Œï¼‰"
        current_content = ""

        for i, section in enumerate(sections):
            if section.startswith('#'):
                # å„²å­˜ä¸Šä¸€å€‹ç« ç¯€
                if current_content.strip():
                    chunks.append(Chunk(
                        text=f"{current_title}\n\n{current_content}",
                        metadata={"section_title": current_title}
                    ))
                current_title = section.strip()
                current_content = ""
            else:
                current_content += section

        # æœ€å¾Œä¸€å€‹ç« ç¯€
        if current_content.strip():
            chunks.append(Chunk(
                text=f"{current_title}\n\n{current_content}",
                metadata={"section_title": current_title}
            ))

        return chunks
```

### å››ç¨®ç­–ç•¥çš„é¸å‹æŒ‡å—

| ç­–ç•¥ | é€Ÿåº¦ | èªæ„å“è³ª | æˆæœ¬ | é©ç”¨å ´æ™¯ |
|------|:----:|:-------:|:----:|---------|
| å›ºå®šå¤§å° | â­â­â­â­â­ | â­â­ | â­ | å¿«é€ŸåŸå‹ã€çµæ§‹ç°¡å–®çš„æ–‡ä»¶ |
| éè¿´åˆ†å¡Š | â­â­â­â­ | â­â­â­â­ | â­â­ | **å¤§å¤šæ•¸ä¼æ¥­æ–‡ä»¶ï¼ˆæ¨è–¦ï¼‰** |
| èªæ„åˆ†å¡Š | â­â­ | â­â­â­â­â­ | â­â­â­â­ | é«˜åƒ¹å€¼æ³•å¾‹/å­¸è¡“æ–‡ä»¶ |
| æ–‡ä»¶çµæ§‹ | â­â­â­ | â­â­â­â­â­ | â­â­ | æœ‰è‰¯å¥½çµæ§‹çš„æŠ€è¡“æ–‡ä»¶ |

---

## 6.4 ç‚ºä»€éº¼ Chunking æ˜¯æ²»ç†å•é¡Œ

Chunking ç­–ç•¥çœ‹èµ·ä¾†æ˜¯ç´”æŠ€è¡“å•é¡Œï¼Œç‚ºä»€éº¼è¦ç´å…¥æ²»ç†ï¼Ÿ

å› ç‚º **Chunking ç­–ç•¥ä¸€æ—¦é¸å®šï¼Œæ›´æ›ä»£åƒ¹æ¥µé«˜**ï¼š

```
ç•¶å‰ç‹€æ…‹ï¼š5,000 ä»½æ–‡ä»¶å·²ç”¨éè¿´åˆ†å¡Šç­–ç•¥åµŒå…¥ï¼Œå­˜å…¥å‘é‡ DB

è‹¥è¦æ›æˆèªæ„åˆ†å¡Šç­–ç•¥ï¼š
1. æ¸…ç©ºå‘é‡ DB çš„æ‰€æœ‰ chunk
2. ç”¨æ–°ç­–ç•¥é‡æ–°åˆ†å¡Š 5,000 ä»½æ–‡ä»¶
3. é‡æ–°åµŒå…¥æ‰€æœ‰ chunkï¼ˆAPI æˆæœ¬ + æ™‚é–“ï¼‰
4. é©—è­‰æ–°ç­–ç•¥çš„ Retrieval å“è³ªä¸æ¯”èˆŠç­–ç•¥å·®
5. è™•ç† re-embed æœŸé–“çš„æœå‹™ä¸­æ–·
```

é€™å°±æ˜¯ç‚ºä»€éº¼ ADR-002ï¼ˆChunking ç­–ç•¥ï¼‰æ˜¯æ°¸ä¹…è¨˜éŒ„ï¼Œéœ€è¦å……åˆ†è«–è­‰æ‰èƒ½æ›´æ”¹ã€‚

> ğŸ”‘ **æ²»ç†é‡é»**ï¼šå‡¡æ˜¯ã€Œæ›´æ›ä»£åƒ¹ > 1 å¤©å·¥ç¨‹å·¥ä½œé‡ã€çš„æ±ºç­–ï¼Œéƒ½éœ€è¦ ADRã€‚

---

## ç·´ç¿’

1. **è¨ˆç®—ç·´ç¿’**ï¼šä¸€ä»½ 10,000 å­—çš„ Word æ–‡ä»¶ï¼Œä½¿ç”¨ `target_size=600, overlap=100` çš„éè¿´åˆ†å¡Šç­–ç•¥ï¼Œé ä¼°æœƒç”¢ç”Ÿå¹¾å€‹ chunksï¼Ÿï¼ˆå‡è¨­ä¸­æ–‡æ¯å€‹å­—ç´„ 1 tokenï¼‰

2. **ç­–ç•¥é¸æ“‡ç·´ç¿’**ï¼šç‚ºä»¥ä¸‹ä¸‰ç¨®æ–‡ä»¶é¸æ“‡æœ€é©åˆçš„ Chunking ç­–ç•¥ï¼Œä¸¦èªªæ˜ç†ç”±ï¼š
   - (a) å…¬å¸çš„æ¨™æº–åˆç´„ç¯„æœ¬ï¼ˆ30 é  Wordï¼Œæœ‰æ¸…æ¥šçš„ç« ç¯€çµæ§‹ï¼‰
   - (b) å®¢æœå·¥å–®ç³»çµ±çš„æ­·å²è¨˜éŒ„ï¼ˆCSV æ ¼å¼ï¼Œæ¯è¡Œä¸€ç­†å°è©±ï¼‰
   - (c) æŠ€è¡“éƒ¨è½æ ¼æ–‡ç« ï¼ˆMarkdown æ ¼å¼ï¼Œæœ‰ç¨‹å¼ç¢¼å€å¡Šï¼‰

3. **å¯¦ä½œç·´ç¿’**ï¼šå¯«ä¸€å€‹å‡½å¼ `evaluate_chunking_strategy(text, chunker, test_questions, retriever)`ï¼Œè¼¸å…¥ä¸€æ®µæ–‡å­—ã€åˆ†å¡Šå™¨ã€æ¸¬è©¦å•é¡Œåˆ—è¡¨å’Œæª¢ç´¢å™¨ï¼Œè¼¸å‡º Hit@3 çš„æº–ç¢ºç‡ã€‚

4. **æ€è€ƒé¡Œ**ï¼šå¦‚æœä¸€ä»½æ–‡ä»¶è¢«åˆ†æˆ 50 å€‹ chunksï¼Œä½†ä½¿ç”¨è€…çš„å•é¡Œéœ€è¦åŒæ™‚ç†è§£ç¬¬ 3 å¡Šå’Œç¬¬ 48 å¡Šçš„å…§å®¹æ‰èƒ½å›ç­”ï¼Œå‚³çµ±çš„ RAG æœƒæ€éº¼å¤±æ•—ï¼Ÿä½ æœ‰ä»€éº¼è§£æ±ºæƒ³æ³•ï¼Ÿ

---

> **ä¸‹ä¸€ç« **ï¼š[ç¬¬ä¸ƒç« ï¼šä¸å¯è®ŠçŸ¥è­˜ç®¡ç†èˆ‡ç‰ˆæœ¬æ§åˆ¶](07-immutable-knowledge.md)  
> æˆ‘å€‘å°‡å­¸ç¿’å¦‚ä½•è¨­è¨ˆçŸ¥è­˜åº«çš„ã€Œä¸å¯è®ŠåŸå‰‡ã€ï¼Œç¢ºä¿æ¯ä¸€æ¬¡çŸ¥è­˜æ›´æ–°éƒ½å¯è¿½æº¯ã€å¯å›æ»¾ã€‚
