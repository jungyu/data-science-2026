# Chapter 4: ç·šæ€§å›æ­¸ - ç”¨ä¸€æ¢ç·šé æ¸¬æœªä¾†

> ã€Œé æ¸¬ï¼Œæ˜¯äººé¡æœ€å¤è€çš„æ…¾æœ›ä¹‹ä¸€ã€‚è€Œç·šæ€§å›æ­¸ï¼Œå°±æ˜¯æœ€ç°¡å–®çš„é æ¸¬æ©Ÿå™¨ã€‚ã€

---

## ğŸ¯ æœ¬ç« ç›®æ¨™

è®€å®Œé€™ä¸€ç« ï¼Œä½ å°‡èƒ½å¤ ï¼š

1. ç†è§£ç·šæ€§å›æ­¸çš„æ•¸å­¸ç›´è¦ºï¼ˆä¸éœ€è¦æ¨å°å…¬å¼ï¼ï¼‰
2. ç”¨ scikit-learn çš„ `LinearRegression` å»ºç«‹å›æ­¸æ¨¡å‹
3. ç†è§£ MSEã€MAEã€RÂ² ç­‰è©•ä¼°æŒ‡æ¨™çš„æ„ç¾©èˆ‡é™·é˜±
4. ç•«å‡ºæ®˜å·®åœ–ï¼ˆResidual Plotï¼‰ä¸¦åˆ¤æ–·æ¨¡å‹å¥½å£
5. é€éæˆ¿åƒ¹é æ¸¬æ¡ˆä¾‹ï¼Œå®Œæ•´èµ°éå›æ­¸å»ºæ¨¡æµç¨‹
6. èªè­˜å›æ­¸æƒ…å¢ƒä¸‹çš„éæ“¬åˆå•é¡Œ

---

## å¾ç”Ÿæ´»é–‹å§‹ï¼šä½ å·²ç¶“æœƒã€Œå›æ­¸ã€äº†

æƒ³åƒä½ æ˜¯ä¸€å€‹æˆ¿ä»²ï¼Œçœ‹éä¸Šç™¾é–“æˆ¿å­ä¹‹å¾Œï¼Œä½ å¿ƒè£¡å¤§æ¦‚æœ‰å€‹æ„Ÿè¦ºï¼š

> ã€Œåªæ•¸è¶Šå¤§ï¼Œæˆ¿åƒ¹è¶Šé«˜ã€‚å¤§æ¦‚æ¯å¤šä¸€åªï¼Œå¤šå€‹ 5 è¬å·¦å³ã€‚ã€

æ­å–œä½ ï¼Œä½ å‰›å‰›åœ¨è…¦è¢‹è£¡åšäº†ä¸€æ¬¡ç·šæ€§å›æ­¸ã€‚

ç·šæ€§å›æ­¸åšçš„äº‹æƒ…ï¼Œå°±æ˜¯æŠŠä½ çš„ã€Œç›´è¦ºã€è®Šæˆä¸€æ¢ç²¾ç¢ºçš„ç·šï¼š

```
æˆ¿åƒ¹ = w Ã— åªæ•¸ + b
```

å…¶ä¸­ï¼š
- `w`ï¼ˆweightï¼Œæ¬Šé‡ï¼‰= æ¯åªå¤šå°‘éŒ¢ï¼ˆæ–œç‡ï¼‰
- `b`ï¼ˆbiasï¼Œåå·®ï¼‰= åŸºæœ¬æˆ¿åƒ¹ï¼ˆæˆªè·ï¼‰

```
æˆ¿åƒ¹ï¼ˆè¬ï¼‰
  ^
  |                          *
  |                      *
  |                  * /
  |              * / â† é€™æ¢ç·šå°±æ˜¯å›æ­¸æ¨¡å‹
  |          * /
  |      * /
  |  * /
  | /
  +----------------------------> åªæ•¸
```

---

## ğŸ’¡ é‡é»è§€å¿µï¼šå›æ­¸çš„æœ¬è³ª

ç·šæ€§å›æ­¸çš„ç›®æ¨™åªæœ‰ä¸€å€‹ï¼š

> **æ‰¾åˆ°ä¸€æ¢ç·šï¼Œè®“æ‰€æœ‰è³‡æ–™é»åˆ°é€™æ¢ç·šçš„è·é›¢ç¸½å’Œæœ€å°ã€‚**

é€™å€‹ã€Œè·é›¢ã€ï¼Œå°±æ˜¯æ‰€è¬‚çš„ã€Œæ®˜å·®ã€ï¼ˆResidualï¼‰ï¼š

```
æ®˜å·® = å¯¦éš›å€¼ - é æ¸¬å€¼

       å¯¦éš›å€¼ *
               |  â† æ®˜å·®ï¼ˆå‚ç›´è·é›¢ï¼‰
               |
       é æ¸¬å€¼ â”€â—â”€â”€ å›æ­¸ç·šä¸Šçš„é»
```

scikit-learn ç”¨çš„æ–¹æ³•å«åš **æœ€å°å¹³æ–¹æ³•ï¼ˆOrdinary Least Squares, OLSï¼‰**ï¼Œ
å®ƒæœ€å°åŒ–çš„æ˜¯æ‰€æœ‰æ®˜å·®çš„å¹³æ–¹å’Œï¼š

```
ç›®æ¨™ï¼šæœ€å°åŒ– Î£ (å¯¦éš›å€¼áµ¢ - é æ¸¬å€¼áµ¢)Â²
```

ç‚ºä»€éº¼ç”¨å¹³æ–¹ï¼Ÿ
- é¿å…æ­£è² æ®˜å·®äº’ç›¸æŠµæ¶ˆ
- æ•¸å­¸ä¸Šæ¯”è¼ƒå¥½å¾®åˆ†ï¼ˆä½†ä½ ä¸éœ€è¦è¨˜é€™å€‹ï¼‰
- æœƒç‰¹åˆ¥æ‡²ç½°å¤§çš„èª¤å·®

---

## å‹•æ‰‹åšï¼šæˆ¿åƒ¹é æ¸¬æ¡ˆä¾‹

### ç¬¬ä¸€æ­¥ï¼šæº–å‚™è³‡æ–™

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# æ¨¡æ“¬æˆ¿åƒ¹è³‡æ–™
np.random.seed(42)
n = 200

# ç‰¹å¾µ
area = np.random.uniform(10, 60, n)         # åªæ•¸
rooms = np.random.randint(1, 5, n)          # æˆ¿é–“æ•¸
age = np.random.uniform(0, 40, n)           # å±‹é½¡ï¼ˆå¹´ï¼‰

# çœŸå¯¦æˆ¿åƒ¹ = 5*åªæ•¸ + 50*æˆ¿é–“æ•¸ - 3*å±‹é½¡ + 200 + é›œè¨Š
price = 5 * area + 50 * rooms - 3 * age + 200 + np.random.normal(0, 30, n)

df = pd.DataFrame({
    'åªæ•¸': area,
    'æˆ¿é–“æ•¸': rooms,
    'å±‹é½¡': age,
    'æˆ¿åƒ¹_è¬': price
})

print(df.head())
print(f"\nè³‡æ–™ç­†æ•¸ï¼š{len(df)}")
print(f"æˆ¿åƒ¹ç¯„åœï¼š{df['æˆ¿åƒ¹_è¬'].min():.0f} ~ {df['æˆ¿åƒ¹_è¬'].max():.0f} è¬")
```

### ç¬¬äºŒæ­¥ï¼šåˆ‡åˆ†è¨“ç·´é›†èˆ‡æ¸¬è©¦é›†

```python
X = df[['åªæ•¸', 'æˆ¿é–“æ•¸', 'å±‹é½¡']]
y = df['æˆ¿åƒ¹_è¬']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(f"è¨“ç·´é›†ï¼š{len(X_train)} ç­†")
print(f"æ¸¬è©¦é›†ï¼š{len(X_test)} ç­†")
```

### ç¬¬ä¸‰æ­¥ï¼šè¨“ç·´æ¨¡å‹

```python
model = LinearRegression()
model.fit(X_train, y_train)

# çœ‹çœ‹æ¨¡å‹å­¸åˆ°äº†ä»€éº¼
print("æ¨¡å‹ä¿‚æ•¸ï¼ˆweightsï¼‰ï¼š")
for name, coef in zip(X.columns, model.coef_):
    print(f"  {name}: {coef:.2f}")
print(f"æˆªè·ï¼ˆbiasï¼‰ï¼š{model.intercept_:.2f}")
```

è¼¸å‡ºå¯èƒ½åƒé€™æ¨£ï¼š

```
æ¨¡å‹ä¿‚æ•¸ï¼ˆweightsï¼‰ï¼š
  åªæ•¸: 4.98        â† æ¥è¿‘çœŸå¯¦å€¼ 5
  æˆ¿é–“æ•¸: 49.12     â† æ¥è¿‘çœŸå¯¦å€¼ 50
  å±‹é½¡: -2.95       â† æ¥è¿‘çœŸå¯¦å€¼ -3
æˆªè·ï¼ˆbiasï¼‰ï¼š201.34  â† æ¥è¿‘çœŸå¯¦å€¼ 200
```

æ¨¡å‹å¹¾ä¹å®Œç¾åœ°ã€Œå­¸æœƒã€äº†æˆ‘å€‘è¨­å®šçš„è¦å‰‡ï¼

---

## ğŸ’¡ é‡é»è§€å¿µï¼šæ¨¡å‹è©•ä¼°ä¸‰åŠå®¢

### 1. MSEï¼ˆå‡æ–¹èª¤å·®ï¼ŒMean Squared Errorï¼‰

```
MSE = (1/n) Ã— Î£ (å¯¦éš›å€¼áµ¢ - é æ¸¬å€¼áµ¢)Â²
```

```python
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f"MSE: {mse:.2f}")
```

- å–®ä½æ˜¯ã€ŒåŸå§‹å–®ä½çš„å¹³æ–¹ã€ï¼ˆä¾‹å¦‚ï¼šè¬Â²ï¼‰
- æ‰€ä»¥å¸¸ç”¨ RMSE = âˆšMSE è®“å–®ä½å›åˆ°åŸæœ¬çš„å°ºåº¦

```python
rmse = np.sqrt(mse)
print(f"RMSE: {rmse:.2f} è¬")
# æ„æ€æ˜¯ã€Œå¹³å‡é æ¸¬èª¤å·®å¤§ç´„ Â±{rmse:.0f} è¬ã€
```

### 2. MAEï¼ˆå¹³å‡çµ•å°èª¤å·®ï¼ŒMean Absolute Errorï¼‰

```python
from sklearn.metrics import mean_absolute_error
mae = mean_absolute_error(y_test, y_pred)
print(f"MAE: {mae:.2f} è¬")
```

- MAE æ¯” MSE æ›´ç›´è¦ºï¼šã€Œå¹³å‡æ¯æ¬¡é æ¸¬å·®å¤šå°‘ã€
- ä½† MAE å°æ¥µç«¯å€¼çš„æ‡²ç½°è¼ƒå°

### 3. RÂ²ï¼ˆæ±ºå®šä¿‚æ•¸ï¼ŒR-squaredï¼‰

```python
r2 = r2_score(y_test, y_pred)
print(f"RÂ²: {r2:.4f}")
```

- RÂ² è¡¨ç¤ºã€Œæ¨¡å‹è§£é‡‹äº†å¤šå°‘æ¯”ä¾‹çš„è®Šç•°ã€
- RÂ² = 1.0 è¡¨ç¤ºå®Œç¾é æ¸¬
- RÂ² = 0.0 è¡¨ç¤ºè·Ÿç›´æ¥çŒœå¹³å‡å€¼ä¸€æ¨£
- RÂ² < 0 æ˜¯æœ‰å¯èƒ½çš„ï¼è¡¨ç¤ºæ¨¡å‹æ¯”çŒœå¹³å‡å€¼é‚„å·®

```
 RÂ² å€¼çš„ç›´è¦ºè§£è®€ï¼š
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  1.0 â”€â”€â”€â”€ å®Œç¾ï¼ˆé€šå¸¸ä»£è¡¨ä½œå¼Šæˆ–è³‡æ–™æ´©æ¼ï¼‰  â”‚
 â”‚  0.9 â”€â”€â”€â”€ éå¸¸å¥½                          â”‚
 â”‚  0.7 â”€â”€â”€â”€ ä¸éŒ¯                            â”‚
 â”‚  0.5 â”€â”€â”€â”€ å‹‰å¼·å¯ç”¨                        â”‚
 â”‚  0.3 â”€â”€â”€â”€ æ¨¡å‹å¾ˆå¼±                        â”‚
 â”‚  0.0 â”€â”€â”€â”€ è·ŸçŒœå¹³å‡å€¼ä¸€æ¨£                  â”‚
 â”‚ <0.0 â”€â”€â”€â”€ æ¯”äº‚çŒœé‚„å·®ï¼Œæ¨¡å‹æœ‰åš´é‡å•é¡Œ      â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš ï¸ å¸¸è¦‹é™·é˜±ï¼šé«˜ RÂ² ä¸ä¸€å®šæ˜¯å¥½äº‹ï¼

é€™æ˜¯åˆå­¸è€…æœ€å¸¸çŠ¯çš„éŒ¯èª¤ï¼š

> ã€Œæˆ‘çš„ RÂ² æœ‰ 0.99 è€¶ï¼æ¨¡å‹è¶…æ£’çš„ï¼ã€

å…ˆåˆ¥æ€¥è‘—æ…¶ç¥ï¼Œé«˜ RÂ² å¯èƒ½ä»£è¡¨ï¼š

### é™·é˜± 1ï¼šè³‡æ–™æ´©æ¼ï¼ˆData Leakageï¼‰

```python
# éŒ¯èª¤ç¤ºç¯„ï¼šæŠŠç­”æ¡ˆæ”¾é€²ç‰¹å¾µè£¡
df['æˆ¿åƒ¹åŠ é›œè¨Š'] = df['æˆ¿åƒ¹_è¬'] + np.random.normal(0, 1, n)

X_bad = df[['åªæ•¸', 'æˆ¿é–“æ•¸', 'å±‹é½¡', 'æˆ¿åƒ¹åŠ é›œè¨Š']]  # å·çœ‹ç­”æ¡ˆï¼
model_bad = LinearRegression()
model_bad.fit(X_bad, y)
print(f"RÂ² = {model_bad.score(X_bad, y):.6f}")  # æ¥è¿‘ 1.0ï¼
# ä½†é€™å€‹æ¨¡å‹åœ¨çœŸå¯¦ä¸–ç•Œæ¯«ç„¡ç”¨è™•
```

### é™·é˜± 2ï¼šéæ“¬åˆï¼ˆOverfittingï¼‰

åœ¨è¨“ç·´é›†ä¸Š RÂ² å¾ˆé«˜ï¼Œä½†åœ¨æ¸¬è©¦é›†ä¸Š RÂ² å¾ˆä½ï¼š

```python
# æ¯”è¼ƒè¨“ç·´é›† vs æ¸¬è©¦é›†çš„è¡¨ç¾
train_r2 = model.score(X_train, y_train)
test_r2 = model.score(X_test, y_test)

print(f"è¨“ç·´é›† RÂ²: {train_r2:.4f}")
print(f"æ¸¬è©¦é›† RÂ²: {test_r2:.4f}")
print(f"å·®è·ï¼š{train_r2 - test_r2:.4f}")

# å¦‚æœå·®è· > 0.1ï¼Œå°±è¦å°å¿ƒéæ“¬åˆäº†ï¼
```

```
éæ“¬åˆè­¦ç¤ºè¡¨ï¼š
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  è¨“ç·´é›† RÂ²   â”‚  æ¸¬è©¦é›† RÂ²   â”‚  ç‹€æ…‹    â”‚
 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
 â”‚    0.95      â”‚    0.93      â”‚  âœ… æ­£å¸¸  â”‚
 â”‚    0.98      â”‚    0.70      â”‚  âš ï¸ éæ“¬åˆâ”‚
 â”‚    0.99      â”‚    0.40      â”‚  ğŸš¨ åš´é‡  â”‚
 â”‚    0.50      â”‚    0.48      â”‚  âŒ æ¬ æ“¬åˆâ”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### é™·é˜± 3ï¼šéç·šæ€§é—œä¿‚ç¡¬å¥—ç·šæ€§æ¨¡å‹

```
çœŸå¯¦ä¸–ç•Œï¼ˆæ›²ç·šé—œä¿‚ï¼‰ï¼š          ç·šæ€§å›æ­¸ç¡¬å¥—çš„çµæœï¼š

 y                              y
 |    ***                       |    ***
 |   *   *                      |   * / *
 | **     **                    | ** /   **
 |*        ***                  |* /      ***
 |            ****              | /          ****
 +â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> x           +â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> x
                                    RÂ² å¯èƒ½é‚„æœ‰ 0.6
                                    ä½†æ¨¡å‹å®Œå…¨éŒ¯èª¤ï¼
```

---

## æ®˜å·®åˆ†æï¼šæ¨¡å‹çš„å¥åº·æª¢æŸ¥

æ®˜å·®åœ–æ˜¯å›æ­¸åˆ†æä¸­æœ€é‡è¦çš„è¨ºæ–·å·¥å…·ï¼š

```python
residuals = y_test - y_pred

# æ®˜å·® vs é æ¸¬å€¼
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.scatter(y_pred, residuals, alpha=0.6)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('é æ¸¬å€¼')
plt.ylabel('æ®˜å·®')
plt.title('æ®˜å·®åœ–')

plt.subplot(1, 2, 2)
plt.hist(residuals, bins=20, edgecolor='black')
plt.xlabel('æ®˜å·®')
plt.ylabel('æ¬¡æ•¸')
plt.title('æ®˜å·®åˆ†ä½ˆ')
plt.tight_layout()
plt.show()
```

### å¥½çš„æ®˜å·®åœ– vs å£çš„æ®˜å·®åœ–

```
âœ… å¥½çš„æ®˜å·®åœ–ï¼ˆéš¨æ©Ÿæ•£å¸ƒï¼‰ï¼š     âŒ å£çš„æ®˜å·®åœ–ï¼ˆæœ‰æ¨¡å¼ï¼‰ï¼š

æ®˜å·®                             æ®˜å·®
  |    *  *     *                  |         * * *
  | *    *  *      *               |       *
  |   *     * * *                  |     *
 0|â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              |   *
  |  *   *    *   *               0|â”€â”€*â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  |     *  *    *                  | *       *
  |  *      *                      |           * *
  +â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> é æ¸¬å€¼      +â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> é æ¸¬å€¼

  æ®˜å·®éš¨æ©Ÿåˆ†å¸ƒ â†’ æ¨¡å‹OK             æ®˜å·®æœ‰æ›²ç·šè¶¨å‹¢ â†’ æ¨¡å‹éºæ¼
                                    äº†éç·šæ€§é—œä¿‚ï¼
```

### æ®˜å·®å‘Šè¨´æˆ‘å€‘ä»€éº¼ï¼Ÿ

```
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  æ®˜å·®åœ–çš„æ¨¡å¼        â”‚  ä»£è¡¨çš„å•é¡Œ                      â”‚
 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
 â”‚  éš¨æ©Ÿæ•£å¸ƒ            â”‚  âœ… æ¨¡å‹å‡è¨­å¤§è‡´æ­£ç¢º              â”‚
 â”‚  æ¼æ–—å½¢ï¼ˆè¶Šä¾†è¶Šæ•£ï¼‰  â”‚  âš ï¸ ç•°æ–¹å·®æ€§ï¼Œèª¤å·®ä¸å‡å‹»          â”‚
 â”‚  U å½¢æˆ–å€’ U å½¢      â”‚  âš ï¸ éºæ¼éç·šæ€§é—œä¿‚               â”‚
 â”‚  æœ‰ç¾¤èš              â”‚  âš ï¸ å¯èƒ½æœ‰æœªè€ƒæ…®çš„é¡åˆ¥è®Šæ•¸        â”‚
 â”‚  æœ‰æ˜é¡¯é›¢ç¾¤å€¼        â”‚  âš ï¸ è³‡æ–™å“è³ªå•é¡Œæˆ–ç‰¹æ®Šæ¡ˆä¾‹        â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§  å‹•å‹•è…¦ï¼šç‚ºä»€éº¼è¦åˆ‡åˆ†è¨“ç·´é›†å’Œæ¸¬è©¦é›†ï¼Ÿ

æƒ³åƒä½ æ˜¯è€å¸«ï¼Œå‡ºäº†ä¸€ä»½è€ƒå·ï¼š

- **æƒ…å¢ƒ A**ï¼šè€ƒè©¦å‰æŠŠè€ƒå·çµ¦å­¸ç”Ÿçœ‹ï¼Œè€ƒè©¦è€ƒä¸€æ¨£çš„é¡Œç›®
- **æƒ…å¢ƒ B**ï¼šè€ƒè©¦å‰çµ¦å­¸ç”Ÿç·´ç¿’é¡Œï¼Œè€ƒè©¦è€ƒæ–°çš„é¡ä¼¼é¡Œç›®

å“ªä¸€ç¨®æ›´èƒ½æ¸¬å‡ºå­¸ç”Ÿçš„çœŸå¯¦èƒ½åŠ›ï¼Ÿç•¶ç„¶æ˜¯ Bï¼

æ©Ÿå™¨å­¸ç¿’ä¹Ÿä¸€æ¨£ï¼š

```
  è¨“ç·´é›†ï¼ˆç·´ç¿’é¡Œï¼‰          æ¸¬è©¦é›†ï¼ˆè€ƒè©¦é¡Œï¼‰
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ æ¨¡å‹åœ¨é€™è£¡   â”‚        â”‚ æ¨¡å‹å¾æ²’çœ‹é â”‚
 â”‚ å­¸ç¿’è¦å¾‹     â”‚  â”€â”€â”€>  â”‚ åœ¨é€™è£¡é©—è­‰   â”‚
 â”‚              â”‚        â”‚ çœŸå¯¦èƒ½åŠ›     â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     80% è³‡æ–™               20% è³‡æ–™
```

```python
# æ¨™æº–åšæ³•
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,     # 20% ç•¶æ¸¬è©¦é›†
    random_state=42    # å›ºå®šéš¨æ©Ÿç¨®å­ï¼Œç¢ºä¿å¯é‡ç¾
)
```

---

## å›æ­¸æƒ…å¢ƒä¸‹çš„éæ“¬åˆ

### ä»€éº¼æ˜¯éæ“¬åˆï¼Ÿ

```
é©ç•¶æ“¬åˆï¼ˆGood Fitï¼‰ï¼š            éæ“¬åˆï¼ˆOverfitï¼‰ï¼š

 y                                y
 |   *  *                         |   *  *
 | *  /  *                        | * â•±â•²  *
 |  / *    *                      |  â•±  â•²* â•±*
 | / *   *                        | â•± *  â•±â•²â•±
 |/*   *                          |â•±*  â•±â•±
 +â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> x               +â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> x

 ç°¡å–®çš„ç·šï¼Œæ•æ‰è¶¨å‹¢               è¤‡é›œçš„æ›²ç·šï¼Œè¨˜ä½æ¯å€‹é»
 æ¸¬è©¦é›†è¡¨ç¾å¥½                      æ¸¬è©¦é›†è¡¨ç¾å·®
```

### éæ“¬åˆçš„ä¿¡è™Ÿ

```python
# ç•¶ç‰¹å¾µå¤ªå¤šæˆ–æ¨¡å‹å¤ªè¤‡é›œæ™‚
from sklearn.preprocessing import PolynomialFeatures

# ç”¨å¤šé …å¼ç‰¹å¾µæ¨¡æ“¬éæ“¬åˆ
for degree in [1, 3, 10, 20]:
    poly = PolynomialFeatures(degree=degree)
    X_train_poly = poly.fit_transform(X_train[['åªæ•¸']])
    X_test_poly = poly.transform(X_test[['åªæ•¸']])

    model_poly = LinearRegression()
    model_poly.fit(X_train_poly, y_train)

    train_r2 = model_poly.score(X_train_poly, y_train)
    test_r2 = model_poly.score(X_test_poly, y_test)

    print(f"degree={degree:2d} | è¨“ç·´ RÂ²={train_r2:.4f} | æ¸¬è©¦ RÂ²={test_r2:.4f} | "
          f"å·®è·={train_r2-test_r2:.4f}")
```

å¯èƒ½è¼¸å‡ºï¼š

```
 degree= 1 | è¨“ç·´ RÂ²=0.7523 | æ¸¬è©¦ RÂ²=0.7418 | å·®è·=0.0105  â† âœ… æ­£å¸¸
 degree= 3 | è¨“ç·´ RÂ²=0.7601 | æ¸¬è©¦ RÂ²=0.7389 | å·®è·=0.0212  â† âœ… é‚„å¥½
 degree=10 | è¨“ç·´ RÂ²=0.7892 | æ¸¬è©¦ RÂ²=0.6102 | å·®è·=0.1790  â† âš ï¸ è­¦å‘Š
 degree=20 | è¨“ç·´ RÂ²=0.8534 | æ¸¬è©¦ RÂ²=-1.234 | å·®è·=2.0874  â† ğŸš¨ çˆ†ç‚¸
```

---

## å®Œæ•´æ¡ˆä¾‹ï¼šå¾é ­åˆ°å°¾çš„å›æ­¸å»ºæ¨¡æµç¨‹

```python
# ============================================================
# æˆ¿åƒ¹é æ¸¬å®Œæ•´æµç¨‹
# ============================================================

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# 1. è¼‰å…¥èˆ‡æ¢ç´¢è³‡æ–™
print("=" * 50)
print("æ­¥é©Ÿ 1ï¼šè³‡æ–™æ¢ç´¢")
print("=" * 50)
print(df.describe().round(2))
print(f"\nç›¸é—œä¿‚æ•¸ï¼š")
print(df.corr()['æˆ¿åƒ¹_è¬'].sort_values(ascending=False).round(3))

# 2. æº–å‚™ç‰¹å¾µèˆ‡ç›®æ¨™
X = df[['åªæ•¸', 'æˆ¿é–“æ•¸', 'å±‹é½¡']]
y = df['æˆ¿åƒ¹_è¬']

# 3. åˆ‡åˆ†è³‡æ–™
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 4. è¨“ç·´æ¨¡å‹
model = LinearRegression()
model.fit(X_train, y_train)

# 5. é æ¸¬
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# 6. è©•ä¼°
print("\n" + "=" * 50)
print("æ­¥é©Ÿ 6ï¼šæ¨¡å‹è©•ä¼°")
print("=" * 50)

metrics = {
    'æŒ‡æ¨™': ['MSE', 'RMSE', 'MAE', 'RÂ²'],
    'è¨“ç·´é›†': [
        mean_squared_error(y_train, y_train_pred),
        np.sqrt(mean_squared_error(y_train, y_train_pred)),
        mean_absolute_error(y_train, y_train_pred),
        r2_score(y_train, y_train_pred)
    ],
    'æ¸¬è©¦é›†': [
        mean_squared_error(y_test, y_test_pred),
        np.sqrt(mean_squared_error(y_test, y_test_pred)),
        mean_absolute_error(y_test, y_test_pred),
        r2_score(y_test, y_test_pred)
    ]
}

metrics_df = pd.DataFrame(metrics)
print(metrics_df.to_string(index=False))

# 7. æ®˜å·®åˆ†æ
residuals = y_test - y_test_pred
print(f"\næ®˜å·®çµ±è¨ˆï¼š")
print(f"  å¹³å‡å€¼ï¼š{residuals.mean():.4f}ï¼ˆæ‡‰æ¥è¿‘ 0ï¼‰")
print(f"  æ¨™æº–å·®ï¼š{residuals.std():.2f}")
print(f"  æœ€å¤§æ­£æ®˜å·®ï¼š{residuals.max():.2f}")
print(f"  æœ€å¤§è² æ®˜å·®ï¼š{residuals.min():.2f}")

# 8. æ¨¡å‹è§£è®€
print("\n" + "=" * 50)
print("æ­¥é©Ÿ 8ï¼šæ¨¡å‹è§£è®€")
print("=" * 50)
print("æ¨¡å‹å…¬å¼ï¼š")
print(f"  æˆ¿åƒ¹ = {model.coef_[0]:.2f} Ã— åªæ•¸"
      f" + {model.coef_[1]:.2f} Ã— æˆ¿é–“æ•¸"
      f" + ({model.coef_[2]:.2f}) Ã— å±‹é½¡"
      f" + {model.intercept_:.2f}")
print("\nç™½è©±è§£è®€ï¼š")
print(f"  æ¯å¢åŠ  1 åªï¼Œæˆ¿åƒ¹å¢åŠ ç´„ {model.coef_[0]:.1f} è¬")
print(f"  æ¯å¤š 1 é–“æˆ¿ï¼Œæˆ¿åƒ¹å¢åŠ ç´„ {model.coef_[1]:.1f} è¬")
print(f"  å±‹é½¡æ¯å¢åŠ  1 å¹´ï¼Œæˆ¿åƒ¹æ¸›å°‘ç´„ {abs(model.coef_[2]):.1f} è¬")
```

---

## â“ æ²’æœ‰ç¬¨å•é¡Œ

**Qï¼šç·šæ€§å›æ­¸åªèƒ½è™•ç†ã€Œç›´ç·šã€é—œä¿‚å—ï¼Ÿ**

Aï¼šåå­—å«ã€Œç·šæ€§ã€ï¼Œä½†ä¸æ˜¯åªèƒ½ç•«ç›´ç·šã€‚é€éå¤šé …å¼ç‰¹å¾µè½‰æ›ï¼ˆPolynomial Featuresï¼‰ï¼Œ
ç·šæ€§å›æ­¸ä¹Ÿèƒ½æ“¬åˆæ›²ç·šã€‚å«ã€Œç·šæ€§ã€æ˜¯å› ç‚ºæ¨¡å‹å°**åƒæ•¸**æ˜¯ç·šæ€§çš„ï¼Œä¸æ˜¯å°**ç‰¹å¾µ**ã€‚

**Qï¼šå¦‚æœæˆ‘æœ‰ 100 å€‹ç‰¹å¾µï¼Œè©²å…¨éƒ¨ä¸Ÿé€²å»å—ï¼Ÿ**

Aï¼šä¸å»ºè­°ã€‚å¤ªå¤šç‰¹å¾µæœƒå°è‡´ï¼š
- éæ“¬åˆé¢¨éšªå¢åŠ 
- æ¨¡å‹é›£ä»¥è§£é‡‹
- è¨ˆç®—æˆæœ¬å¢åŠ 
- å¤šé‡å…±ç·šæ€§å•é¡Œï¼ˆç‰¹å¾µä¹‹é–“é«˜åº¦ç›¸é—œï¼‰

å»ºè­°å…ˆåšç‰¹å¾µé¸æ“‡æˆ–ä½¿ç”¨æ­£å‰‡åŒ–ï¼ˆRidge, Lassoï¼‰ï¼Œé€™æ˜¯ä¸‹ä¸€ç« çš„å…§å®¹ã€‚

**Qï¼šRÂ² å¯ä»¥æ˜¯è² çš„ï¼Ÿé€™æ˜¯ä»€éº¼æ„æ€ï¼Ÿ**

Aï¼šå¯ä»¥ï¼RÂ² ç‚ºè² è¡¨ç¤ºä½ çš„æ¨¡å‹æ¯”ã€Œæ°¸é çŒœå¹³å‡å€¼ã€é‚„å·®ã€‚é€šå¸¸ä»£è¡¨ï¼š
- æ¨¡å‹é¸éŒ¯äº†ï¼ˆä¾‹å¦‚ç”¨ç·šæ€§æ¨¡å‹æ“¬åˆå®Œå…¨éç·šæ€§çš„è³‡æ–™ï¼‰
- è¨“ç·´å‡ºäº†å•é¡Œ
- ç‰¹å¾µå’Œç›®æ¨™è®Šæ•¸ä¹‹é–“æ²’æœ‰ä½ å‡è¨­çš„é‚£ç¨®é—œä¿‚

**Qï¼šMSE å’Œ MAE è¦é¸å“ªå€‹ï¼Ÿ**

Aï¼šçœ‹ä½ çš„å ´æ™¯ï¼š
- **MSE / RMSE**ï¼šå°å¤§èª¤å·®æ‡²ç½°æ›´é‡ã€‚é©åˆã€Œå¤§éŒ¯æ¯”å°éŒ¯åš´é‡å¾—å¤šã€çš„å ´æ™¯ï¼ˆå¦‚é‡‘èé æ¸¬ï¼‰ã€‚
- **MAE**ï¼šå°æ‰€æœ‰èª¤å·®ä¸€è¦–åŒä»ã€‚é©åˆã€Œæ¯å€‹èª¤å·®éƒ½å·®ä¸å¤šåš´é‡ã€çš„å ´æ™¯ã€‚

**Qï¼šç‚ºä»€éº¼ random_state=42ï¼Ÿ**

Aï¼š42 æœ¬èº«æ²’æœ‰ç‰¹åˆ¥æ„ç¾©ï¼ˆå®ƒä¾†è‡ªã€Šä¹ä¸éŠæˆ²ã€‹ä¸­ã€Œç”Ÿå‘½ã€å®‡å®™åŠè¬äº‹è¬ç‰©çš„çµ‚æ¥µç­”æ¡ˆã€ï¼‰ã€‚
é‡é»æ˜¯è¨­å®šä»»ä½•å›ºå®šæ•¸å­—ï¼Œç¢ºä¿æ¯æ¬¡åˆ‡åˆ†è³‡æ–™çš„çµæœä¸€æ¨£ï¼Œè®“å¯¦é©—å¯ä»¥é‡ç¾ã€‚

---

## ğŸ§  å‹•å‹•è…¦ï¼šé€™å€‹æ¨¡å‹å¯ä»¥ä¸Šç·šå—ï¼Ÿ

ä½ è¨“ç·´äº†ä¸€å€‹æˆ¿åƒ¹é æ¸¬æ¨¡å‹ï¼Œå¾—åˆ°ä»¥ä¸‹çµæœï¼š

```
è¨“ç·´é›† RÂ² = 0.92
æ¸¬è©¦é›† RÂ² = 0.89
RMSE = 25 è¬
```

åœ¨æ±ºå®šæ˜¯å¦ä¸Šç·šä¹‹å‰ï¼Œå•å•è‡ªå·±ï¼š

1. **RÂ² = 0.89 ä»£è¡¨ä»€éº¼ï¼Ÿ** æ¨¡å‹è§£é‡‹äº† 89% çš„æˆ¿åƒ¹è®Šç•°ã€‚
2. **RMSE = 25 è¬æ˜¯å¦å¯æ¥å—ï¼Ÿ** å¦‚æœæˆ¿åƒ¹ç¯„åœæ˜¯ 200-2000 è¬ï¼Œ25 è¬çš„èª¤å·®å¯èƒ½å¯ä»¥æ¥å—ã€‚
   å¦‚æœæ˜¯ 100-300 è¬çš„æˆ¿å­ï¼Œ25 è¬çš„èª¤å·®å°±å¤ªå¤§äº†ã€‚
3. **è¨“ç·´é›†å’Œæ¸¬è©¦é›†çš„å·®è·ï¼Ÿ** 0.92 - 0.89 = 0.03ï¼Œå·®è·å°ï¼Œæ²’æœ‰æ˜é¡¯éæ“¬åˆã€‚
4. **æ®˜å·®æ˜¯å¦éš¨æ©Ÿï¼Ÿ** éœ€è¦ç•«æ®˜å·®åœ–ç¢ºèªã€‚
5. **è³‡æ–™æ˜¯å¦æœ‰ä»£è¡¨æ€§ï¼Ÿ** è¨“ç·´è³‡æ–™ä¾†è‡ªå“ªè£¡ï¼Ÿèƒ½ä»£è¡¨å¯¦éš›æ‡‰ç”¨å ´æ™¯å—ï¼Ÿ

> **çµè«–**ï¼šæ•¸å­—åªæ˜¯ä¸€å€‹åƒè€ƒï¼Œèƒ½ä¸èƒ½ä¸Šç·šå–æ±ºæ–¼**å•†æ¥­éœ€æ±‚**å’Œ**è³‡æ–™å“è³ª**ã€‚

---

## âš ï¸ å¸¸è¦‹é™·é˜±

### é™·é˜± 1ï¼šå¿˜è¨˜æª¢æŸ¥ç·šæ€§å‡è¨­

```python
# æ°¸é å…ˆç•«æ•£ä½ˆåœ–ï¼
import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 3, figsize=(15, 4))
for i, col in enumerate(['åªæ•¸', 'æˆ¿é–“æ•¸', 'å±‹é½¡']):
    axes[i].scatter(df[col], df['æˆ¿åƒ¹_è¬'], alpha=0.5, s=10)
    axes[i].set_xlabel(col)
    axes[i].set_ylabel('æˆ¿åƒ¹ï¼ˆè¬ï¼‰')
plt.tight_layout()
plt.show()
```

### é™·é˜± 2ï¼šç”¨è¨“ç·´é›†è©•ä¼°æ¨¡å‹

```python
# âŒ éŒ¯èª¤ï¼šç”¨è¨“ç·´è³‡æ–™è©•ä¼°
score = model.score(X_train, y_train)  # å¤ªæ¨‚è§€äº†ï¼

# âœ… æ­£ç¢ºï¼šç”¨æ¸¬è©¦è³‡æ–™è©•ä¼°
score = model.score(X_test, y_test)    # çœŸå¯¦è¡¨ç¾
```

### é™·é˜± 3ï¼šå¿½ç•¥ç‰¹å¾µå°ºåº¦

å¦‚æœä¸€å€‹ç‰¹å¾µæ˜¯ã€Œåªæ•¸ï¼ˆ10-60ï¼‰ã€ï¼Œå¦ä¸€å€‹æ˜¯ã€Œç¸½åƒ¹ï¼ˆ200-2000è¬ï¼‰ã€ï¼Œ
å®ƒå€‘çš„ä¿‚æ•¸ä¸èƒ½ç›´æ¥æ¯”è¼ƒå¤§å°ä¾†åˆ¤æ–·é‡è¦æ€§ã€‚

```python
# æ¨™æº–åŒ–å¾Œçš„ä¿‚æ•¸æ‰èƒ½æ¯”è¼ƒ
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

model_scaled = LinearRegression()
model_scaled.fit(X_train_scaled, y_train)

print("æ¨™æº–åŒ–å¾Œçš„ä¿‚æ•¸ï¼ˆå¯æ¯”è¼ƒé‡è¦æ€§ï¼‰ï¼š")
for name, coef in zip(X.columns, model_scaled.coef_):
    print(f"  {name}: {coef:.2f}")
```

### é™·é˜± 4ï¼šå¤–æ¨é æ¸¬

ç·šæ€§å›æ­¸åœ¨è¨“ç·´è³‡æ–™ç¯„åœå¤–çš„é æ¸¬æ˜¯ä¸å¯é çš„ï¼š

```
  è³‡æ–™ç¯„åœ               æ¨¡å‹å¤–æ¨ï¼ˆå±éšªå€åŸŸï¼‰
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  * * * * *   â”‚     â”‚    ???           â”‚
 â”‚ æ¨¡å‹åœ¨é€™è£¡   â”‚ --> â”‚  å¯èƒ½å®Œå…¨ä¸å°ï¼  â”‚
 â”‚  å­¸åˆ°è¦å¾‹    â”‚     â”‚  ç¾å¯¦å¯èƒ½æ˜¯æ›²ç·š  â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   åªæ•¸ 10-60            åªæ•¸ 100+
```

---

## æœ¬ç« å›é¡§

```
ç·šæ€§å›æ­¸çŸ¥è­˜åœ°åœ–ï¼š

 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚                   ç·šæ€§å›æ­¸                           â”‚
 â”‚                                                     â”‚
 â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
 â”‚  â”‚ æ¨¡å‹è¨“ç·´  â”‚  â”‚ æ¨¡å‹è©•ä¼°  â”‚  â”‚  æ¨¡å‹è¨ºæ–·     â”‚  â”‚
 â”‚  â”‚           â”‚  â”‚           â”‚  â”‚               â”‚  â”‚
 â”‚  â”‚ .fit()    â”‚  â”‚ MSE/RMSE  â”‚  â”‚  æ®˜å·®åœ–       â”‚  â”‚
 â”‚  â”‚ .predict()â”‚  â”‚ MAE       â”‚  â”‚  éæ“¬åˆæª¢æŸ¥   â”‚  â”‚
 â”‚  â”‚ .coef_    â”‚  â”‚ RÂ²        â”‚  â”‚  ç·šæ€§å‡è¨­     â”‚  â”‚
 â”‚  â”‚ .interceptâ”‚  â”‚           â”‚  â”‚  å¤–æ¨é¢¨éšª     â”‚  â”‚
 â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
 â”‚                                                     â”‚
 â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
 â”‚  â”‚              é‡è¦æé†’                         â”‚  â”‚
 â”‚  â”‚  - é«˜ RÂ² â‰  å¥½æ¨¡å‹                            â”‚  â”‚
 â”‚  â”‚  - ä¸€å®šè¦çœ‹æ®˜å·®åœ–                             â”‚  â”‚
 â”‚  â”‚  - è¨“ç·´é›†ã€æ¸¬è©¦é›†éƒ½è¦è©•ä¼°                     â”‚  â”‚
 â”‚  â”‚  - å…ˆç•«åœ–ï¼Œå†å»ºæ¨¡                             â”‚  â”‚
 â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ èª²å¾Œç·´ç¿’

### ç·´ç¿’ 1ï¼šåŸºç¤æ“ä½œ

ç”¨ scikit-learn å…§å»ºçš„ `california_housing` è³‡æ–™é›†ï¼š

```python
from sklearn.datasets import fetch_california_housing
data = fetch_california_housing()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = pd.Series(data.target, name='MedHouseVal')
```

1. åˆ‡åˆ†è¨“ç·´é›†å’Œæ¸¬è©¦é›†
2. è¨“ç·´ LinearRegression æ¨¡å‹
3. è¨ˆç®— MSEã€RMSEã€MAEã€RÂ²
4. ç•«æ®˜å·®åœ–
5. å“ªå€‹ç‰¹å¾µå°æˆ¿åƒ¹å½±éŸ¿æœ€å¤§ï¼Ÿï¼ˆæç¤ºï¼šå…ˆæ¨™æº–åŒ–ï¼‰

### ç·´ç¿’ 2ï¼šéæ“¬åˆå¯¦é©—

1. ç”¨ `PolynomialFeatures` å‰µé€  degree=1 åˆ° degree=15 çš„ç‰¹å¾µ
2. å°æ¯å€‹ degree è¨“ç·´æ¨¡å‹ï¼Œè¨˜éŒ„è¨“ç·´å’Œæ¸¬è©¦ RÂ²
3. ç•«å‡º degree vs RÂ² çš„åœ–ï¼Œæ‰¾å‡ºã€Œæœ€ä½³ç”œèœœé»ã€
4. è§€å¯Ÿï¼šå¾å“ªå€‹ degree é–‹å§‹å‡ºç¾æ˜é¡¯éæ“¬åˆï¼Ÿ

### ç·´ç¿’ 3ï¼šæ€è€ƒé¡Œ

ä¸€å®¶æˆ¿ä»²å…¬å¸æƒ³ç”¨ä½ çš„æ¨¡å‹ä¾†å®šåƒ¹ã€‚ä½ æœƒæ€éº¼å‘éæŠ€è¡“èƒŒæ™¯çš„ä¸»ç®¡è§£é‡‹ï¼š
- æ¨¡å‹çš„æº–ç¢ºåº¦ï¼ˆä¸èƒ½åªèªª RÂ²ï¼ï¼‰
- æ¨¡å‹çš„é™åˆ¶ï¼ˆä»€éº¼æƒ…æ³ä¸‹é æ¸¬ä¸æº–ï¼Ÿï¼‰
- æ˜¯å¦å¯ä»¥ç›´æ¥ä¸Šç·šä½¿ç”¨ï¼Ÿ

---

## ä¸‹ä¸€ç« é å‘Š

æˆ‘å€‘å·²ç¶“å­¸æœƒäº†é æ¸¬ã€Œé€£çºŒå€¼ã€ï¼ˆæˆ¿åƒ¹ï¼‰ï¼Œ
ä¸‹ä¸€ç« æˆ‘å€‘è¦å­¸ç¿’é æ¸¬ã€Œé¡åˆ¥ã€ï¼ˆæ˜¯/å¦ã€æ­£å¸¸/ç•°å¸¸ï¼‰ï¼Œ
ä¹Ÿå°±æ˜¯â€”â€”**é‚è¼¯æ–¯å›æ­¸èˆ‡åˆ†é¡è©•ä¼°**ã€‚

> ã€Œç•¶ä½ çš„å•é¡Œä¸æ˜¯ã€Œå¤šå°‘ã€è€Œæ˜¯ã€Œæ˜¯ä¸æ˜¯ã€çš„æ™‚å€™ï¼Œ
>   ä½ éœ€è¦çš„å°±ä¸å†æ˜¯ä¸€æ¢ç·šï¼Œè€Œæ˜¯ä¸€å€‹é–€æª»ã€‚ã€
