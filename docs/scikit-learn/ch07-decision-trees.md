# Chapter 7: æ±ºç­–æ¨¹ â€” è®“æ©Ÿå™¨å­¸æœƒã€Œå•å•é¡Œã€

> **ç¬¬ 7 é€±ï½œDecision Trees**

---

## ğŸ¯ æœ¬ç« ç›®æ¨™

è®€å®Œé€™ä¸€ç« ï¼Œä½ å°‡èƒ½å¤ ï¼š

1. ç†è§£æ±ºç­–æ¨¹çš„é‹ä½œåŸç† â€” å®ƒå°±æ˜¯åœ¨ã€Œå•å•é¡Œã€
2. ç”¨ scikit-learn çš„ `DecisionTreeClassifier` è¨“ç·´ä¸€æ£µæ¨¹
3. ææ‡‚ Gini Impurity å’Œ Information Gain åˆ°åº•åœ¨ç®—ä»€éº¼
4. çŸ¥é“æ±ºç­–æ¨¹ç‚ºä»€éº¼å®¹æ˜“ overfittingï¼Œä»¥åŠæ€éº¼æ§åˆ¶å®ƒ
5. å­¸æœƒç”¨ `max_depth`ã€`min_samples_split`ã€`min_samples_leaf` èª¿åƒ
6. ç†è§£å‰ªæï¼ˆPruningï¼‰çš„æ¦‚å¿µ
7. é«”æœƒæ±ºç­–æ¨¹æœ€å¤§çš„å„ªå‹¢ï¼š**å¯è§£é‡‹æ€§ï¼ˆInterpretabilityï¼‰**

---

## ä½ æ¯å¤©éƒ½åœ¨ç”¨æ±ºç­–æ¨¹

ä½ å¯èƒ½æ²’æ„è­˜åˆ°ï¼Œä½†ä½ æ¯å¤©æ—©ä¸Šéƒ½åœ¨è·‘ä¸€æ£µæ±ºç­–æ¨¹ï¼š

```
                    ä»Šå¤©è¦ç©¿ä»€éº¼ï¼Ÿ
                        |
                   ä¸‹é›¨äº†å—ï¼Ÿ
                  /          \
                æ˜¯            å¦
               /                \
          å¸¶å‚˜+ç©¿é›¨é‹        æº«åº¦ > 25Â°Cï¼Ÿ
                              /        \
                            æ˜¯          å¦
                           /              \
                        çŸ­è¢–+çŸ­è¤²       å¤–å¥—+é•·è¤²
```

çœ‹åˆ°äº†å—ï¼Ÿä½ åœ¨åšçš„äº‹æƒ…å°±æ˜¯ï¼š

1. **å•ä¸€å€‹å•é¡Œ**ï¼ˆä¸‹é›¨äº†å—ï¼Ÿï¼‰
2. **æ ¹æ“šç­”æ¡ˆåˆ†å…©é‚Šèµ°**
3. **ç¹¼çºŒå•ä¸‹ä¸€å€‹å•é¡Œ**ï¼ˆæº«åº¦é«˜å—ï¼Ÿï¼‰
4. **ç›´åˆ°å¾—å‡ºçµè«–**ï¼ˆç©¿ä»€éº¼ï¼‰

æ±ºç­–æ¨¹æ¼”ç®—æ³•åšçš„äº‹æƒ…**ä¸€æ¨¡ä¸€æ¨£**ï¼Œåªä¸éå®ƒå•çš„å•é¡Œæ˜¯ï¼š
ã€Œé€™å€‹ç‰¹å¾µçš„å€¼ï¼Œæœ‰æ²’æœ‰è¶…éæŸå€‹é–€æª»ï¼Ÿã€

---

## ğŸ’¡ é‡é»è§€å¿µï¼šæ±ºç­–æ¨¹çš„æœ¬è³ª

> æ±ºç­–æ¨¹å°±æ˜¯ä¸€é€£ä¸²çš„ **if-else åˆ¤æ–·**ï¼Œè‡ªå‹•å¾è³‡æ–™ä¸­å­¸å‡ºä¾†çš„ã€‚
> å®ƒä¸éœ€è¦ä½ å‘Šè¨´å®ƒã€Œå…ˆå•ä»€éº¼ã€ï¼Œå®ƒæœƒè‡ªå·±æ‰¾å‡º**æœ€å¥½çš„å•é¡Œé †åº**ã€‚

---

## ç¬¬ä¸€æ£µæ±ºç­–æ¨¹ï¼šç”¨ scikit-learn å‹•æ‰‹åš

è®“æˆ‘å€‘ç”¨ç¶“å…¸çš„é³¶å°¾èŠ±ï¼ˆIrisï¼‰è³‡æ–™é›†ä¾†è¨“ç·´ä¸€æ£µæ±ºç­–æ¨¹ï¼š

```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# è¼‰å…¥è³‡æ–™
iris = load_iris()
X, y = iris.data, iris.target

# åˆ‡åˆ†è¨“ç·´é›†èˆ‡æ¸¬è©¦é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# å»ºç«‹æ±ºç­–æ¨¹æ¨¡å‹
tree_clf = DecisionTreeClassifier(random_state=42)
tree_clf.fit(X_train, y_train)

# é æ¸¬èˆ‡è©•ä¼°
y_pred = tree_clf.predict(X_test)
print(f"æº–ç¢ºç‡: {accuracy_score(y_test, y_pred):.4f}")
# æº–ç¢ºç‡: 1.0000  <-- åˆ¥å¤ªé«˜èˆˆï¼Œé€™å¯èƒ½æ˜¯ overfittingï¼
```

å°±é€™æ¨£ï¼Œä¸‰è¡Œæ ¸å¿ƒç¨‹å¼ç¢¼ï¼Œä½ å°±è¨“ç·´å¥½äº†ä¸€æ£µæ±ºç­–æ¨¹ã€‚

---

## æ¨¹åˆ°åº•é•·ä»€éº¼æ¨£ï¼Ÿè®“æˆ‘å€‘çœ‹çœ‹

```python
from sklearn.tree import export_text

# ç”¨æ–‡å­—å°å‡ºé€™æ£µæ¨¹
tree_rules = export_text(tree_clf, feature_names=iris.feature_names)
print(tree_rules)
```

è¼¸å‡ºæœƒåƒé€™æ¨£ï¼ˆç°¡åŒ–ç‰ˆï¼‰ï¼š

```
|--- petal length (cm) <= 2.45
|   |--- class: 0 (setosa)
|--- petal length (cm) >  2.45
|   |--- petal width (cm) <= 1.75
|   |   |--- petal length (cm) <= 4.95
|   |   |   |--- class: 1 (versicolor)
|   |   |--- petal length (cm) >  4.95
|   |   |   |--- class: 2 (virginica)
|   |--- petal width (cm) >  1.75
|   |   |--- class: 2 (virginica)
```

çœ‹çœ‹é€™æ£µæ¨¹åœ¨åšä»€éº¼ï¼š

```
            èŠ±ç“£é•·åº¦ <= 2.45?
              /            \
            æ˜¯              å¦
           /                  \
      setosa          èŠ±ç“£å¯¬åº¦ <= 1.75?
       (å®Œæˆ!)          /            \
                      æ˜¯              å¦
                     /                  \
              èŠ±ç“£é•·åº¦ <= 4.95?      virginica
                /          \          (å®Œæˆ!)
              æ˜¯            å¦
             /                \
         versicolor        virginica
          (å®Œæˆ!)           (å®Œæˆ!)
```

ğŸ’¡ **é€™å°±æ˜¯æ±ºç­–æ¨¹æœ€å¤§çš„å„ªå‹¢ï¼šä½ å¯ä»¥ç›´æ¥çœ‹æ‡‚å®ƒåœ¨æƒ³ä»€éº¼ï¼**

ä½ èƒ½è·Ÿè€é—†èªªï¼šã€Œæ¨¡å‹åˆ¤æ–·é€™æœµèŠ±æ˜¯ setosaï¼Œå› ç‚ºå®ƒçš„èŠ±ç“£é•·åº¦å°æ–¼ 2.45 å…¬åˆ†ã€‚ã€
è©¦è©¦çœ‹ç”¨ç¥ç¶“ç¶²è·¯è§£é‡‹çœ‹çœ‹ï¼Ÿã€Œå‘ƒ...å› ç‚ºç¬¬ä¸‰å±¤çš„ç¬¬ 47 å€‹ç¥ç¶“å…ƒçš„æ¬Šé‡æ˜¯ 0.0372...ã€

---

## ğŸ§  å‹•å‹•è…¦

åœ¨ä¸Šé¢çš„æ±ºç­–æ¨¹ä¸­ï¼Œç‚ºä»€éº¼ç¬¬ä¸€å€‹å•é¡Œå•çš„æ˜¯ã€ŒèŠ±ç“£é•·åº¦ã€è€Œä¸æ˜¯ã€ŒèŠ±è¼å¯¬åº¦ã€ï¼Ÿ
æ˜¯æˆ‘å€‘å‘Šè¨´æ¨¡å‹çš„å—ï¼Ÿé‚„æ˜¯æ¨¡å‹è‡ªå·±å­¸å‡ºä¾†çš„ï¼Ÿ

ï¼ˆç­”æ¡ˆåœ¨ä¸‹ä¸€ç¯€æ­æ›‰ï¼‰

---

## Gini Impurityï¼šæ±ºç­–æ¨¹æ€éº¼æ±ºå®šã€Œå…ˆå•ä»€éº¼ã€

æ±ºç­–æ¨¹é¢å°ä¸€å€‹é—œéµå•é¡Œï¼š**ç‰¹å¾µé‚£éº¼å¤šï¼Œæˆ‘è©²å…ˆå•å“ªå€‹ï¼Ÿ**

ç­”æ¡ˆæ˜¯ï¼š**å“ªå€‹å•é¡Œèƒ½æŠŠè³‡æ–™åˆ†å¾—æœ€ã€Œä¹¾æ·¨ã€ï¼Œå°±å…ˆå•å“ªå€‹ã€‚**

ã€Œä¹¾æ·¨ã€æ˜¯ä»€éº¼æ„æ€ï¼Ÿæƒ³åƒä½ æœ‰ä¸€è¢‹å½ˆç ï¼š

```
è¢‹å­ A: ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´         <-- å¾ˆä¹¾æ·¨ï¼å…¨éƒ¨éƒ½æ˜¯ç´…è‰²
è¢‹å­ B: ğŸ”´ğŸ”´ğŸ”µğŸ”µğŸŸ¢         <-- å¾ˆäº‚ï¼ä»€éº¼é¡è‰²éƒ½æœ‰
è¢‹å­ C: ğŸ”´ğŸ”´ğŸ”´ğŸ”µğŸ”µ         <-- é‚„è¡Œï¼Œå¤§éƒ¨åˆ†æ˜¯ç´…è‰²
```

**Gini Impurityï¼ˆåŸºå°¼ä¸ç´”åº¦ï¼‰** å°±æ˜¯åœ¨é‡åŒ–é€™å€‹ã€Œäº‚çš„ç¨‹åº¦ã€ï¼š

```
Gini = 1 - Î£(páµ¢Â²)

å…¶ä¸­ páµ¢ = ç¬¬ i é¡çš„æ¯”ä¾‹
```

ä¾†ç®—ç®—çœ‹ï¼š

```
è¢‹å­ A: Gini = 1 - (1.0Â²) = 0.0          <-- å®Œå…¨ç´”æ·¨ï¼
è¢‹å­ B: Gini = 1 - (0.4Â² + 0.4Â² + 0.2Â²) = 1 - 0.36 = 0.64  <-- å¾ˆäº‚
è¢‹å­ C: Gini = 1 - (0.6Â² + 0.4Â²) = 1 - 0.52 = 0.48          <-- ä¸­ç­‰
```

**Gini = 0** è¡¨ç¤ºå®Œå…¨ç´”æ·¨ï¼ˆæ‰€æœ‰æ¨£æœ¬åŒä¸€é¡ï¼‰
**Gini è¶Šé«˜** è¡¨ç¤ºè¶Šæ··äº‚

---

## Information Gainï¼šå¦ä¸€ç¨®è¡¡é‡æ–¹å¼

é™¤äº† Giniï¼Œé‚„æœ‰ä¸€ç¨®æ–¹å¼å« **Information Gainï¼ˆè³‡è¨Šå¢ç›Šï¼‰**ï¼Œ
åŸºæ–¼ **Entropyï¼ˆç†µï¼‰** çš„æ¦‚å¿µï¼š

```
Entropy = -Î£(páµ¢ Ã— logâ‚‚(páµ¢))
```

```
è¢‹å­ A: Entropy = -(1.0 Ã— logâ‚‚(1.0)) = 0.0           <-- æ²’æœ‰ä¸ç¢ºå®šæ€§
è¢‹å­ B: Entropy = -(0.4Ã—logâ‚‚0.4 + 0.4Ã—logâ‚‚0.4 + 0.2Ã—logâ‚‚0.2) â‰ˆ 1.52
è¢‹å­ C: Entropy = -(0.6Ã—logâ‚‚0.6 + 0.4Ã—logâ‚‚0.4) â‰ˆ 0.97
```

**Information Gain = åˆ†è£‚å‰çš„ Entropy - åˆ†è£‚å¾Œçš„åŠ æ¬Š Entropy**

ç›´è¦ºä¸Šï¼š
- Gini å’Œ Entropy å¤§å¤šæ™‚å€™çµæœå·®ä¸å¤š
- scikit-learn é è¨­ç”¨ Giniï¼ˆè¨ˆç®—æ¯”è¼ƒå¿«ï¼‰
- æƒ³ç”¨ Entropy çš„è©±ï¼š`DecisionTreeClassifier(criterion='entropy')`

```python
# æ¯”è¼ƒå…©ç¨® criterion
tree_gini = DecisionTreeClassifier(criterion='gini', random_state=42)
tree_entropy = DecisionTreeClassifier(criterion='entropy', random_state=42)

tree_gini.fit(X_train, y_train)
tree_entropy.fit(X_train, y_train)

print(f"Gini æº–ç¢ºç‡:    {tree_gini.score(X_test, y_test):.4f}")
print(f"Entropy æº–ç¢ºç‡: {tree_entropy.score(X_test, y_test):.4f}")
```

---

## ğŸ’¡ é‡é»è§€å¿µï¼šæ±ºç­–æ¨¹çš„åˆ†è£‚æµç¨‹

```
å°æ¯å€‹ç¯€é»ï¼ˆnodeï¼‰ï¼š
  1. éæ­·æ‰€æœ‰ç‰¹å¾µ
  2. å°æ¯å€‹ç‰¹å¾µï¼Œå˜—è©¦æ‰€æœ‰å¯èƒ½çš„åˆ†è£‚é»
  3. è¨ˆç®—æ¯ç¨®åˆ†è£‚çš„ Gini Impurityï¼ˆæˆ– Information Gainï¼‰
  4. é¸æ“‡è®“å­ç¯€é»æœ€ã€Œç´”æ·¨ã€çš„é‚£å€‹åˆ†è£‚
  5. é‡è¤‡ï¼Œç›´åˆ°ï¼š
     - ç¯€é»å®Œå…¨ç´”æ·¨ï¼ˆGini = 0ï¼‰
     - é”åˆ°åœæ­¢æ¢ä»¶ï¼ˆmax_depth ç­‰ï¼‰
```

---

## âš ï¸ å¸¸è¦‹é™·é˜±ï¼šæ±ºç­–æ¨¹çš„ Overfitting å•é¡Œ

æ±ºç­–æ¨¹æœ‰ä¸€å€‹å¾ˆåš´é‡çš„å•é¡Œï¼š**å®ƒå¤ªèªçœŸäº†ã€‚**

å¦‚æœä½ ä¸è¨­é™ï¼Œæ±ºç­–æ¨¹æœƒä¸€ç›´åˆ†è£‚ã€ä¸€ç›´åˆ†è£‚ï¼Œç›´åˆ°æ¯å€‹è‘‰ç¯€é»åªå‰©ä¸€å€‹æ¨£æœ¬ã€‚
å®ƒæœƒæŠŠè¨“ç·´è³‡æ–™çš„æ¯å€‹ç´°ç¯€ã€æ¯å€‹é›œè¨Šéƒ½ã€ŒèƒŒã€ä¸‹ä¾†ã€‚

```python
# ä¸è¨­é™çš„æ±ºç­–æ¨¹
tree_no_limit = DecisionTreeClassifier(random_state=42)
tree_no_limit.fit(X_train, y_train)

print(f"è¨“ç·´é›†æº–ç¢ºç‡: {tree_no_limit.score(X_train, y_train):.4f}")
print(f"æ¸¬è©¦é›†æº–ç¢ºç‡: {tree_no_limit.score(X_test, y_test):.4f}")

print(f"æ¨¹çš„æ·±åº¦: {tree_no_limit.get_depth()}")
print(f"è‘‰ç¯€é»æ•¸: {tree_no_limit.get_n_leaves()}")
```

ä½ å¯èƒ½æœƒçœ‹åˆ°ï¼š
```
è¨“ç·´é›†æº–ç¢ºç‡: 1.0000   <-- å®Œç¾ï¼ä½†é€™æ˜¯åœ¨ã€ŒèƒŒç­”æ¡ˆã€
æ¸¬è©¦é›†æº–ç¢ºç‡: 0.9556   <-- çœŸæ­£çš„å¯¦åŠ›
æ¨¹çš„æ·±åº¦: 5
è‘‰ç¯€é»æ•¸: 9
```

é€™å°±åƒä¸€å€‹å­¸ç”ŸæŠŠè€ƒå¤é¡Œçš„ç­”æ¡ˆå…¨éƒ¨èƒŒä¸‹ä¾†ï¼Œä½†é‡åˆ°æ–°é¡Œç›®å°±å‚»äº†ã€‚

---

## æ§åˆ¶æ¨¹çš„ç”Ÿé•·ï¼šè¶…åƒæ•¸èª¿æ•´

scikit-learn æä¾›äº†å¹¾å€‹é‡è¦çš„è¶…åƒæ•¸ä¾†æ§åˆ¶æ±ºç­–æ¨¹çš„ç”Ÿé•·ï¼š

### 1. `max_depth` â€” é™åˆ¶æ¨¹çš„æ·±åº¦

```
max_depth=2 çš„æ¨¹ï¼š          max_depth=10 çš„æ¨¹ï¼š

     [å•é¡Œ1]                    [å•é¡Œ1]
     /    \                     /    \
  [å•é¡Œ2] [å•é¡Œ3]           [å•é¡Œ2] [å•é¡Œ3]
  / \     / \               / \     / \
 A   B   C   D           ...  ... ...  ...
                          ï¼ˆç¹¼çºŒå¾€ä¸‹å¾ˆå¤šå±¤ï¼‰

 ç°¡å–®ã€ä¸æ˜“ overfit         è¤‡é›œã€å®¹æ˜“ overfit
```

```python
# æ¯”è¼ƒä¸åŒæ·±åº¦
for depth in [1, 2, 3, 5, None]:
    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)
    tree.fit(X_train, y_train)
    train_acc = tree.score(X_train, y_train)
    test_acc = tree.score(X_test, y_test)
    label = "ç„¡é™" if depth is None else str(depth)
    print(f"max_depth={label:>3}: è¨“ç·´={train_acc:.4f}, æ¸¬è©¦={test_acc:.4f}")
```

### 2. `min_samples_split` â€” ç¯€é»æœ€å°‘è¦å¤šå°‘æ¨£æœ¬æ‰èƒ½ç¹¼çºŒåˆ†

```python
# è‡³å°‘è¦ 10 å€‹æ¨£æœ¬æ‰å…è¨±ç¹¼çºŒåˆ†è£‚
tree = DecisionTreeClassifier(min_samples_split=10, random_state=42)
```

### 3. `min_samples_leaf` â€” æ¯å€‹è‘‰ç¯€é»è‡³å°‘è¦æœ‰å¤šå°‘æ¨£æœ¬

```python
# æ¯å€‹è‘‰ç¯€é»è‡³å°‘è¦æœ‰ 5 å€‹æ¨£æœ¬
tree = DecisionTreeClassifier(min_samples_leaf=5, random_state=42)
```

### è¶…åƒæ•¸å°ç…§è¡¨

```
+---------------------+--------+------------------------------+
| è¶…åƒæ•¸              | é è¨­å€¼ | æ•ˆæœ                         |
+---------------------+--------+------------------------------+
| max_depth           | None   | é™åˆ¶æ¨¹çš„æœ€å¤§æ·±åº¦             |
| min_samples_split   | 2      | ç¯€é»åˆ†è£‚æ‰€éœ€æœ€å°‘æ¨£æœ¬æ•¸       |
| min_samples_leaf    | 1      | è‘‰ç¯€é»æœ€å°‘æ¨£æœ¬æ•¸             |
| max_leaf_nodes      | None   | é™åˆ¶è‘‰ç¯€é»ç¸½æ•¸               |
| max_features        | None   | æ¯æ¬¡åˆ†è£‚è€ƒæ…®çš„ç‰¹å¾µæ•¸         |
| min_impurity_decrease| 0.0   | æœ€å°ä¸ç´”åº¦ä¸‹é™é‡             |
+---------------------+--------+------------------------------+

        max_depth å°     â†’  æ¨¹æ·º  â†’  underfitting é¢¨éšª
        max_depth å¤§     â†’  æ¨¹æ·±  â†’  overfitting é¢¨éšª
        min_samples_leaf å¤§ â†’ è‘‰ç²—  â†’  underfitting é¢¨éšª
        min_samples_leaf å° â†’ è‘‰ç´°  â†’  overfitting é¢¨éšª
```

---

## å‰ªæï¼ˆPruningï¼‰çš„æ¦‚å¿µ

å‰ªææœ‰å…©ç¨®ç­–ç•¥ï¼š

### Pre-pruningï¼ˆé å‰ªæï¼‰â€” æå‰åœæ­¢ç”Ÿé•·

å°±æ˜¯æˆ‘å€‘ä¸Šé¢åšçš„äº‹æƒ…ï¼šè¨­å®š `max_depth`ã€`min_samples_split` ç­‰ç­‰ã€‚
åœ¨æ¨¹é‚„åœ¨ç”Ÿé•·çš„æ™‚å€™å°±é™åˆ¶å®ƒã€‚

### Post-pruningï¼ˆå¾Œå‰ªæï¼‰â€” å…ˆé•·å®Œå†ä¿®å‰ª

å…ˆè®“æ¨¹å®Œå…¨é•·å¥½ï¼Œç„¶å¾ŒæŠŠé‚£äº›ã€Œæ²’ä»€éº¼ç”¨ã€çš„åˆ†æ”¯ç æ‰ã€‚

scikit-learn æ”¯æ´çš„å¾Œå‰ªææ–¹å¼æ˜¯ **Cost-Complexity Pruning**ï¼Œ
ä½¿ç”¨ `ccp_alpha` åƒæ•¸ï¼š

```python
# Cost-Complexity Pruning
# ccp_alpha è¶Šå¤§ï¼Œä¿®å‰ªè¶Šå¤š
tree_pruned = DecisionTreeClassifier(ccp_alpha=0.02, random_state=42)
tree_pruned.fit(X_train, y_train)

print(f"ä¿®å‰ªå¾Œçš„æ·±åº¦: {tree_pruned.get_depth()}")
print(f"ä¿®å‰ªå¾Œçš„è‘‰ç¯€é»æ•¸: {tree_pruned.get_n_leaves()}")
print(f"æ¸¬è©¦é›†æº–ç¢ºç‡: {tree_pruned.score(X_test, y_test):.4f}")
```

æ‰¾å‡ºæœ€ä½³ `ccp_alpha` çš„æ–¹æ³•ï¼š

```python
import numpy as np

# å–å¾—ä¸åŒ alpha å€¼å°æ‡‰çš„æ•ˆæœ
path = tree_no_limit.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas = path.ccp_alphas

# å°æ¯å€‹ alpha è¨“ç·´ä¸€æ£µæ¨¹
train_scores = []
test_scores = []

for alpha in ccp_alphas:
    tree = DecisionTreeClassifier(ccp_alpha=alpha, random_state=42)
    tree.fit(X_train, y_train)
    train_scores.append(tree.score(X_train, y_train))
    test_scores.append(tree.score(X_test, y_test))

# æ‰¾å‡ºæ¸¬è©¦é›†è¡¨ç¾æœ€å¥½çš„ alpha
best_idx = np.argmax(test_scores)
print(f"æœ€ä½³ ccp_alpha: {ccp_alphas[best_idx]:.4f}")
print(f"å°æ‡‰æ¸¬è©¦æº–ç¢ºç‡: {test_scores[best_idx]:.4f}")
```

---

## ğŸ§  å‹•å‹•è…¦

æ±ºç­–æ¨¹çš„ `max_depth=1` æ™‚ï¼Œé€™æ£µæ¨¹å«åšã€ŒDecision Stumpï¼ˆæ±ºç­–æ¨¹æ¨ï¼‰ã€ã€‚
å®ƒåªå•ä¸€å€‹å•é¡Œå°±åšå‡ºåˆ¤æ–·ã€‚

æƒ³æƒ³çœ‹ï¼šä»€éº¼æ™‚å€™ Decision Stump å°±å¤ ç”¨äº†ï¼Ÿä»€éº¼æ™‚å€™å®ƒå®Œå…¨ä¸è¡Œï¼Ÿ

ï¼ˆæç¤ºï¼šæƒ³æƒ³è³‡æ–™æ˜¯ã€Œç·šæ€§å¯åˆ†ã€çš„æƒ…æ³ï¼‰

---

## æ±ºç­–æ¨¹è¦–è¦ºåŒ–

é™¤äº†æ–‡å­—ç‰ˆï¼Œæˆ‘å€‘ä¹Ÿå¯ä»¥ç•«å‡ºæ¼‚äº®çš„æ¨¹ç‹€åœ–ï¼š

```python
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

# è¨“ç·´ä¸€æ£µæœ‰é™æ·±åº¦çš„æ¨¹ï¼ˆæ¯”è¼ƒå¥½çœ‹ï¼‰
tree_viz = DecisionTreeClassifier(max_depth=3, random_state=42)
tree_viz.fit(X_train, y_train)

# ç•«å‡ºæ±ºç­–æ¨¹
plt.figure(figsize=(16, 8))
plot_tree(
    tree_viz,
    feature_names=iris.feature_names,
    class_names=iris.target_names,
    filled=True,       # ç”¨é¡è‰²å¡«å……
    rounded=True,      # åœ“è§’æ–¹æ¡†
    fontsize=10
)
plt.title("é³¶å°¾èŠ±æ±ºç­–æ¨¹ (max_depth=3)")
plt.tight_layout()
plt.savefig("decision_tree_iris.png", dpi=150)
plt.show()
```

æ¯å€‹ç¯€é»æœƒé¡¯ç¤ºï¼š
```
+---------------------------+
| petal length <= 2.45      |  <-- åˆ†è£‚æ¢ä»¶
| gini = 0.667              |  <-- ä¸ç´”åº¦
| samples = 105             |  <-- æ¨£æœ¬æ•¸
| value = [34, 32, 39]      |  <-- å„é¡åˆ¥æ•¸é‡
| class = virginica          |  <-- å¤šæ•¸é¡åˆ¥
+---------------------------+
```

---

## æ±ºç­–æ¨¹çš„å„ªç¼ºé»ç¸½æ•´ç†

```
+------------------+----------------------------------------+
| å„ªé»             | èªªæ˜                                   |
+------------------+----------------------------------------+
| å¯è§£é‡‹æ€§å¼·       | å¯ä»¥ç›´æ¥çœ‹æ‡‚æ¨¡å‹çš„åˆ¤æ–·é‚è¼¯             |
| ä¸éœ€è¦ç‰¹å¾µç¸®æ”¾   | ä¸ç”¨åš StandardScaler æˆ– MinMaxScaler  |
| è™•ç†éç·šæ€§é—œä¿‚   | å¤©ç”Ÿå¯ä»¥è™•ç†éç·šæ€§çš„æ±ºç­–é‚Šç•Œ           |
| é€Ÿåº¦å¿«           | è¨“ç·´å’Œé æ¸¬éƒ½å¾ˆå¿«                       |
| å¯è™•ç†æ··åˆå‹è³‡æ–™ | æ•¸å€¼å‹å’Œé¡åˆ¥å‹ç‰¹å¾µéƒ½å¯ä»¥               |
+------------------+----------------------------------------+

+------------------+----------------------------------------+
| ç¼ºé»             | èªªæ˜                                   |
+------------------+----------------------------------------+
| å®¹æ˜“ overfitting | ä¸è¨­é™çš„è©±æœƒå®Œå…¨è¨˜ä½è¨“ç·´è³‡æ–™           |
| ä¸ç©©å®š           | è³‡æ–™å°è®Šå‹•å¯èƒ½ç”¢ç”Ÿå®Œå…¨ä¸åŒçš„æ¨¹         |
| åå‘å¤šé¡åˆ¥ç‰¹å¾µ   | é¡åˆ¥å¤šçš„ç‰¹å¾µå®¹æ˜“è¢«å„ªå…ˆé¸ä¸­             |
| ç„¡æ³•å¤–æ¨         | åªèƒ½é æ¸¬è¨“ç·´è³‡æ–™ç¯„åœå…§çš„å€¼             |
| æ±ºç­–é‚Šç•Œæ˜¯ç›´è§’   | åªèƒ½ç”¢ç”Ÿå¹³è¡Œæ–¼åº§æ¨™è»¸çš„åˆ†è£‚             |
+------------------+----------------------------------------+
```

---

## å¯¦æˆ°ç¯„ä¾‹ï¼šç”¨æ±ºç­–æ¨¹é æ¸¬éµé”å°¼è™Ÿç”Ÿå­˜

```python
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# æ¨¡æ“¬éµé”å°¼è™Ÿé¢¨æ ¼çš„è³‡æ–™
# ï¼ˆå¯¦éš›ä½¿ç”¨æ™‚å¯ä»¥å¾ seaborn è¼‰å…¥ï¼šimport seaborn as sns; df = sns.load_dataset('titanic')ï¼‰
from sklearn.datasets import fetch_openml
titanic = fetch_openml('titanic', version=1, as_frame=True, parser='auto')
df = titanic.frame

# ç°¡å–®çš„ç‰¹å¾µå·¥ç¨‹
df = df[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'survived']].dropna()
df['sex'] = df['sex'].map({'male': 0, 'female': 1})
df['survived'] = df['survived'].astype(int)

X = df.drop('survived', axis=1)
y = df['survived']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# è¨“ç·´ä¸€æ£µæœ‰é™æ·±åº¦çš„æ±ºç­–æ¨¹
tree = DecisionTreeClassifier(max_depth=4, min_samples_leaf=10, random_state=42)
tree.fit(X_train, y_train)

# è©•ä¼°
y_pred = tree.predict(X_test)
print(classification_report(y_test, y_pred, target_names=['ç½¹é›£', 'ç”Ÿé‚„']))

# çœ‹çœ‹æ¨¡å‹å­¸åˆ°äº†ä»€éº¼
print("\nç‰¹å¾µé‡è¦æ€§:")
for name, importance in zip(X.columns, tree.feature_importances_):
    print(f"  {name:>8}: {'â–ˆ' * int(importance * 40)} {importance:.4f}")
```

å¯èƒ½çš„è¼¸å‡ºï¼š

```
ç‰¹å¾µé‡è¦æ€§:
    pclass: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             0.1523
       sex: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.5847
       age: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           0.2015
     sibsp: â–ˆ                  0.0198
     parch: â–ˆ                  0.0112
      fare: â–ˆâ–ˆ                 0.0305
```

æ€§åˆ¥ï¼ˆsexï¼‰æ˜¯æœ€é‡è¦çš„ç‰¹å¾µ â€” ã€Œå¥³å£«å„ªå…ˆã€çš„é€ƒç”ŸåŸå‰‡è¢«æ¨¡å‹å­¸åˆ°äº†ï¼

---

## â“ æ²’æœ‰ç¬¨å•é¡Œ

**Q: æ±ºç­–æ¨¹å¯ä»¥ç”¨åœ¨è¿´æ­¸å•é¡Œå—ï¼Ÿ**
A: å¯ä»¥ï¼ç”¨ `DecisionTreeRegressor`ã€‚å®ƒä¸æ˜¯ç”¨ Gini ä¾†åˆ†è£‚ï¼Œè€Œæ˜¯ç”¨ MSEï¼ˆå‡æ–¹èª¤å·®ï¼‰ã€‚
è‘‰ç¯€é»çš„é æ¸¬å€¼æ˜¯è©²ç¯€é»ä¸­æ‰€æœ‰æ¨£æœ¬çš„å¹³å‡å€¼ã€‚

**Q: Gini å’Œ Entropy åˆ°åº•å·®åœ¨å“ªï¼Ÿ**
A: å¤§éƒ¨åˆ†æƒ…æ³ä¸‹çµæœå·®ä¸å¤šã€‚Gini è¨ˆç®—æ¯”è¼ƒå¿«ï¼ˆä¸ç”¨ç®— logï¼‰ï¼Œæ‰€ä»¥æ˜¯ scikit-learn çš„é è¨­å€¼ã€‚
Entropy å‚¾å‘æ–¼ç”¢ç”Ÿç¨å¾®æ›´å¹³è¡¡çš„æ¨¹ã€‚å¯¦å‹™ä¸Šï¼ŒèŠ±æ™‚é–“èª¿ `max_depth` æ¯”ç³¾çµ criterion æœ‰ç”¨å¤šäº†ã€‚

**Q: æ±ºç­–æ¨¹éœ€è¦åšç‰¹å¾µç¸®æ”¾å—ï¼Ÿ**
A: ä¸éœ€è¦ï¼é€™æ˜¯æ±ºç­–æ¨¹çš„ä¸€å¤§å„ªé»ã€‚å› ç‚ºå®ƒåªçœ‹ã€Œå¤§æ–¼æˆ–å°æ–¼æŸå€‹å€¼ã€ï¼Œ
ä¸åƒ SVM æˆ– KNN é‚£æ¨£æœƒå—åˆ°ç‰¹å¾µå°ºåº¦çš„å½±éŸ¿ã€‚

**Q: ç‚ºä»€éº¼èªªæ±ºç­–æ¨¹ã€Œä¸ç©©å®šã€ï¼Ÿ**
A: å› ç‚ºåªè¦è¨“ç·´è³‡æ–™ç¨å¾®æ”¹è®Šï¼ˆæ¯”å¦‚å¤šä¸€ç­†æˆ–å°‘ä¸€ç­†ï¼‰ï¼Œ
æ•´æ£µæ¨¹çš„çµæ§‹å¯èƒ½å®Œå…¨ä¸åŒã€‚é€™å°±æ˜¯ç‚ºä»€éº¼ä¸‹ä¸€ç« çš„ Random Forest è¦æŠŠå¾ˆå¤šæ£µæ¨¹çµ„åˆèµ·ä¾†ã€‚

**Q: `max_depth` åˆ°åº•è¦è¨­å¤šå°‘ï¼Ÿ**
A: æ²’æœ‰æ¨™æº–ç­”æ¡ˆã€‚é€šå¸¸å¾ 3-5 é–‹å§‹è©¦ï¼Œç”¨äº¤å‰é©—è­‰ï¼ˆç¬¬ 9 ç« æœƒæ•™ï¼‰æ‰¾æœ€ä½³å€¼ã€‚
å¤ªæ·ºæœƒ underfitï¼Œå¤ªæ·±æœƒ overfitã€‚

---

## ğŸ“ èª²å¾Œç·´ç¿’

### ç·´ç¿’ 1ï¼šæ‰‹å‹•è¨ˆç®— Gini

å‡è¨­ä¸€å€‹ç¯€é»æœ‰ 100 å€‹æ¨£æœ¬ï¼š60 å€‹æ˜¯é¡åˆ¥ Aï¼Œ40 å€‹æ˜¯é¡åˆ¥ Bã€‚

1. è¨ˆç®—é€™å€‹ç¯€é»çš„ Gini Impurity
2. å¦‚æœåˆ†è£‚å¾Œï¼Œå·¦é‚Šæœ‰ 50 å€‹ï¼ˆ45A, 5Bï¼‰ï¼Œå³é‚Šæœ‰ 50 å€‹ï¼ˆ15A, 35Bï¼‰ï¼Œ
   è¨ˆç®—åˆ†è£‚å¾Œçš„åŠ æ¬Š Gini
3. é€™æ¬¡åˆ†è£‚å¥½ä¸å¥½ï¼Ÿç‚ºä»€éº¼ï¼Ÿ

### ç·´ç¿’ 2ï¼šè¶…åƒæ•¸å¯¦é©—

```python
# ç”¨ sklearn çš„ make_classification ç”¢ç”Ÿè³‡æ–™
from sklearn.datasets import make_classification

X, y = make_classification(
    n_samples=500, n_features=10, n_informative=5,
    n_redundant=2, random_state=42
)

# è©¦è©¦ä¸åŒçš„è¶…åƒæ•¸çµ„åˆï¼Œè¨˜éŒ„è¨“ç·´èˆ‡æ¸¬è©¦æº–ç¢ºç‡
# 1. max_depth = [2, 4, 6, 8, None]
# 2. min_samples_leaf = [1, 5, 10, 20]
# 3. å“ªå€‹çµ„åˆåœ¨æ¸¬è©¦é›†ä¸Šè¡¨ç¾æœ€å¥½ï¼Ÿ
```

### ç·´ç¿’ 3ï¼šè¦–è¦ºåŒ–æ¯”è¼ƒ

è¨“ç·´å…©æ£µæ±ºç­–æ¨¹ï¼ˆä¸€æ£µ `max_depth=2`ï¼Œä¸€æ£µä¸è¨­é™ï¼‰ï¼Œ
ç”¨ `plot_tree` æŠŠå…©æ£µæ¨¹éƒ½ç•«å‡ºä¾†ï¼Œæ¯”è¼ƒå®ƒå€‘çš„è¤‡é›œåº¦å·®ç•°ã€‚

---

## æœ¬ç« ç¸½çµ

```
+------------------------------------------+
|           æ±ºç­–æ¨¹æ ¸å¿ƒè§€å¿µ                 |
+------------------------------------------+
| 1. æ±ºç­–æ¨¹ = è‡ªå‹•å­¸ç¿’çš„ if-else è¦å‰‡     |
| 2. ç”¨ Gini / Entropy æ±ºå®šåˆ†è£‚æ–¹å¼       |
| 3. æœ€å¤§å„ªå‹¢ = å¯è§£é‡‹æ€§                  |
| 4. æœ€å¤§é¢¨éšª = overfitting               |
| 5. ç”¨ max_depth ç­‰åƒæ•¸æ§åˆ¶è¤‡é›œåº¦        |
| 6. å‰ªæ = ç æ‰æ²’ç”¨çš„åˆ†æ”¯               |
+------------------------------------------+

ä¸‹ä¸€ç« ï¼Œæˆ‘å€‘è¦æŠŠå¾ˆå¤šæ£µæ±ºç­–æ¨¹çµ„åˆèµ·ä¾†ï¼Œ
ç”¨ã€Œä¸‰å€‹è‡­çš®åŒ å‹éä¸€å€‹è«¸è‘›äº®ã€çš„ç²¾ç¥ï¼Œ
æ‰“é€ æ›´å¼·å¤§çš„æ¨¡å‹ â€” Random Forestï¼
```
