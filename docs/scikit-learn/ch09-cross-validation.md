# Chapter 9: äº¤å‰é©—è­‰èˆ‡æ¨¡å‹è©•ä¼° â€” ä¸èƒ½åªè·‘ä¸€æ¬¡ï¼Œä¸èƒ½åªçœ‹ Accuracy

> **ç¬¬ 9 é€±ï½œCross-Validation & Model Evaluation**

---

## ğŸ¯ æœ¬ç« ç›®æ¨™

è®€å®Œé€™ä¸€ç« ï¼Œä½ å°‡èƒ½å¤ ï¼š

1. ç†è§£ç‚ºä»€éº¼å–®ä¸€çš„ train/test split ä¸å¤ å¯é 
2. æŒæ¡ K-Fold Cross Validation çš„åŸç†
3. ç”¨ `cross_val_score` åšäº¤å‰é©—è­‰
4. ç†è§£ Stratified K-Fold çš„é‡è¦æ€§
5. å­¸æœƒç”¨ `classification_report` çœ‹å®Œæ•´çš„è©•ä¼°æŒ‡æ¨™
6. ææ‡‚ Precisionã€Recallã€F1-Score å„ä»£è¡¨ä»€éº¼
7. ç†è§£ PR Curve å’Œ ROC Curve çš„å·®ç•°å’Œç”¨é€”
8. ç”¨ `GridSearchCV` åšè¶…åƒæ•¸æœå°‹
9. å»ºç«‹ã€Œä¸èƒ½åªè·‘ä¸€æ¬¡ã€ä¸èƒ½åªçœ‹ accuracyã€çš„æ­£ç¢ºè§€å¿µ

---

## æ•…äº‹ï¼šé‚£å€‹ã€Œæº–ç¢ºç‡ 95%ã€çš„æ¨¡å‹

å°æ˜åœ¨å…¬å¸è£¡è¨“ç·´äº†ä¸€å€‹æ¨¡å‹ï¼Œèˆˆé«˜é‡‡çƒˆåœ°è·Ÿä¸»ç®¡å ±å‘Šï¼š

> ã€Œä¸»ç®¡ï¼æˆ‘çš„æ¨¡å‹æº–ç¢ºç‡ 95%ï¼ã€

ä¸»ç®¡å•ï¼šã€Œä½ æ€éº¼è©•ä¼°çš„ï¼Ÿã€

> ã€Œæˆ‘åˆ‡äº† 70% è¨“ç·´ã€30% æ¸¬è©¦ï¼Œæ¸¬è©¦é›†æº–ç¢ºç‡ 95%ï¼ã€

ä¸»ç®¡åˆå•ï¼šã€Œä½ åªè·‘äº†ä¸€æ¬¡ï¼Ÿã€

> ã€Œ...å°ã€‚ã€

ä¸»ç®¡æ–æ–é ­ï¼šã€Œå¦‚æœä½ æ›å€‹ `random_state`ï¼Œæº–ç¢ºç‡å¯èƒ½æ‰åˆ° 85%ã€‚
ä½ æ€éº¼çŸ¥é“ 95% ä¸æ˜¯é‹æ°£å¥½ï¼Ÿã€

```
        random_state=42     random_state=123    random_state=7

        æº–ç¢ºç‡: 95%          æº–ç¢ºç‡: 87%          æº–ç¢ºç‡: 91%
              ^
              |
              ä½ åªçœ‹äº†é€™æ¬¡ï¼Œ
              ç„¶å¾Œå°±è·Ÿä¸»ç®¡èªª 95% äº†
```

**é€™å°±æ˜¯ç‚ºä»€éº¼æˆ‘å€‘éœ€è¦äº¤å‰é©—è­‰ã€‚**

---

## ğŸ’¡ é‡é»è§€å¿µï¼šç‚ºä»€éº¼å–®æ¬¡ Train/Test Split ä¸å¤ 

```
å•é¡Œ 1ï¼šçµæœå–æ±ºæ–¼ã€Œæ€éº¼åˆ‡ã€
  â†’ ä¸åŒçš„ random_state æœƒå¾—åˆ°ä¸åŒçš„çµæœ
  â†’ ä½ çœ‹åˆ°çš„å¯èƒ½æ˜¯ã€Œæœ€å¥½çš„æƒ…æ³ã€æˆ–ã€Œæœ€å·®çš„æƒ…æ³ã€

å•é¡Œ 2ï¼šæµªè²»è³‡æ–™
  â†’ 30% çš„è³‡æ–™åªç”¨ä¾†æ¸¬è©¦ï¼Œæ²’æœ‰åƒèˆ‡è¨“ç·´
  â†’ è³‡æ–™é‡å°‘çš„æ™‚å€™ç‰¹åˆ¥å¯æƒœ

å•é¡Œ 3ï¼šç„¡æ³•ä¼°è¨ˆã€Œä¸ç¢ºå®šæ€§ã€
  â†’ å–®æ¬¡çµæœç„¡æ³•å‘Šè¨´ä½ æ¨¡å‹è¡¨ç¾çš„ã€Œç©©å®šåº¦ã€
  â†’ ä½ ä¸çŸ¥é“ 95% çš„èƒŒå¾Œæœ‰å¤šå°‘è®Šç•°
```

---

## K-Fold Cross Validationï¼šè·‘ K æ¬¡ï¼Œå–å¹³å‡

K-Fold çš„åšæ³•å¾ˆç›´è¦ºï¼š

1. æŠŠè³‡æ–™åˆ†æˆ K ç­‰ä»½ï¼ˆé€šå¸¸ K=5 æˆ– K=10ï¼‰
2. æ¯æ¬¡ç”¨ K-1 ä»½åšè¨“ç·´ï¼Œå‰©ä¸‹ 1 ä»½åšæ¸¬è©¦
3. é‡è¤‡ K æ¬¡ï¼Œæ¯ä»½è³‡æ–™éƒ½ç•¶éä¸€æ¬¡æ¸¬è©¦é›†
4. å– K æ¬¡çµæœçš„**å¹³å‡å€¼**å’Œ**æ¨™æº–å·®**

```
5-Fold Cross Validationï¼š

Fold 1: [TEST ] [Train] [Train] [Train] [Train]  â†’ æº–ç¢ºç‡: 0.93
Fold 2: [Train] [TEST ] [Train] [Train] [Train]  â†’ æº–ç¢ºç‡: 0.95
Fold 3: [Train] [Train] [TEST ] [Train] [Train]  â†’ æº–ç¢ºç‡: 0.91
Fold 4: [Train] [Train] [Train] [TEST ] [Train]  â†’ æº–ç¢ºç‡: 0.94
Fold 5: [Train] [Train] [Train] [Train] [TEST ]  â†’ æº–ç¢ºç‡: 0.92

å¹³å‡: 0.930 Â± 0.015
```

ç¾åœ¨ä½ å¯ä»¥èªªï¼šã€Œæˆ‘çš„æ¨¡å‹æº–ç¢ºç‡æ˜¯ **93.0% Â± 1.5%**ã€â€”
é€™æ¯”èªªã€Œ95%ã€æœ‰æ„ç¾©å¤šäº†ï¼

---

## ç”¨ scikit-learn åšäº¤å‰é©—è­‰

```python
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris

# è¼‰å…¥è³‡æ–™
iris = load_iris()
X, y = iris.data, iris.target

# å»ºç«‹æ¨¡å‹ï¼ˆä¸éœ€è¦å…ˆ fitï¼cross_val_score æœƒè‡ªå‹•åšï¼‰
rf = RandomForestClassifier(n_estimators=100, random_state=42)

# 5-Fold äº¤å‰é©—è­‰
scores = cross_val_score(rf, X, y, cv=5, scoring='accuracy')

print(f"æ¯ä¸€æŠ˜çš„æº–ç¢ºç‡: {scores}")
print(f"å¹³å‡æº–ç¢ºç‡:     {scores.mean():.4f}")
print(f"æ¨™æº–å·®:         {scores.std():.4f}")
print(f"å ±å‘Š:           {scores.mean():.4f} Â± {scores.std():.4f}")
```

è¼¸å‡ºå¯èƒ½åƒé€™æ¨£ï¼š

```
æ¯ä¸€æŠ˜çš„æº–ç¢ºç‡: [0.9667 0.9667 0.9333 0.9667 1.0000]
å¹³å‡æº–ç¢ºç‡:     0.9667
æ¨™æº–å·®:         0.0211
å ±å‘Š:           0.9667 Â± 0.0211
```

å°±é€™éº¼ç°¡å–®ï¼ä¸€è¡Œ `cross_val_score` å°±æå®šäº†ã€‚

---

## ğŸ§  å‹•å‹•è…¦

å¦‚æœ K = Nï¼ˆN æ˜¯è³‡æ–™ç¸½æ•¸ï¼‰ï¼Œé‚£æ¯æ¬¡åªç”¨ 1 ç­†è³‡æ–™åšæ¸¬è©¦ã€‚
é€™å«åš **Leave-One-Out Cross Validation (LOOCV)**ã€‚

æƒ³æƒ³çœ‹ï¼šLOOCV çš„å„ªé»å’Œç¼ºé»åˆ†åˆ¥æ˜¯ä»€éº¼ï¼Ÿ
ï¼ˆæç¤ºï¼šæœ€å……åˆ†åˆ©ç”¨è³‡æ–™ vs. è¨ˆç®—æˆæœ¬ï¼‰

---

## Stratified K-Foldï¼šè™•ç†é¡åˆ¥ä¸å¹³è¡¡

æ™®é€šçš„ K-Fold æœ‰ä¸€å€‹å•é¡Œï¼šå¦‚æœé¡åˆ¥åˆ†å¸ƒä¸å‡ï¼ŒæŸä¸€æŠ˜å¯èƒ½å…¨æ˜¯åŒä¸€é¡ã€‚

```
å‡è¨­è³‡æ–™æœ‰ 90% æ˜¯é¡åˆ¥ Aï¼Œ10% æ˜¯é¡åˆ¥ Bï¼š

æ™®é€š K-Fold å¯èƒ½çš„åˆ‡æ³•ï¼ˆé‹æ°£ä¸å¥½çš„è©±ï¼‰ï¼š
Fold 1: [AAAA] [AABA] [AABA] [AABA] [ABBB]
         â† é€™ä¸€æŠ˜å¹¾ä¹æ²’æœ‰ Bï¼æ¸¬å‡ºä¾†çš„çµæœä¸æº–ï¼

Stratified K-Fold ä¿è­‰æ¯ä¸€æŠ˜çš„é¡åˆ¥æ¯”ä¾‹éƒ½ä¸€æ¨£ï¼š
Fold 1: [AAB ] [AAB ] [AAB ] [AAB ] [AAB ]
         â† æ¯ä¸€æŠ˜éƒ½æœ‰ 90% A, 10% Bï¼Œå…¬å¹³ï¼
```

```python
from sklearn.model_selection import StratifiedKFold, cross_val_score

# æ–¹æ³• 1ï¼šç”¨ StratifiedKFold ç‰©ä»¶
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(rf, X, y, cv=skf, scoring='accuracy')

# æ–¹æ³• 2ï¼šå…¶å¯¦ cross_val_score å°åˆ†é¡å•é¡Œé è¨­å°±ç”¨ StratifiedKFoldï¼
# æ‰€ä»¥ cv=5 ç­‰åŒæ–¼ StratifiedKFold(n_splits=5)
scores = cross_val_score(rf, X, y, cv=5, scoring='accuracy')  # è‡ªå‹• Stratified
```

ğŸ’¡ **å¥½æ¶ˆæ¯ï¼š`cross_val_score` å°åˆ†é¡å•é¡Œé è¨­å°±ä½¿ç”¨ StratifiedKFoldã€‚**
ä½ ä¸éœ€è¦ç‰¹åˆ¥è¨­å®šï¼

---

## ğŸ’¡ é‡é»è§€å¿µï¼šä¸èƒ½åªçœ‹ Accuracy

æº–ç¢ºç‡ï¼ˆAccuracyï¼‰æœ‰ä¸€å€‹åš´é‡çš„ç›²é»ã€‚çœ‹é€™å€‹ä¾‹å­ï¼š

```
ä¿¡ç”¨å¡è©é¨™åµæ¸¬ï¼š
  æ­£å¸¸äº¤æ˜“: 9,900 ç­† (99%)
  è©é¨™äº¤æ˜“:   100 ç­† (1%)

å¦‚æœæˆ‘çš„æ¨¡å‹ã€Œæ°¸é é æ¸¬æ­£å¸¸ã€ï¼š
  æº–ç¢ºç‡ = 9900 / 10000 = 99%  â† å“‡ï¼99%ï¼

ä½†æ˜¯...å®ƒä¸€ç­†è©é¨™éƒ½æŠ“ä¸åˆ°ï¼é€™å€‹æ¨¡å‹æ ¹æœ¬æ²’ç”¨ï¼
```

æ‰€ä»¥æˆ‘å€‘éœ€è¦æ›´å¤šå…ƒçš„è©•ä¼°æŒ‡æ¨™ã€‚

---

## Classification Reportï¼šä¸€æ¬¡çœ‹æ¸…æ‰€æœ‰æŒ‡æ¨™

```python
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_breast_cancer

# è¼‰å…¥ä¹³ç™Œè³‡æ–™é›†
cancer = load_breast_cancer()
X, y = cancer.data, cancer.target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)

print(classification_report(y_test, y_pred, target_names=['æƒ¡æ€§', 'è‰¯æ€§']))
```

è¼¸å‡ºï¼š

```
              precision    recall  f1-score   support

          æƒ¡æ€§       0.95      0.95      0.95        63
          è‰¯æ€§       0.97      0.97      0.97       108

    accuracy                           0.96       171
   macro avg       0.96      0.96      0.96       171
weighted avg       0.96      0.96      0.96       171
```

### é€™äº›æŒ‡æ¨™åˆ°åº•åœ¨è¬›ä»€éº¼ï¼Ÿ

ç”¨ä¸€å€‹æ¯”å–»ä¾†è§£é‡‹ï¼š

```
ä½ æ˜¯ä¸€å€‹æ©Ÿå ´å®‰æª¢å“¡ï¼Œè¦æ‰¾å‡ºè¡Œæä¸­çš„é•ç¦å“ã€‚

                    å¯¦éš›æœ‰é•ç¦å“    å¯¦éš›æ²’æœ‰é•ç¦å“
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  ä½ èªªã€Œæœ‰ã€       â”‚ TP (æŠ“åˆ°äº†!) â”‚ FP (èª¤å ±å†¤æ‰) â”‚
                   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  ä½ èªªã€Œæ²’æœ‰ã€     â”‚ FN (æ¼æ‰äº†!) â”‚ TN (æ­£ç¢ºæ”¾è¡Œ) â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TP = True Positive   (æœ‰ï¼Œä½ èªªæœ‰ âœ…)
FP = False Positive  (æ²’æœ‰ï¼Œä½ èªªæœ‰ âŒ)  â† å†¤æ‰å¥½äºº
FN = False Negative  (æœ‰ï¼Œä½ èªªæ²’æœ‰ âŒ)  â† æ”¾éå£äººï¼
TN = True Negative   (æ²’æœ‰ï¼Œä½ èªªæ²’æœ‰ âœ…)
```

```
Precisionï¼ˆç²¾ç¢ºç‡ï¼‰= TP / (TP + FP)
  ã€Œä½ èªªæ˜¯é•ç¦å“çš„æ±è¥¿ä¸­ï¼Œæœ‰å¤šå°‘çœŸçš„æ˜¯ï¼Ÿã€
  â†’ é«˜ Precision = å¾ˆå°‘å†¤æ‰å¥½äºº

Recallï¼ˆå¬å›ç‡ï¼‰= TP / (TP + FN)
  ã€Œæ‰€æœ‰é•ç¦å“ä¸­ï¼Œä½ æŠ“åˆ°äº†å¤šå°‘ï¼Ÿã€
  â†’ é«˜ Recall = å¾ˆå°‘æ¼æ‰å£äºº

F1-Score = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
  â†’ Precision å’Œ Recall çš„èª¿å’Œå¹³å‡æ•¸
  â†’ å…©è€…çš„å¹³è¡¡æŒ‡æ¨™
```

---

## âš ï¸ å¸¸è¦‹é™·é˜±ï¼šPrecision vs. Recall çš„å–æ¨

ä½ ä¸å¯èƒ½åŒæ™‚è®“ Precision å’Œ Recall éƒ½é”åˆ° 100%ã€‚
å®ƒå€‘ä¹‹é–“å­˜åœ¨**æ¬Šè¡¡ï¼ˆtrade-offï¼‰**ï¼š

```
æé«˜é–€æª»ï¼ˆä¿å®ˆåˆ¤æ–·ï¼‰ï¼š
  â†’ Precision ä¸Šå‡ â†‘ï¼ˆæ›´å°‘å†¤æ‰ï¼‰
  â†’ Recall ä¸‹é™ â†“ï¼ˆæ›´å¤šæ¼ç¶²ï¼‰

é™ä½é–€æª»ï¼ˆå¯¬é¬†åˆ¤æ–·ï¼‰ï¼š
  â†’ Precision ä¸‹é™ â†“ï¼ˆæ›´å¤šå†¤æ‰ï¼‰
  â†’ Recall ä¸Šå‡ â†‘ï¼ˆæ›´å°‘æ¼ç¶²ï¼‰
```

**è©²é‡è¦– Precision é‚„æ˜¯ Recallï¼Ÿå–æ±ºæ–¼ä½ çš„æ‡‰ç”¨å ´æ™¯ï¼š**

```
+--------------------+-------------+-------------+
| å ´æ™¯               | é‡è¦–å“ªå€‹    | ç‚ºä»€éº¼      |
+--------------------+-------------+-------------+
| ç™Œç—‡è¨ºæ–·           | Recall      | ä¸èƒ½æ¼æ‰ç—…äºº|
| åƒåœ¾ä¿¡ä»¶éæ¿¾       | Precision   | ä¸èƒ½èª¤åˆªä¿¡  |
| è©é¨™åµæ¸¬           | Recall      | ä¸èƒ½æ”¾éé¨™å­|
| æ¨è–¦ç³»çµ±           | Precision   | æ¨çˆ›çš„æ›´ç…©  |
| è‡ªé§•è»Šè¡Œäººåµæ¸¬     | Recall      | ä¸èƒ½æ¼æ‰è¡Œäºº|
+--------------------+-------------+-------------+
```

---

## PR Curve å’Œ ROC Curve

### PR Curveï¼ˆPrecision-Recall Curveï¼‰

PR Curve é¡¯ç¤º Precision å’Œ Recall åœ¨ä¸åŒé–€æª»ä¸‹çš„é—œä¿‚ã€‚

```python
from sklearn.metrics import precision_recall_curve, average_precision_score
import matplotlib.pyplot as plt

# å–å¾—æ©Ÿç‡é æ¸¬ï¼ˆè€Œä¸æ˜¯åˆ†é¡çµæœï¼‰
y_scores = rf.predict_proba(X_test)[:, 1]

# è¨ˆç®— PR Curve
precision, recall, thresholds = precision_recall_curve(y_test, y_scores)
ap = average_precision_score(y_test, y_scores)

# ç•«åœ–
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, 'b-', linewidth=2, label=f'RF (AP={ap:.3f})')
plt.xlabel('Recallï¼ˆå¬å›ç‡ï¼‰')
plt.ylabel('Precisionï¼ˆç²¾ç¢ºç‡ï¼‰')
plt.title('Precision-Recall Curve')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
```

```
PR Curve ç¤ºæ„åœ–ï¼š

Precision
   1.0 |*****
       |     ****
       |         ***
   0.5 |            ***
       |               ****
       |                   *****
   0.0 +-------------------------
       0.0              0.5    1.0
                    Recall

â†’ æ›²ç·šè¶Šé è¿‘å³ä¸Šè§’è¶Šå¥½
â†’ AP (Average Precision) è¶Šé«˜è¶Šå¥½
â†’ åœ¨é¡åˆ¥ä¸å¹³è¡¡æ™‚æ¯” ROC æ›´æœ‰åƒè€ƒåƒ¹å€¼
```

### ROC Curveï¼ˆReceiver Operating Characteristic Curveï¼‰

ROC Curve é¡¯ç¤º True Positive Rate å’Œ False Positive Rate çš„é—œä¿‚ã€‚

```python
from sklearn.metrics import roc_curve, roc_auc_score

# è¨ˆç®— ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_scores)
auc = roc_auc_score(y_test, y_scores)

# ç•«åœ–
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, 'b-', linewidth=2, label=f'RF (AUC={auc:.3f})')
plt.plot([0, 1], [0, 1], 'r--', label='éš¨æ©ŸçŒœæ¸¬ (AUC=0.5)')
plt.xlabel('False Positive Rateï¼ˆå½é™½ç‡ï¼‰')
plt.ylabel('True Positive Rateï¼ˆçœŸé™½ç‡ï¼‰')
plt.title('ROC Curve')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
```

```
ROC Curve ç¤ºæ„åœ–ï¼š

TPR (True Positive Rate)
   1.0 |        ********
       |      **
       |    **
   0.5 |  **     /
       | *      / â† éš¨æ©ŸçŒœæ¸¬ï¼ˆå°è§’ç·šï¼‰
       |*      /
   0.0 +------/----------
       0.0   0.5       1.0
       FPR (False Positive Rate)

â†’ æ›²ç·šè¶Šé è¿‘å·¦ä¸Šè§’è¶Šå¥½
â†’ AUC (Area Under Curve) è¶Šæ¥è¿‘ 1.0 è¶Šå¥½
â†’ AUC = 0.5 è¡¨ç¤ºè·Ÿéš¨æ©ŸçŒœæ¸¬ä¸€æ¨£å·®
```

### PR Curve vs. ROC Curveï¼šä»€éº¼æ™‚å€™ç”¨å“ªå€‹ï¼Ÿ

```
+---------------------+----------------------------+
| PR Curve            | ROC Curve                  |
+---------------------+----------------------------+
| é¡åˆ¥ä¸å¹³è¡¡æ™‚æ›´æ•æ„Ÿ  | é¡åˆ¥å¹³è¡¡æ™‚è¡¨ç¾å¥½           |
| é—œæ³¨æ­£é¡åˆ¥çš„è¡¨ç¾    | ç¶œåˆçœ‹å…©å€‹é¡åˆ¥             |
| é†«ç™‚ã€è©é¨™åµæ¸¬     | ä¸€èˆ¬åˆ†é¡å•é¡Œ               |
| AP æ˜¯é—œéµæŒ‡æ¨™       | AUC æ˜¯é—œéµæŒ‡æ¨™             |
+---------------------+----------------------------+

ç¶“é©—æ³•å‰‡ï¼š
  é¡åˆ¥ä¸å¹³è¡¡ â†’ ç”¨ PR Curve
  é¡åˆ¥å¹³è¡¡   â†’ ç”¨ ROC Curve
  ä¸ç¢ºå®š     â†’ å…©å€‹éƒ½çœ‹
```

---

## å…¬å¹³æ¯”è¼ƒå¤šå€‹æ¨¡å‹

äº¤å‰é©—è­‰æœ€æ£’çš„ç”¨é€”ä¹‹ä¸€ï¼š**å…¬å¹³åœ°æ¯”è¼ƒä¸åŒçš„æ¨¡å‹ã€‚**

```python
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
from sklearn.datasets import load_breast_cancer
import numpy as np

# è¼‰å…¥è³‡æ–™
cancer = load_breast_cancer()
X, y = cancer.data, cancer.target

# å®šç¾©è¦æ¯”è¼ƒçš„æ¨¡å‹
models = {
    'Logistic Regression': LogisticRegression(max_iter=10000),
    'Decision Tree':       DecisionTreeClassifier(random_state=42),
    'Random Forest':       RandomForestClassifier(n_estimators=100, random_state=42),
    'SVM':                 SVC(random_state=42),
    'KNN':                 KNeighborsClassifier(),
}

# ç”¨ 5-Fold äº¤å‰é©—è­‰æ¯”è¼ƒ
print(f"{'æ¨¡å‹':<25} {'å¹³å‡æº–ç¢ºç‡':>10} {'æ¨™æº–å·®':>10}")
print("=" * 50)

results = {}
for name, model in models.items():
    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
    results[name] = scores
    print(f"{name:<25} {scores.mean():>10.4f} {scores.std():>10.4f}")
```

å¯èƒ½çš„è¼¸å‡ºï¼š

```
æ¨¡å‹                          å¹³å‡æº–ç¢ºç‡       æ¨™æº–å·®
==================================================
Logistic Regression           0.9508     0.0260
Decision Tree                 0.9226     0.0229
Random Forest                 0.9614     0.0200
SVM                           0.9192     0.0350
KNN                           0.9261     0.0193
```

ç¾åœ¨ä½ å¯ä»¥æœ‰ä¿¡å¿ƒåœ°èªªï¼šã€Œåœ¨é€™å€‹è³‡æ–™é›†ä¸Šï¼ŒRandom Forest è¡¨ç¾æœ€å¥½ï¼Œ
è€Œä¸”æ¨™æº–å·®ä¹Ÿæœ€å°ï¼Œä»£è¡¨å®ƒæœ€ç©©å®šã€‚ã€

---

## ğŸ§  å‹•å‹•è…¦

åœ¨ä¸Šé¢çš„æ¯”è¼ƒä¸­ï¼ŒSVM çš„æº–ç¢ºç‡æœ€ä½ï¼ˆ0.9192ï¼‰ï¼Œæ¨™æº–å·®ä¹Ÿæœ€å¤§ï¼ˆ0.0350ï¼‰ã€‚
ä½† SVM ç†è«–ä¸Šæ˜¯å¾ˆå¼·çš„æ¼”ç®—æ³•ã€‚ç‚ºä»€éº¼åœ¨é€™è£¡è¡¨ç¾é€™éº¼å·®ï¼Ÿ

ï¼ˆæç¤ºï¼šSVM å°ç‰¹å¾µå°ºåº¦**éå¸¸**æ•æ„Ÿã€‚æˆ‘å€‘æœ‰åš `StandardScaler` å—ï¼Ÿï¼‰

---

## GridSearchCVï¼šè‡ªå‹•æ‰¾æœ€ä½³è¶…åƒæ•¸

æ‰‹å‹•èª¿åƒå¤ªç´¯äº†ã€‚`GridSearchCV` æœƒå¹«ä½ **çª®èˆ‰æ‰€æœ‰è¶…åƒæ•¸çµ„åˆ**ï¼Œ
ç”¨äº¤å‰é©—è­‰æ‰¾å‡ºæœ€å¥½çš„é‚£çµ„ã€‚

```python
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_breast_cancer

# è¼‰å…¥è³‡æ–™
cancer = load_breast_cancer()
X, y = cancer.data, cancer.target

# å®šç¾©è¦æœå°‹çš„è¶…åƒæ•¸ç©ºé–“
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 10, None],
    'min_samples_leaf': [1, 2, 5],
    'max_features': ['sqrt', 'log2'],
}

# è¨ˆç®—ç¸½å…±è¦å˜—è©¦å¹¾ç¨®çµ„åˆ
n_combinations = 3 * 4 * 3 * 2  # = 72 ç¨®çµ„åˆ
print(f"å…± {n_combinations} ç¨®çµ„åˆ Ã— 5 æŠ˜ = {n_combinations * 5} æ¬¡è¨“ç·´")

# å»ºç«‹ GridSearchCV
grid_search = GridSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_grid=param_grid,
    cv=5,                  # 5-Fold äº¤å‰é©—è­‰
    scoring='accuracy',    # è©•ä¼°æŒ‡æ¨™
    n_jobs=-1,             # å¹³è¡ŒåŒ–ï¼
    verbose=1              # é¡¯ç¤ºé€²åº¦
)

# åŸ·è¡Œæœå°‹
grid_search.fit(X, y)

# æŸ¥çœ‹çµæœ
print(f"\næœ€ä½³åƒæ•¸: {grid_search.best_params_}")
print(f"æœ€ä½³äº¤å‰é©—è­‰æº–ç¢ºç‡: {grid_search.best_score_:.4f}")
```

```
GridSearchCV çš„é‹ä½œæ–¹å¼ï¼š

è¶…åƒæ•¸ç©ºé–“ï¼š
  n_estimators:   [50, 100, 200]
  max_depth:      [3, 5, 10, None]
  min_samples_leaf: [1, 2, 5]
  max_features:   ['sqrt', 'log2']

  â†’ 3 Ã— 4 Ã— 3 Ã— 2 = 72 ç¨®çµ„åˆ

æ¯ç¨®çµ„åˆåš 5-Fold CVï¼š
  â†’ 72 Ã— 5 = 360 æ¬¡æ¨¡å‹è¨“ç·´

æ‰¾å‡ºå¹³å‡æº–ç¢ºç‡æœ€é«˜çš„é‚£çµ„ï¼š
  â†’ æœ€ä½³åƒæ•¸: {'max_depth': 10, 'max_features': 'sqrt',
               'min_samples_leaf': 1, 'n_estimators': 200}
  â†’ æœ€ä½³æº–ç¢ºç‡: 0.9649
```

### ç”¨æœ€ä½³æ¨¡å‹åšé æ¸¬

```python
# grid_search.best_estimator_ å°±æ˜¯ç”¨æœ€ä½³åƒæ•¸è¨“ç·´å¥½çš„æ¨¡å‹
best_model = grid_search.best_estimator_

# ç›´æ¥ç”¨å®ƒä¾†é æ¸¬
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# é‡æ–°ç”¨æœ€ä½³åƒæ•¸åœ¨è¨“ç·´é›†ä¸Šè¨“ç·´
best_model.fit(X_train, y_train)
print(f"æ¸¬è©¦é›†æº–ç¢ºç‡: {best_model.score(X_test, y_test):.4f}")
```

---

## âš ï¸ å¸¸è¦‹é™·é˜±

### é™·é˜± 1ï¼šç”¨æ¸¬è©¦é›†ä¾†é¸æ¨¡å‹

```
âŒ éŒ¯èª¤åšæ³•ï¼š
  1. åˆ‡ train/test
  2. åœ¨ train ä¸Šè¨“ç·´å¤šå€‹æ¨¡å‹
  3. åœ¨ test ä¸Šé¸è¡¨ç¾æœ€å¥½çš„ â† é€™ç­‰æ–¼ç”¨ test ä¾†åšæ±ºç­–ï¼
  4. å ±å‘Š test ä¸Šçš„çµæœ â† çµæœåæ¨‚è§€ï¼

âœ… æ­£ç¢ºåšæ³•ï¼š
  1. åˆ‡ train/test
  2. åœ¨ train ä¸Šç”¨äº¤å‰é©—è­‰é¸æœ€å¥½çš„æ¨¡å‹å’Œè¶…åƒæ•¸
  3. æœ€å¾Œç”¨ test åšæœ€çµ‚è©•ä¼°ï¼ˆåªçœ‹ä¸€æ¬¡ï¼ï¼‰
  4. å ±å‘Š test ä¸Šçš„çµæœ â† å…¬æ­£ç„¡åçš„çµæœ
```

### é™·é˜± 2ï¼šåœ¨æ•´å€‹è³‡æ–™é›†ä¸Šåš GridSearchCV ç„¶å¾Œå ±å‘Š best_score_

```
âš ï¸ å¦‚æœä½ å°æ•´å€‹è³‡æ–™é›†ï¼ˆåŒ…å«æ¸¬è©¦é›†ï¼‰åš GridSearchCVï¼Œ
   best_score_ å°±æœ‰ã€Œè³‡è¨Šæ´©æ¼ã€çš„é¢¨éšªã€‚

æ­£ç¢ºæµç¨‹ï¼š
  1. å…ˆåˆ‡å‡ºæ¸¬è©¦é›†ï¼ˆä¸ç¢°å®ƒï¼‰
  2. åœ¨è¨“ç·´é›†ä¸Šåš GridSearchCV
  3. ç”¨æœ€ä½³æ¨¡å‹åœ¨æ¸¬è©¦é›†ä¸Šåšæœ€çµ‚è©•ä¼°
```

### é™·é˜± 3ï¼šGridSearchCV çš„è¨ˆç®—é‡çˆ†ç‚¸

```
å¦‚æœä½ æœ‰ 5 å€‹è¶…åƒæ•¸ï¼Œæ¯å€‹æœ‰ 5 å€‹å€™é¸å€¼ï¼š
  5^5 = 3125 ç¨®çµ„åˆ
  Ã— 5 æŠ˜ = 15,625 æ¬¡è¨“ç·´ï¼

è§£æ±ºæ–¹æ¡ˆï¼š
  â†’ RandomizedSearchCVï¼šéš¨æ©ŸæŠ½æ¨£è¶…åƒæ•¸çµ„åˆ
  â†’ å…ˆç”¨ç²—æœå°‹ç¸®å°ç¯„åœï¼Œå†ç”¨ç´°æœå°‹ç²¾èª¿
  â†’ ç”¨æ›´å°‘çš„æŠ˜æ•¸ï¼ˆcv=3 è€Œä¸æ˜¯ cv=10ï¼‰
```

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# ç”¨éš¨æ©Ÿåˆ†å¸ƒå®šç¾©è¶…åƒæ•¸ç©ºé–“
param_distributions = {
    'n_estimators': randint(50, 500),
    'max_depth': randint(3, 20),
    'min_samples_leaf': randint(1, 10),
    'max_features': ['sqrt', 'log2'],
}

# RandomizedSearchCVï¼šåªéš¨æ©Ÿè©¦ 50 ç¨®çµ„åˆ
random_search = RandomizedSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_distributions=param_distributions,
    n_iter=50,             # åªè©¦ 50 ç¨®ï¼ˆè€Œä¸æ˜¯çª®èˆ‰æ‰€æœ‰ï¼‰
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42
)

random_search.fit(X, y)
print(f"æœ€ä½³åƒæ•¸: {random_search.best_params_}")
print(f"æœ€ä½³æº–ç¢ºç‡: {random_search.best_score_:.4f}")
```

---

## å®Œæ•´çš„æ¨¡å‹è©•ä¼°å·¥ä½œæµç¨‹

æŠŠé€™ä¸€ç« å­¸åˆ°çš„æ‰€æœ‰æ±è¥¿ä¸²èµ·ä¾†ï¼š

```python
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import (
    train_test_split, cross_val_score, GridSearchCV, StratifiedKFold
)
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score

# ===== Step 1: è¼‰å…¥è³‡æ–™ä¸¦åˆ‡å‡ºæœ€çµ‚æ¸¬è©¦é›† =====
cancer = load_breast_cancer()
X, y = cancer.data, cancer.target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"è¨“ç·´é›†: {X_train.shape[0]} ç­†")
print(f"æ¸¬è©¦é›†: {X_test.shape[0]} ç­†\n")

# ===== Step 2: ç”¨äº¤å‰é©—è­‰æ¯”è¼ƒå€™é¸æ¨¡å‹ =====
candidates = {
    'Logistic Regression': LogisticRegression(max_iter=10000),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
}

print("=== å€™é¸æ¨¡å‹æ¯”è¼ƒï¼ˆ5-Fold CVï¼‰===")
print(f"{'æ¨¡å‹':<25} {'å¹³å‡æº–ç¢ºç‡':>10} {'æ¨™æº–å·®':>8}")
print("-" * 45)

for name, model in candidates.items():
    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
    print(f"{name:<25} {scores.mean():>10.4f} {scores.std():>8.4f}")

# ===== Step 3: å°æœ€ä½³å€™é¸æ¨¡å‹åšè¶…åƒæ•¸èª¿æ•´ =====
print("\n=== GridSearchCV è¶…åƒæ•¸æœå°‹ ===")

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [5, 10, None],
    'min_samples_leaf': [1, 2, 5],
}

grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid=param_grid,
    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
    scoring='accuracy',
    n_jobs=-1
)
grid_search.fit(X_train, y_train)

print(f"æœ€ä½³åƒæ•¸: {grid_search.best_params_}")
print(f"æœ€ä½³ CV æº–ç¢ºç‡: {grid_search.best_score_:.4f}")

# ===== Step 4: æœ€çµ‚è©•ä¼°ï¼ˆåªåœ¨æ¸¬è©¦é›†ä¸Šè·‘ä¸€æ¬¡ï¼ï¼‰=====
print("\n=== æœ€çµ‚è©•ä¼°ï¼ˆæ¸¬è©¦é›†ï¼‰===")

best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
y_proba = best_model.predict_proba(X_test)[:, 1]

print(classification_report(
    y_test, y_pred,
    target_names=['æƒ¡æ€§', 'è‰¯æ€§']
))
print(f"ROC AUC: {roc_auc_score(y_test, y_proba):.4f}")
```

---

## â“ æ²’æœ‰ç¬¨å•é¡Œ

**Q: K-Fold çš„ K è¦è¨­å¤šå°‘ï¼Ÿ**
A: æœ€å¸¸è¦‹çš„æ˜¯ K=5 æˆ– K=10ã€‚K=5 æ¯”è¼ƒå¿«ï¼ŒK=10 æ¯”è¼ƒæº–ã€‚
è³‡æ–™é‡å¾ˆå°‘æ™‚å¯ä»¥ç”¨ K=10 ç”šè‡³ LOOCVã€‚è³‡æ–™é‡å¾ˆå¤§æ™‚ K=5 å°±å¤ äº†ã€‚

**Q: ç‚ºä»€éº¼ `cross_val_score` ä¸éœ€è¦å…ˆ `fit`ï¼Ÿ**
A: å› ç‚º `cross_val_score` å…§éƒ¨æœƒè‡ªå‹•åš K æ¬¡çš„ fit å’Œ predictã€‚
ä½ å‚³é€²å»çš„æ¨¡å‹ç‰©ä»¶åªæ˜¯ä¸€å€‹ã€Œæ¨¡æ¿ã€ï¼Œå®ƒæœƒè¢«è¤‡è£½ K æ¬¡ã€‚

**Q: `scoring='accuracy'` å¯ä»¥æ›æˆå…¶ä»–æŒ‡æ¨™å—ï¼Ÿ**
A: å¯ä»¥ï¼å¸¸ç”¨çš„æœ‰ï¼š
- `'f1'` æˆ– `'f1_macro'`ï¼šF1 åˆ†æ•¸
- `'roc_auc'`ï¼šROC AUCï¼ˆéœ€è¦äºŒåˆ†é¡ï¼‰
- `'precision'`ã€`'recall'`
- `'neg_mean_squared_error'`ï¼šè¿´æ­¸å•é¡Œç”¨

**Q: GridSearchCV å’Œ RandomizedSearchCV æ€éº¼é¸ï¼Ÿ**
A: è¶…åƒæ•¸ç©ºé–“å°ï¼ˆ<100 ç¨®çµ„åˆï¼‰ç”¨ GridSearchCVã€‚
è¶…åƒæ•¸ç©ºé–“å¤§ï¼ˆ>100 ç¨®çµ„åˆï¼‰ç”¨ RandomizedSearchCVã€‚

**Q: äº¤å‰é©—è­‰çš„çµæœå’Œæœ€çµ‚æ¸¬è©¦é›†çš„çµæœå·®å¾ˆå¤šæ€éº¼è¾¦ï¼Ÿ**
A: å¦‚æœ CV çµæœæ¯”æ¸¬è©¦é›†å¥½å¾ˆå¤šï¼Œå¯èƒ½æ˜¯ CV çš„éç¨‹ä¸­æœ‰è³‡è¨Šæ´©æ¼ã€‚
å¦‚æœ CV çµæœæ¯”æ¸¬è©¦é›†å·®ï¼Œå¯èƒ½åªæ˜¯æ¸¬è©¦é›†ã€Œå‰›å¥½æ¯”è¼ƒå®¹æ˜“ã€ã€‚
å·®ç•°åœ¨ 2-3% ä»¥å…§é€šå¸¸æ˜¯æ­£å¸¸çš„ã€‚

**Q: ç‚ºä»€éº¼è¦ `stratify=y`ï¼Ÿ**
A: ç¢ºä¿è¨“ç·´é›†å’Œæ¸¬è©¦é›†çš„é¡åˆ¥æ¯”ä¾‹è·ŸåŸå§‹è³‡æ–™ä¸€æ¨£ã€‚
å¦‚æœä¸é€™æ¨£åšï¼Œå¯èƒ½æŸå€‹é¡åˆ¥å…¨è·‘åˆ°æ¸¬è©¦é›†å»äº†ï¼Œè¨“ç·´é›†æ ¹æœ¬å­¸ä¸åˆ°ã€‚

---

## ğŸ“ èª²å¾Œç·´ç¿’

### ç·´ç¿’ 1ï¼šé«”é©—ã€Œåªè·‘ä¸€æ¬¡ã€çš„å±éšª

```python
from sklearn.datasets import load_wine
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

wine = load_wine()
X, y = wine.data, wine.target

# ç”¨ 10 å€‹ä¸åŒçš„ random_state åˆ‡åˆ†è³‡æ–™
# è¨˜éŒ„æ¯æ¬¡çš„æ¸¬è©¦é›†æº–ç¢ºç‡
# è¨ˆç®—å¹³å‡å€¼å’Œæ¨™æº–å·®
# å†ç”¨ cross_val_score åš 10-Fold CV
# æ¯”è¼ƒå…©ç¨®æ–¹å¼çš„çµæœ
```

### ç·´ç¿’ 2ï¼šç”¨ä¸åŒæŒ‡æ¨™åšäº¤å‰é©—è­‰

```python
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

cancer = load_breast_cancer()
X, y = cancer.data, cancer.target

rf = RandomForestClassifier(n_estimators=100, random_state=42)

# ç”¨ä»¥ä¸‹å››ç¨® scoring åšäº¤å‰é©—è­‰ï¼š
# 'accuracy', 'f1', 'precision', 'recall'
# æ¯”è¼ƒçµæœï¼Œå“ªå€‹æŒ‡æ¨™æœ€ä½ï¼Ÿç‚ºä»€éº¼ï¼Ÿ
```

### ç·´ç¿’ 3ï¼šå®Œæ•´çš„ GridSearchCV æµç¨‹

```python
# ç”¨ load_wine è³‡æ–™é›†
# 1. åˆ‡å‡º 20% çš„æ¸¬è©¦é›†ï¼ˆstratify!ï¼‰
# 2. åœ¨è¨“ç·´é›†ä¸Šç”¨ GridSearchCV èª¿æ•´ Random Forest
#    æœå°‹: n_estimators=[50,100,200], max_depth=[3,5,10,None], min_samples_leaf=[1,5]
# 3. å°å‡ºæœ€ä½³åƒæ•¸å’Œæœ€ä½³ CV åˆ†æ•¸
# 4. ç”¨æœ€ä½³æ¨¡å‹åœ¨æ¸¬è©¦é›†ä¸Šåšæœ€çµ‚è©•ä¼°
# 5. å°å‡ºå®Œæ•´çš„ classification_report
```

---

## æœ¬ç« ç¸½çµ

```
+--------------------------------------------------+
|       äº¤å‰é©—è­‰èˆ‡æ¨¡å‹è©•ä¼°æ ¸å¿ƒè§€å¿µ                 |
+--------------------------------------------------+
| 1. ä¸èƒ½åªè·‘ä¸€æ¬¡ â†’ ç”¨ K-Fold äº¤å‰é©—è­‰            |
| 2. ä¸èƒ½åªçœ‹ accuracy â†’ çœ‹ P/R/F1/AUC            |
| 3. StratifiedKFold è™•ç†é¡åˆ¥ä¸å¹³è¡¡                |
| 4. cross_val_score ä¸€è¡Œæå®šäº¤å‰é©—è­‰              |
| 5. GridSearchCV è‡ªå‹•æ‰¾æœ€ä½³è¶…åƒæ•¸                 |
| 6. æ¸¬è©¦é›†åªèƒ½çœ‹ä¸€æ¬¡ï¼ä¸èƒ½ç”¨å®ƒé¸æ¨¡å‹              |
| 7. é¡åˆ¥ä¸å¹³è¡¡ â†’ çœ‹ PR Curve                      |
| 8. é¡åˆ¥å¹³è¡¡ â†’ çœ‹ ROC Curve                       |
+--------------------------------------------------+

æ­£ç¢ºçš„è©•ä¼°æµç¨‹ï¼š

 åŸå§‹è³‡æ–™
    |
    â”œâ”€â”€ è¨“ç·´é›† (80%)
    |      |
    |      â”œâ”€â”€ äº¤å‰é©—è­‰: é¸æ¨¡å‹ + èª¿åƒ
    |      |     (GridSearchCV)
    |      |
    |      â””â”€â”€ æœ€ä½³æ¨¡å‹
    |
    â””â”€â”€ æ¸¬è©¦é›† (20%)
           |
           â””â”€â”€ æœ€çµ‚è©•ä¼°ï¼ˆåªçœ‹ä¸€æ¬¡ï¼ï¼‰
                |
                â””â”€â”€ å ±å‘Š: Accuracy, F1, AUC, classification_report

ä¸‹ä¸€ç« æˆ‘å€‘å°‡é€²å…¥æ›´é€²éšçš„ä¸»é¡Œã€‚
ä½†è¨˜ä½ï¼Œä¸ç®¡ä½ ç”¨ä»€éº¼æ¼”ç®—æ³•ï¼Œ
é€™ä¸€ç« æ•™çš„è©•ä¼°æ–¹æ³•éƒ½é©ç”¨ï¼
```
