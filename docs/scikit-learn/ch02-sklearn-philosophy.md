# ç¬¬ 2 ç« ï¼šScikit-learn çš„è¨­è¨ˆå“²å­¸

> **ã€Œå¥½çš„ API è¨­è¨ˆå°±åƒå¥½çš„ç¬‘è©±â€”â€”ä¸éœ€è¦è§£é‡‹ã€‚ã€** â€” æ”¹ç·¨è‡ªç¨‹å¼è¨­è¨ˆæ ¼è¨€

---

## ğŸ¯ æœ¬ç« ç›®æ¨™

è®€å®Œé€™ä¸€ç« ï¼Œä½ å°‡èƒ½å¤ ï¼š

- [ ] è§£é‡‹ Estimator API çš„æ ¸å¿ƒè¨­è¨ˆç†å¿µ
- [ ] ç†Ÿç·´ä½¿ç”¨ `fit()`ã€`predict()`ã€`transform()` ä¸‰å¤§æ–¹æ³•
- [ ] åœ¨ä¸åŒæ¼”ç®—æ³•ä¹‹é–“è¼•é¬†åˆ‡æ›ï¼Œåªæ”¹ä¸€è¡Œç¨‹å¼ç¢¼
- [ ] å€åˆ† Classifierã€Regressorã€Transformerã€Clusterer å››å¤§é¡ä¼°è¨ˆå™¨
- [ ] åˆ†è¾¨ã€Œåƒæ•¸ï¼ˆParametersï¼‰ã€èˆ‡ã€Œè¶…åƒæ•¸ï¼ˆHyperparametersï¼‰ã€çš„å·®ç•°

---

## æ•…äº‹æ™‚é–“ï¼šé¤å»³çš„æ¨™æº–åŒ–æµç¨‹

æƒ³åƒä½ é–‹äº†ä¸€å®¶é€£é–é¤å»³ã€‚æ¯å®¶åˆ†åº—çš„å»šå¸«ä¸åŒã€é£Ÿæä¾†æºä¸åŒï¼Œä½†ä½ å¸Œæœ›å®¢äºº
ä¸ç®¡èµ°é€²å“ªä¸€å®¶ï¼Œé«”é©—éƒ½æ˜¯ä¸€æ¨£çš„ã€‚æ€éº¼åšåˆ°ï¼Ÿ

**ç­”æ¡ˆï¼šåˆ¶å®šæ¨™æº–ä½œæ¥­æµç¨‹ï¼ˆSOPï¼‰ã€‚**

```
æ¯å®¶åˆ†åº—éƒ½éµå¾ªç›¸åŒçš„æµç¨‹ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                             â”‚
â”‚  1. æº–å‚™é£Ÿæï¼ˆprepareï¼‰                      â”‚
â”‚  2. çƒ¹é£ªï¼ˆcookï¼‰                             â”‚
â”‚  3. æ“ºç›¤å‡ºé¤ï¼ˆserveï¼‰                        â”‚
â”‚                                             â”‚
â”‚  ä¸ç®¡æ˜¯ç‰›æ’é¤¨ã€ç¾©å¤§åˆ©é¤å»³ã€é‚„æ˜¯æ‹‰éºµåº—ï¼Œ      â”‚
â”‚  æµç¨‹éƒ½æ˜¯ï¼šæº–å‚™ â†’ çƒ¹é£ª â†’ å‡ºé¤               â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

scikit-learn åšäº†ä¸€æ¨¡ä¸€æ¨£çš„äº‹â€”â€”å®ƒç‚ºæ‰€æœ‰æ©Ÿå™¨å­¸ç¿’æ¼”ç®—æ³•åˆ¶å®šäº†çµ±ä¸€çš„ SOPï¼š

```
æ‰€æœ‰æ¨¡å‹éƒ½éµå¾ªç›¸åŒçš„æµç¨‹ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                             â”‚
â”‚  1. å»ºç«‹æ¨¡å‹ï¼ˆ__init__ï¼‰                     â”‚
â”‚  2. è¨“ç·´æ¨¡å‹ï¼ˆfitï¼‰                          â”‚
â”‚  3. ä½¿ç”¨æ¨¡å‹ï¼ˆpredict / transformï¼‰          â”‚
â”‚                                             â”‚
â”‚  ä¸ç®¡æ˜¯é‚è¼¯è¿´æ­¸ã€æ±ºç­–æ¨¹ã€é‚„æ˜¯ SVMï¼Œ          â”‚
â”‚  æµç¨‹éƒ½æ˜¯ï¼šå»ºç«‹ â†’ fit â†’ predict              â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

é€™å°±æ˜¯ **Estimator API** çš„æ ¸å¿ƒç²¾ç¥ã€‚

---

## Estimator APIï¼šä¸€å€‹ä»‹é¢çµ±æ²»æ‰€æœ‰æ¨¡å‹

### ä¸‰å€‹ä½ å¿…é ˆè¨˜ä½çš„æ–¹æ³•

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Estimator API                           â”‚
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  fit()   â”‚â”€â”€â”€â†’â”‚  æ¨¡å‹    â”‚â”€â”€â”€â†’â”‚  predict()        â”‚    â”‚
â”‚  â”‚  è¨“ç·´    â”‚    â”‚  å·²è¨“ç·´  â”‚    â”‚  æˆ– transform()   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                            â”‚
â”‚  fit(X, y)        å…§éƒ¨ç‹€æ…‹æ›´æ–°     predict(X_new)          â”‚
â”‚  ã€Œå­¸ç¿’ã€          ã€Œå·²ç¶“å­¸æœƒäº†ã€    ã€Œçµ¦æˆ‘ç­”æ¡ˆã€             â”‚
â”‚                                    transform(X_new)        â”‚
â”‚                                    ã€Œå¹«æˆ‘è½‰æ›è³‡æ–™ã€         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### `fit(X, y)` â€” è¨“ç·´ï¼ˆå­¸ç¿’ï¼‰

> é¡æ¯”ï¼šå°±åƒå­¸ç”Ÿåœ¨çœ‹èª²æœ¬ã€åšç·´ç¿’é¡Œã€‚

```python
model.fit(X_train, y_train)
# X_train = é¡Œç›®ï¼ˆç‰¹å¾µï¼‰
# y_train = ç­”æ¡ˆï¼ˆæ¨™ç±¤ï¼‰
# fit ä¹‹å¾Œï¼Œæ¨¡å‹ã€Œå­¸æœƒäº†ã€ï¼Œå…§éƒ¨åƒæ•¸è¢«æ›´æ–°
```

**é‡é»ï¼š** `fit()` æ°¸é å›å‚³æ¨¡å‹æœ¬èº«ï¼ˆ`self`ï¼‰ï¼Œæ‰€ä»¥ä½ å¯ä»¥éˆå¼å‘¼å«ï¼š
```python
model.fit(X_train, y_train).predict(X_test)  # ä¸€è¡Œæå®šè¨“ç·´+é æ¸¬
```

#### `predict(X)` â€” é æ¸¬

> é¡æ¯”ï¼šå­¸ç”Ÿä¸Šè€ƒå ´ï¼Œé¢å°å¾æœªè¦‹éçš„é¡Œç›®ã€‚

```python
y_pred = model.predict(X_test)
# X_test = æ–°çš„é¡Œç›®ï¼ˆæ¨¡å‹æ²’çœ‹éçš„ï¼‰
# y_pred = æ¨¡å‹çµ¦å‡ºçš„ç­”æ¡ˆ
```

#### `transform(X)` â€” è½‰æ›

> é¡æ¯”ï¼šä¸æ˜¯çµ¦ç­”æ¡ˆï¼Œè€Œæ˜¯å¹«ä½ æŠŠè³‡æ–™ã€Œæ•´ç†ã€æˆæ›´å¥½ç”¨çš„æ ¼å¼ã€‚

```python
X_scaled = scaler.transform(X_test)
# æŠŠåŸå§‹è³‡æ–™è½‰æ›æˆæ¨™æº–åŒ–å¾Œçš„è³‡æ–™
# ï¼ˆä¾‹å¦‚ï¼šæŠŠèº«é«˜å¾å…¬åˆ†è½‰æˆ z-scoreï¼‰
```

### ğŸ’¡ é‡é»è§€å¿µ

> **scikit-learn çš„è¨­è¨ˆå“²å­¸å°±æ˜¯ï¼šä¸ç®¡ç”¨ä»€éº¼æ¼”ç®—æ³•ï¼Œä½ çš„ç¨‹å¼ç¢¼çµæ§‹æ°¸é ç›¸åŒã€‚**
>
> é€™ä»£è¡¨ä½ å¯ä»¥ï¼š
> 1. å­¸ä¸€æ¬¡ APIï¼Œç”¨åœ¨æ‰€æœ‰æ¨¡å‹ä¸Š
> 2. è¼•é¬†æ›¿æ›ä¸åŒæ¼”ç®—æ³•ä¾†å¯¦é©—
> 3. ç”¨ Pipeline æŠŠå¤šå€‹æ­¥é©Ÿä¸²èµ·ä¾†ï¼ˆç¬¬ 3 ç« æœƒæ•™ï¼‰

---

## çœ¼è¦‹ç‚ºæ†‘ï¼šä¸‰å€‹æ¨¡å‹ï¼ŒåŒä¸€å¥— API

è®“æˆ‘å€‘ç”¨å¯¦éš›ç¨‹å¼ç¢¼è­‰æ˜ã€‚æˆ‘å€‘æ‹¿ç¬¬ 1 ç« çš„ Iris è³‡æ–™é›†ï¼Œç”¨ä¸‰å€‹å®Œå…¨ä¸åŒçš„
æ¼”ç®—æ³•ä¾†åˆ†é¡â€”â€”ä½†ç¨‹å¼ç¢¼çµæ§‹å¹¾ä¹ä¸€æ¨¡ä¸€æ¨£ã€‚

### æº–å‚™å·¥ä½œ

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# è¼‰å…¥è³‡æ–™ï¼Œåˆ†å‰²è¨“ç·´/æ¸¬è©¦é›†
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(
    iris.data, iris.target, test_size=0.2, random_state=42
)
```

### æ¨¡å‹ 1ï¼šé‚è¼¯è¿´æ­¸ï¼ˆLogistic Regressionï¼‰

```python
from sklearn.linear_model import LogisticRegression

model_lr = LogisticRegression(max_iter=200)
model_lr.fit(X_train, y_train)                    # â† fit
y_pred_lr = model_lr.predict(X_test)              # â† predict
print(f"é‚è¼¯è¿´æ­¸æº–ç¢ºç‡ï¼š{accuracy_score(y_test, y_pred_lr):.2%}")
```

### æ¨¡å‹ 2ï¼šæ”¯æŒå‘é‡æ©Ÿï¼ˆSVCï¼‰

```python
from sklearn.svm import SVC

model_svc = SVC()
model_svc.fit(X_train, y_train)                   # â† ä¸€æ¨¡ä¸€æ¨£çš„ fit
y_pred_svc = model_svc.predict(X_test)            # â† ä¸€æ¨¡ä¸€æ¨£çš„ predict
print(f"SVC æº–ç¢ºç‡ï¼š{accuracy_score(y_test, y_pred_svc):.2%}")
```

### æ¨¡å‹ 3ï¼šK è¿‘é„°ï¼ˆKNeighborsClassifierï¼‰

```python
from sklearn.neighbors import KNeighborsClassifier

model_knn = KNeighborsClassifier(n_neighbors=5)
model_knn.fit(X_train, y_train)                   # â† é‚„æ˜¯ä¸€æ¨£çš„ fit
y_pred_knn = model_knn.predict(X_test)            # â† é‚„æ˜¯ä¸€æ¨£çš„ predict
print(f"KNN æº–ç¢ºç‡ï¼š{accuracy_score(y_test, y_pred_knn):.2%}")
```

### çœ‹å‡ºä¾†äº†å—ï¼Ÿ

```
æ¨¡å‹ 1ï¼šLogisticRegression()  â†’  fit()  â†’  predict()
æ¨¡å‹ 2ï¼šSVC()                 â†’  fit()  â†’  predict()
æ¨¡å‹ 3ï¼šKNeighborsClassifier()â†’  fit()  â†’  predict()
                                  â†‘           â†‘
                              å®Œå…¨ç›¸åŒ     å®Œå…¨ç›¸åŒ
                              çš„ä»‹é¢ï¼     çš„ä»‹é¢ï¼
```

**åªæœ‰ã€Œå»ºç«‹æ¨¡å‹ã€é‚£ä¸€è¡Œä¸åŒã€‚** å…¶ä»–çš„ `fit`ã€`predict`ã€`accuracy_score` å…¨éƒ¨é€šç”¨ã€‚

### ç”¨è¿´åœˆæŠŠä¸‰å€‹æ¨¡å‹è·‘å®Œ

å› ç‚º API çµ±ä¸€ï¼Œæˆ‘å€‘å¯ä»¥å¯«å‡ºéå¸¸å„ªé›…çš„æ¯”è¼ƒç¨‹å¼ç¢¼ï¼š

```python
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

# æŠŠä¸‰å€‹æ¨¡å‹æ”¾é€²å­—å…¸
models = {
    "é‚è¼¯è¿´æ­¸": LogisticRegression(max_iter=200),
    "SVM": SVC(),
    "KNN": KNeighborsClassifier(n_neighbors=5),
}

# ç”¨è¿´åœˆè·‘å®Œæ‰€æœ‰æ¨¡å‹
print("=" * 40)
print(f"{'æ¨¡å‹åç¨±':<12} {'æº–ç¢ºç‡':>8}")
print("=" * 40)

for name, model in models.items():
    model.fit(X_train, y_train)
    acc = accuracy_score(y_test, model.predict(X_test))
    print(f"{name:<12} {acc:>8.2%}")

print("=" * 40)
```

è¼¸å‡ºï¼š
```
========================================
æ¨¡å‹åç¨±         æº–ç¢ºç‡
========================================
é‚è¼¯è¿´æ­¸       100.00%
SVM            100.00%
KNN            100.00%
========================================
```

### ğŸ§  å‹•å‹•è…¦

å¦‚æœ scikit-learn çš„æ¯å€‹æ¨¡å‹éƒ½æœ‰ä¸åŒçš„æ–¹æ³•åç¨±
ï¼ˆæ¯”å¦‚ `lr.train()`ã€`svc.learn()`ã€`knn.classify()`ï¼‰ï¼Œ
ä¸Šé¢çš„è¿´åœˆé‚„å¯«å¾—å‡ºä¾†å—ï¼Ÿ

<details>
<summary>é»æˆ‘çœ‹ç­”æ¡ˆ</summary>

å¯«ä¸å‡ºä¾†ï¼Œæˆ–è€…è‡³å°‘éå¸¸éº»ç…©ã€‚ä½ éœ€è¦ç‚ºæ¯å€‹æ¨¡å‹å¯«ä¸åŒçš„å‘¼å«æ–¹å¼ï¼Œ
å……æ»¿ if-else åˆ¤æ–·ã€‚

çµ±ä¸€çš„ API è®“ä½ å¯ä»¥æŠŠæ¨¡å‹ç•¶ä½œã€Œå¯æ›¿æ›çš„é›¶ä»¶ã€â€”â€”
åªè¦å®ƒæœ‰ `fit()` å’Œ `predict()`ï¼Œå°±èƒ½ç›´æ¥æ›¿æ›ä½¿ç”¨ã€‚

é€™åœ¨è»Ÿé«”å·¥ç¨‹ä¸­å«åšã€Œå¤šå‹ï¼ˆPolymorphismï¼‰ã€æˆ–ã€Œé´¨å­å‹åˆ¥ï¼ˆDuck Typingï¼‰ã€ï¼š
å¦‚æœå®ƒèµ°èµ·ä¾†åƒé´¨å­ã€å«èµ·ä¾†åƒé´¨å­ï¼Œé‚£å®ƒå°±æ˜¯é´¨å­ã€‚
å¦‚æœå®ƒæœ‰ `fit()` å’Œ `predict()`ï¼Œé‚£å®ƒå°±æ˜¯ sklearn çš„æ¨¡å‹ã€‚

</details>

---

## ç‚ºä»€éº¼çµ±ä¸€è¨­è¨ˆé€™éº¼é‡è¦ï¼Ÿ

### å·¥ç¨‹è§’åº¦

```
æ²’æœ‰çµ±ä¸€ API çš„ä¸–ç•Œï¼š                æœ‰çµ±ä¸€ API çš„ä¸–ç•Œï¼š

# æ¯å€‹æ¨¡å‹å¯«æ³•ä¸åŒ                   # æ‰€æœ‰æ¨¡å‹å¯«æ³•ä¸€æ¨£
lr.train(data, labels)              model.fit(X, y)
svc.learn(features, targets)        model.fit(X, y)
knn.classify_fit(x_data, y_data)    model.fit(X, y)
tree.build_tree(samples, classes)   model.fit(X, y)

lr.classify(new_data)               model.predict(X_new)
svc.predict_class(new_features)     model.predict(X_new)
knn.find_nearest(x_new)             model.predict(X_new)
tree.traverse(new_samples)          model.predict(X_new)

â†’ æ¯æ›ä¸€å€‹æ¨¡å‹å°±è¦æ”¹ä¸€å †ç¨‹å¼ç¢¼       â†’ æ›æ¨¡å‹åªæ”¹ä¸€è¡Œ
â†’ ç„¡æ³•å¯«é€šç”¨çš„å·¥å…·å‡½å¼               â†’ å¯ä»¥å¯«é©ç”¨æ‰€æœ‰æ¨¡å‹çš„å·¥å…·
â†’ å­¸ç¿’æ›²ç·šé™¡å³­                       â†’ å­¸ä¸€æ¬¡ï¼Œçµ‚èº«å—ç”¨
```

### ç§‘å­¸å¯¦é©—è§’åº¦

åœ¨åšæ©Ÿå™¨å­¸ç¿’å¯¦é©—æ™‚ï¼Œä½ ç¶“å¸¸éœ€è¦ã€Œç”¨ä¸åŒæ¨¡å‹è·‘åŒæ¨£çš„è³‡æ–™ï¼Œæ¯”è¼ƒçµæœã€ã€‚
çµ±ä¸€çš„ API è®“é€™ä»¶äº‹è®Šå¾—æ¥µç‚ºç°¡å–®ï¼š

```python
# æƒ³è¦åŠ ä¸€å€‹æ–°æ¨¡å‹ï¼Ÿåªéœ€è¦ä¸€è¡Œï¼
from sklearn.tree import DecisionTreeClassifier

models["æ±ºç­–æ¨¹"] = DecisionTreeClassifier()
# å®Œã€‚ä¸ç”¨æ”¹å…¶ä»–ä»»ä½•ç¨‹å¼ç¢¼ã€‚
```

---

## å››å¤§ä¼°è¨ˆå™¨é¡å‹

scikit-learn çš„æ‰€æœ‰æ¨¡å‹ï¼ˆä¼°è¨ˆå™¨ï¼‰å¯ä»¥åˆ†æˆå››å¤§é¡ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Estimator å››å¤§å®¶æ—                            â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  Classifier  â”‚  â”‚  Regressor   â”‚  â”‚  Transformer  â”‚          â”‚
â”‚  â”‚  åˆ†é¡å™¨       â”‚  â”‚  è¿´æ­¸å™¨      â”‚  â”‚  è½‰æ›å™¨       â”‚          â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚          â”‚
â”‚  â”‚ fit(X,y)    â”‚  â”‚ fit(X,y)    â”‚  â”‚ fit(X)       â”‚          â”‚
â”‚  â”‚ predict(X)  â”‚  â”‚ predict(X)  â”‚  â”‚ transform(X) â”‚          â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚          â”‚
â”‚  â”‚ è¼¸å‡ºï¼šé¡åˆ¥   â”‚  â”‚ è¼¸å‡ºï¼šæ•¸å€¼   â”‚  â”‚ è¼¸å‡ºï¼šè½‰æ›   â”‚          â”‚
â”‚  â”‚ è²“/ç‹—/é³¥     â”‚  â”‚ 23.5, 89.1  â”‚  â”‚ å¾Œçš„è³‡æ–™     â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚  â”‚  Clusterer   â”‚                                               â”‚
â”‚  â”‚  åˆ†ç¾¤å™¨       â”‚                                               â”‚
â”‚  â”‚              â”‚                                               â”‚
â”‚  â”‚ fit(X)       â”‚  â† æ³¨æ„ï¼æ²’æœ‰ yï¼Œå› ç‚ºæ˜¯éç›£ç£å¼               â”‚
â”‚  â”‚ predict(X)   â”‚                                               â”‚
â”‚  â”‚              â”‚                                               â”‚
â”‚  â”‚ è¼¸å‡ºï¼šç¾¤çµ„   â”‚                                               â”‚
â”‚  â”‚ ç·¨è™Ÿ 0,1,2   â”‚                                               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Classifierï¼ˆåˆ†é¡å™¨ï¼‰

**ä»»å‹™ï¼š** é æ¸¬é¡åˆ¥ï¼ˆé›¢æ•£å€¼ï¼‰

```python
# å¸¸è¦‹çš„åˆ†é¡å™¨
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier

# éƒ½æœ‰ fit(X, y) + predict(X)
# éƒ½æœ‰ score(X, y) å›å‚³æº–ç¢ºç‡
```

**æ‡‰ç”¨å ´æ™¯ï¼š** åƒåœ¾éƒµä»¶åµæ¸¬ã€ç–¾ç—…è¨ºæ–·ã€åœ–ç‰‡åˆ†é¡

### Regressorï¼ˆè¿´æ­¸å™¨ï¼‰

**ä»»å‹™ï¼š** é æ¸¬æ•¸å€¼ï¼ˆé€£çºŒå€¼ï¼‰

```python
# å¸¸è¦‹çš„è¿´æ­¸å™¨
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor

# ä¹Ÿæ˜¯ fit(X, y) + predict(X)
# score(X, y) å›å‚³çš„æ˜¯ RÂ² åˆ†æ•¸ï¼ˆä¸æ˜¯æº–ç¢ºç‡ï¼‰
```

**æ‡‰ç”¨å ´æ™¯ï¼š** æˆ¿åƒ¹é æ¸¬ã€æº«åº¦é æ¸¬ã€éŠ·å”®é¡é ä¼°

### Transformerï¼ˆè½‰æ›å™¨ï¼‰

**ä»»å‹™ï¼š** è½‰æ›è³‡æ–™ï¼ˆä¸æ˜¯é æ¸¬ï¼‰

```python
# å¸¸è¦‹çš„è½‰æ›å™¨
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.decomposition import PCA

# fit(X) + transform(X)
# æˆ–è€…ä¸€æ­¥åˆ°ä½ï¼šfit_transform(X)
```

**æ‡‰ç”¨å ´æ™¯ï¼š** è³‡æ–™æ¨™æº–åŒ–ã€é™ç¶­ã€ç·¨ç¢¼è½‰æ›

### Clustererï¼ˆåˆ†ç¾¤å™¨ï¼‰

**ä»»å‹™ï¼š** æŠŠè³‡æ–™åˆ†æˆå¹¾å€‹ç¾¤çµ„

```python
# å¸¸è¦‹çš„åˆ†ç¾¤å™¨
from sklearn.cluster import KMeans, DBSCAN
from sklearn.mixture import GaussianMixture

# fit(X) â† æ³¨æ„æ²’æœ‰ yï¼
# predict(X) å›å‚³æ¯ç­†è³‡æ–™å±¬æ–¼å“ªä¸€ç¾¤
```

**æ‡‰ç”¨å ´æ™¯ï¼š** å®¢æˆ¶åˆ†ç¾¤ã€æ–‡ä»¶ä¸»é¡Œåˆ†é¡ã€ç•°å¸¸åµæ¸¬

### â“ æ²’æœ‰ç¬¨å•é¡Œ

**Qï¼šTransformer å’Œæ·±åº¦å­¸ç¿’çš„ Transformerï¼ˆå¦‚ GPTï¼‰æ˜¯åŒä¸€å€‹æ±è¥¿å—ï¼Ÿ**

Aï¼šå®Œå…¨ä¸æ˜¯ï¼scikit-learn çš„ Transformer æ˜¯æŒ‡ã€Œè³‡æ–™è½‰æ›å™¨ã€
ï¼ˆå¦‚æ¨™æº–åŒ–ã€æ­£è¦åŒ–ï¼‰ï¼Œæ˜¯ä¸€å€‹è¨­è¨ˆæ¨¡å¼ã€‚æ·±åº¦å­¸ç¿’çš„ Transformer
æ˜¯ä¸€ç¨®ç¥ç¶“ç¶²è·¯æ¶æ§‹ã€‚åŒåä½†æ¯«ç„¡é—œä¿‚ï¼Œåˆ¥ææ··äº†ã€‚

**Qï¼šç‚ºä»€éº¼åˆ†ç¾¤å™¨çš„ `fit()` ä¸éœ€è¦ `y`ï¼Ÿ**

Aï¼šå› ç‚ºåˆ†ç¾¤æ˜¯éç›£ç£å¼å­¸ç¿’â€”â€”æ²’æœ‰æ¨™æº–ç­”æ¡ˆã€‚æ¨¡å‹è‡ªå·±å¾è³‡æ–™ä¸­
æ‰¾å‡ºç¾¤çµ„çµæ§‹ã€‚æ‰€ä»¥ `fit(X)` å°±å¤ äº†ï¼Œä¸éœ€è¦æ¨™ç±¤ `y`ã€‚

**Qï¼š`fit_transform()` å’Œå…ˆ `fit()` å† `transform()` æœ‰ä»€éº¼å·®åˆ¥ï¼Ÿ**

Aï¼šçµæœå®Œå…¨ä¸€æ¨£ï¼`fit_transform(X)` å°±æ˜¯ `fit(X).transform(X)` çš„
ç°¡å¯«ï¼Œæ•ˆç‡ä¸Šå¯èƒ½ç¨å¿«ä¸€é»ã€‚ä½†è¦æ³¨æ„ï¼šåªåœ¨è¨“ç·´é›†ä¸Šç”¨ `fit_transform()`ï¼Œ
æ¸¬è©¦é›†åªèƒ½ç”¨ `transform()`ï¼ˆåŸå› åœ¨ç¬¬ 3 ç« æœƒè©³ç´°è§£é‡‹ï¼‰ã€‚

---

## åƒæ•¸ vs è¶…åƒæ•¸

é€™æ˜¯ä¸€å€‹éå¸¸é‡è¦ä½†åˆå­¸è€…å¸¸ææ··çš„è§€å¿µã€‚

### åƒæ•¸ï¼ˆParametersï¼‰â€” æ¨¡å‹è‡ªå·±å­¸çš„

> é¡æ¯”ï¼šå­¸ç”Ÿç¶“éå­¸ç¿’ä¹‹å¾Œã€Œå…§åŒ–ã€çš„çŸ¥è­˜ã€‚

```
æ¨¡å‹è¨“ç·´å‰ï¼šä¸çŸ¥é“æ€éº¼åˆ†é¡
         â†“ fit(X_train, y_train)
æ¨¡å‹è¨“ç·´å¾Œï¼šå­¸åˆ°äº†åˆ†é¡çš„ã€Œæ¬Šé‡ã€å’Œã€Œåå·®ã€

é€™äº›å­¸åˆ°çš„æ±è¥¿å°±æ˜¯ã€Œåƒæ•¸ã€ï¼š
  - é‚è¼¯è¿´æ­¸çš„æ¬Šé‡ï¼ˆcoefficientsï¼‰
  - æ±ºç­–æ¨¹çš„åˆ†å‰²é»
  - KNN è¨˜ä½çš„æ‰€æœ‰è¨“ç·´è³‡æ–™é»
```

åœ¨ scikit-learn ä¸­ï¼Œæ¨¡å‹å­¸åˆ°çš„åƒæ•¸éƒ½æœ‰ **ä¸‹åº•ç·šçµå°¾** `_`ï¼š

```python
model = LogisticRegression(max_iter=200)
model.fit(X_train, y_train)

# è¨“ç·´å¾Œæ‰æœ‰çš„å±¬æ€§ï¼ˆæœ‰åº•ç·š _ï¼‰
print(model.coef_)          # æ¬Šé‡ä¿‚æ•¸ â† æœ‰åº•ç·š
print(model.intercept_)     # æˆªè·    â† æœ‰åº•ç·š
print(model.classes_)       # å­¸åˆ°çš„é¡åˆ¥ â† æœ‰åº•ç·š
```

### è¶…åƒæ•¸ï¼ˆHyperparametersï¼‰â€” äººé¡è¨­å®šçš„

> é¡æ¯”ï¼šè€å¸«å¹«å­¸ç”Ÿè¨­å®šçš„ã€Œå­¸ç¿’æ–¹å¼ã€â€”â€”è¦ä¸è¦é–‹å†·æ°£ã€æ¯å ‚èª²å¤šé•·ã€åˆ†å¹¾çµ„è¨è«–ã€‚

```
è¶…åƒæ•¸æ˜¯ä½ åœ¨è¨“ç·´ã€Œä¹‹å‰ã€å°±è¦æ±ºå®šçš„è¨­å®šï¼š
  - KNN çš„ n_neighborsï¼ˆè¦çœ‹å¹¾å€‹é„°å±…ï¼Ÿï¼‰
  - é‚è¼¯è¿´æ­¸çš„ Cï¼ˆæ­£å‰‡åŒ–å¼·åº¦ï¼‰
  - æ±ºç­–æ¨¹çš„ max_depthï¼ˆæ¨¹æœ€æ·±å¹¾å±¤ï¼Ÿï¼‰
```

```python
# è¶…åƒæ•¸åœ¨å»ºç«‹æ¨¡å‹æ™‚è¨­å®šï¼ˆæ²’æœ‰åº•ç·šï¼‰
model = KNeighborsClassifier(n_neighbors=5)     # è¶…åƒæ•¸
model = LogisticRegression(C=1.0, max_iter=200) # è¶…åƒæ•¸
model = DecisionTreeClassifier(max_depth=3)     # è¶…åƒæ•¸
```

### ä¸€å¼µè¡¨çœ‹æ‡‚å·®ç•°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               â”‚  åƒæ•¸ (Parameters)   â”‚ è¶…åƒæ•¸               â”‚
â”‚               â”‚                      â”‚ (Hyperparameters)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  èª°æ±ºå®šçš„ï¼Ÿ   â”‚  æ¨¡å‹è‡ªå·±å­¸çš„         â”‚  äººé¡ï¼ˆä½ ï¼‰è¨­å®šçš„     â”‚
â”‚  ä»€éº¼æ™‚å€™æœ‰ï¼Ÿ â”‚  fit() ä¹‹å¾Œ          â”‚  fit() ä¹‹å‰          â”‚
â”‚  å‘½åæ…£ä¾‹     â”‚  æœ‰åº•ç·š coef_        â”‚  æ²’åº•ç·š n_neighbors  â”‚
â”‚  æ€éº¼èª¿æ•´ï¼Ÿ   â”‚  çµ¦æ›´å¤š/æ›´å¥½çš„è³‡æ–™    â”‚  æ‰‹å‹•èª¿æˆ–è‡ªå‹•æœå°‹    â”‚
â”‚  ä¾‹å­         â”‚  æ¬Šé‡ã€æˆªè·ã€åˆ†å‰²é»   â”‚  å­¸ç¿’ç‡ã€é„°å±…æ•¸ã€    â”‚
â”‚               â”‚                      â”‚  æ¨¹æ·±åº¦ã€æ­£å‰‡åŒ–      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ§  å‹•å‹•è…¦

ä»¥ä¸‹å“ªäº›æ˜¯ã€Œåƒæ•¸ã€ï¼Œå“ªäº›æ˜¯ã€Œè¶…åƒæ•¸ã€ï¼Ÿ

1. KNN ä¸­çš„ `n_neighbors=5`
2. é‚è¼¯è¿´æ­¸è¨“ç·´å¾Œçš„ `coef_`
3. æ±ºç­–æ¨¹çš„ `max_depth=10`
4. KMeans å­¸åˆ°çš„ `cluster_centers_`
5. RandomForest çš„ `n_estimators=100`

<details>
<summary>é»æˆ‘çœ‹ç­”æ¡ˆ</summary>

1. **è¶…åƒæ•¸** â€” ä½ åœ¨è¨“ç·´å‰æ±ºå®šè¦çœ‹å¹¾å€‹é„°å±…
2. **åƒæ•¸** â€” æ¨¡å‹è¨“ç·´å¾Œå­¸åˆ°çš„æ¬Šé‡ï¼ˆæ³¨æ„åº•ç·š `_`ï¼‰
3. **è¶…åƒæ•¸** â€” ä½ åœ¨è¨“ç·´å‰é™åˆ¶æ¨¹çš„æ·±åº¦
4. **åƒæ•¸** â€” æ¨¡å‹è¨“ç·´å¾Œç®—å‡ºçš„ç¾¤å¿ƒä½ç½®ï¼ˆæ³¨æ„åº•ç·š `_`ï¼‰
5. **è¶…åƒæ•¸** â€” ä½ åœ¨è¨“ç·´å‰æ±ºå®šè¦ç¨®å¹¾æ£µæ¨¹

**è¨˜æ†¶å£è¨£ï¼šæœ‰åº•ç·šçš„æ˜¯æ¨¡å‹å­¸çš„ï¼Œæ²’åº•ç·šçš„æ˜¯ä½ è¨­çš„ã€‚**

</details>

---

## å¯¦æˆ°ï¼šç”¨ `get_params()` å’Œ `set_params()` ç®¡ç†è¶…åƒæ•¸

scikit-learn æä¾›çµ±ä¸€çš„æ–¹å¼ä¾†æŸ¥çœ‹å’Œä¿®æ”¹è¶…åƒæ•¸ï¼š

```python
model = KNeighborsClassifier(n_neighbors=5)

# æŸ¥çœ‹æ‰€æœ‰è¶…åƒæ•¸
print(model.get_params())
# {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski',
#  'n_neighbors': 5, 'p': 2, 'weights': 'uniform', ...}

# ä¿®æ”¹è¶…åƒæ•¸
model.set_params(n_neighbors=3)
print(model.get_params()['n_neighbors'])  # 3
```

é€™åœ¨è‡ªå‹•åŒ–èª¿åƒæ™‚éå¸¸æœ‰ç”¨ï¼ˆå¾Œé¢ç« ç¯€æœƒæ•™ GridSearchCVï¼‰ã€‚

---

## `score()` æ–¹æ³•ï¼šå¿«é€Ÿè©•ä¼°

æ¯å€‹ä¼°è¨ˆå™¨éƒ½æœ‰ä¸€å€‹ `score()` æ–¹æ³•ï¼Œè®“ä½ å¿«é€Ÿå¾—åˆ°è©•ä¼°åˆ†æ•¸ï¼š

```python
# åˆ†é¡å™¨çš„ score() å›å‚³æº–ç¢ºç‡ï¼ˆaccuracyï¼‰
clf = LogisticRegression(max_iter=200)
clf.fit(X_train, y_train)
print(f"æº–ç¢ºç‡ï¼š{clf.score(X_test, y_test):.2%}")

# è¿´æ­¸å™¨çš„ score() å›å‚³ RÂ² åˆ†æ•¸
from sklearn.linear_model import LinearRegression
from sklearn.datasets import load_diabetes

diabetes = load_diabetes()
X_tr, X_te, y_tr, y_te = train_test_split(
    diabetes.data, diabetes.target, test_size=0.2, random_state=42
)
reg = LinearRegression()
reg.fit(X_tr, y_tr)
print(f"RÂ² åˆ†æ•¸ï¼š{reg.score(X_te, y_te):.4f}")
```

---

## å®Œæ•´æ¯”è¼ƒç¯„ä¾‹

æŠŠé€™ä¸€ç« å­¸åˆ°çš„å…¨éƒ¨ä¸²èµ·ä¾†ï¼š

```python
"""
ç¬¬ 2 ç« ï¼šEstimator API å®Œæ•´ç¤ºç¯„
å±•ç¤ºçµ±ä¸€ API çš„å¼·å¤§ä¹‹è™•
"""

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# åŒ¯å…¥å„ç¨®åˆ†é¡å™¨
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# ===== æº–å‚™è³‡æ–™ =====
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(
    iris.data, iris.target, test_size=0.2, random_state=42
)

# ===== å®šç¾©æ¨¡å‹ =====
models = {
    "é‚è¼¯è¿´æ­¸":   LogisticRegression(max_iter=200),
    "SVM":        SVC(kernel='rbf'),
    "KNN(k=3)":   KNeighborsClassifier(n_neighbors=3),
    "KNN(k=7)":   KNeighborsClassifier(n_neighbors=7),
    "æ±ºç­–æ¨¹":     DecisionTreeClassifier(max_depth=3),
    "éš¨æ©Ÿæ£®æ—":   RandomForestClassifier(n_estimators=100, random_state=42),
}

# ===== çµ±ä¸€è¨“ç·´èˆ‡è©•ä¼° =====
print("=" * 50)
print(f"{'æ¨¡å‹':<14} {'è¨“ç·´æº–ç¢ºç‡':>10} {'æ¸¬è©¦æº–ç¢ºç‡':>10}")
print("=" * 50)

for name, model in models.items():
    # è¨“ç·´
    model.fit(X_train, y_train)

    # è©•ä¼°
    train_acc = model.score(X_train, y_train)
    test_acc = model.score(X_test, y_test)

    # éæ“¬åˆæŒ‡æ¨™
    gap = train_acc - test_acc
    flag = " âš ï¸" if gap > 0.05 else ""

    print(f"{name:<14} {train_acc:>10.2%} {test_acc:>10.2%}{flag}")

print("=" * 50)
print("âš ï¸ = è¨“ç·´/æ¸¬è©¦å·®è· > 5%ï¼Œå¯èƒ½æœ‰éæ“¬åˆé¢¨éšª")

# ===== è©³ç´°å ±å‘Šï¼ˆç”¨æœ€ä½³æ¨¡å‹ï¼‰=====
best_model = models["éš¨æ©Ÿæ£®æ—"]
y_pred = best_model.predict(X_test)
print("\néš¨æ©Ÿæ£®æ—çš„è©³ç´°åˆ†é¡å ±å‘Šï¼š")
print(classification_report(
    y_test, y_pred, target_names=iris.target_names
))
```

---

## æœ¬ç« æ¶æ§‹åœ–

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Scikit-learn Estimator API                    â”‚
â”‚                                                            â”‚
â”‚  æ ¸å¿ƒä»‹é¢ï¼š                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  model = SomeEstimator(hyperparameters)     â”‚           â”‚
â”‚  â”‚  model.fit(X_train, y_train)                â”‚           â”‚
â”‚  â”‚  predictions = model.predict(X_test)        â”‚           â”‚
â”‚  â”‚  score = model.score(X_test, y_test)        â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                            â”‚
â”‚  å››å¤§å®¶æ—ï¼š                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Classifier â”‚ Regressor  â”‚Transformer â”‚ Clusterer  â”‚     â”‚
â”‚  â”‚ åˆ†é¡       â”‚ è¿´æ­¸       â”‚ è½‰æ›       â”‚ åˆ†ç¾¤       â”‚     â”‚
â”‚  â”‚            â”‚            â”‚            â”‚            â”‚     â”‚
â”‚  â”‚ predictâ†’   â”‚ predictâ†’   â”‚ transformâ†’ â”‚ predictâ†’   â”‚     â”‚
â”‚  â”‚ é¡åˆ¥       â”‚ æ•¸å€¼       â”‚ æ–°è³‡æ–™     â”‚ ç¾¤çµ„ç·¨è™Ÿ   â”‚     â”‚
â”‚  â”‚            â”‚            â”‚            â”‚            â”‚     â”‚
â”‚  â”‚ ç›£ç£å¼     â”‚ ç›£ç£å¼     â”‚ é€šå¸¸ç„¡ç›£ç£ â”‚ éç›£ç£å¼   â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                            â”‚
â”‚  å…©ç¨®æ±è¥¿ï¼š                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ Parameters       â”‚ Hyperparameters       â”‚              â”‚
â”‚  â”‚ æ¨¡å‹å­¸åˆ°çš„ coef_ â”‚ äººè¨­å®šçš„ n_neighbors  â”‚              â”‚
â”‚  â”‚ fit() ä¹‹å¾Œæ‰æœ‰   â”‚ fit() ä¹‹å‰å°±è¦æ±ºå®š    â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš ï¸ å¸¸è¦‹é™·é˜±

1. **åœ¨ `fit()` ä¹‹å‰å°±å‘¼å« `predict()`**
   â†’ æœƒå ±éŒ¯ `NotFittedError`ã€‚æ¨¡å‹é‚„æ²’å­¸ï¼Œæ€éº¼é æ¸¬ï¼Ÿ

2. **åˆ†é¡å•é¡Œç”¨äº†è¿´æ­¸å™¨ï¼ˆæˆ–åéä¾†ï¼‰**
   â†’ `LinearRegression` é æ¸¬çš„æ˜¯é€£çºŒæ•¸å€¼ï¼Œä¸æ˜¯é¡åˆ¥ã€‚
   è¦åˆ†é¡è«‹ç”¨ `LogisticRegression`ï¼ˆåå­—æœ‰è¿´æ­¸ä½†å…¶å¯¦æ˜¯åˆ†é¡å™¨ï¼‰ã€‚

3. **å¿˜è¨˜è¨­å®š `random_state`**
   â†’ æœ‰äº›æ¨¡å‹ï¼ˆå¦‚ RandomForestï¼‰åŒ…å«éš¨æ©Ÿæ€§ã€‚ä¸å›ºå®šç¨®å­ï¼Œ
   æ¯æ¬¡è·‘çµæœéƒ½ä¸åŒï¼Œç„¡æ³•æ¯”è¼ƒã€‚

4. **ææ·· `transform()` å’Œ `predict()`**
   â†’ Transformer ç”¨ `transform()` è½‰æ›è³‡æ–™ï¼Œä¸æœƒçµ¦ä½ é æ¸¬çµæœã€‚
   Classifier/Regressor ç”¨ `predict()` åšé æ¸¬ã€‚

5. **ä¸çœ‹ `get_params()` å°±ç”¨é è¨­å€¼**
   â†’ é è¨­è¶…åƒæ•¸ä¸ä¸€å®šé©åˆä½ çš„è³‡æ–™ã€‚è‡³å°‘çœ‹ä¸€ä¸‹æœ‰å“ªäº›å¯ä»¥èª¿ã€‚

---

## ğŸ’¡ é‡é»è§€å¿µå›é¡§

| è§€å¿µ | ä¸€å¥è©±è§£é‡‹ |
|------|-----------|
| Estimator API | scikit-learn çµ±ä¸€çš„æ¨¡å‹ä»‹é¢ï¼šinit â†’ fit â†’ predict |
| `fit()` | è®“æ¨¡å‹å¾è¨“ç·´è³‡æ–™ä¸­å­¸ç¿’ |
| `predict()` | ç”¨å­¸å¥½çš„æ¨¡å‹åšé æ¸¬ |
| `transform()` | æŠŠè³‡æ–™è½‰æ›æˆä¸åŒçš„è¡¨ç¤ºå½¢å¼ |
| `score()` | å¿«é€Ÿè©•ä¼°æ¨¡å‹è¡¨ç¾çš„æ–¹æ³• |
| Classifier | é æ¸¬é¡åˆ¥çš„æ¨¡å‹ï¼ˆåˆ†é¡å•é¡Œï¼‰ |
| Regressor | é æ¸¬æ•¸å€¼çš„æ¨¡å‹ï¼ˆè¿´æ­¸å•é¡Œï¼‰ |
| Transformer | è½‰æ›è³‡æ–™çš„å·¥å…·ï¼ˆå¦‚æ¨™æº–åŒ–ï¼‰ |
| Clusterer | æŠŠè³‡æ–™åˆ†ç¾¤çš„æ¨¡å‹ï¼ˆéç›£ç£å¼ï¼‰ |
| Parameters | æ¨¡å‹è‡ªå·±å¾è³‡æ–™ä¸­å­¸åˆ°çš„å€¼ï¼ˆæœ‰åº•ç·š `_`ï¼‰ |
| Hyperparameters | ä½ åœ¨è¨“ç·´å‰è¨­å®šçš„å€¼ï¼ˆæ²’æœ‰åº•ç·šï¼‰ |

---

## ğŸ“ èª²å¾Œç·´ç¿’

### ç·´ç¿’ 1ï¼šåŠ å…¥æ›´å¤šæ¨¡å‹

åœ¨æœ¬ç« çš„æ¯”è¼ƒç¨‹å¼ç¢¼ä¸­ï¼Œè©¦è‘—åŠ å…¥ä»¥ä¸‹æ¨¡å‹ï¼Œçœ‹çœ‹æº–ç¢ºç‡å¦‚ä½•ï¼š

```python
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import GradientBoostingClassifier

# åŠ åˆ° models å­—å…¸ä¸­ï¼Œç„¶å¾Œé‡æ–°è·‘æ¯”è¼ƒ
models["è²æ°åˆ†é¡å™¨"] = GaussianNB()
models["æ¢¯åº¦æå‡"] = GradientBoostingClassifier(n_estimators=100, random_state=42)
```

### ç·´ç¿’ 2ï¼šè§€å¯Ÿåƒæ•¸

è¨“ç·´ä¸€å€‹ `DecisionTreeClassifier`ï¼Œç„¶å¾Œè§€å¯Ÿå®ƒå­¸åˆ°çš„åƒæ•¸ï¼š

```python
tree = DecisionTreeClassifier(max_depth=3, random_state=42)
tree.fit(X_train, y_train)

print("è¶…åƒæ•¸ï¼ˆä½ è¨­å®šçš„ï¼‰ï¼š")
print(f"  max_depth = {tree.get_params()['max_depth']}")

print("\nåƒæ•¸ï¼ˆæ¨¡å‹å­¸åˆ°çš„ï¼‰ï¼š")
print(f"  ç‰¹å¾µé‡è¦åº¦ = {tree.feature_importances_}")
print(f"  æ¨¹çš„æ·±åº¦   = {tree.get_depth()}")
print(f"  è‘‰ç¯€é»æ•¸   = {tree.get_n_leaves()}")
```

å“ªå€‹ç‰¹å¾µæœ€é‡è¦ï¼Ÿå’Œä½ ç›´è¦ºä¸€æ¨£å—ï¼Ÿ

### ç·´ç¿’ 3ï¼šè¶…åƒæ•¸å¯¦é©—

æ”¹è®Š KNN çš„ `n_neighbors`ï¼ˆå¾ 1 åˆ° 20ï¼‰ï¼Œç•«å‡ºæº–ç¢ºç‡çš„è®ŠåŒ–ï¼š

```python
import matplotlib.pyplot as plt

k_values = range(1, 21)
train_scores = []
test_scores = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    train_scores.append(knn.score(X_train, y_train))
    test_scores.append(knn.score(X_test, y_test))

plt.figure(figsize=(10, 5))
plt.plot(k_values, train_scores, 'o-', label='è¨“ç·´é›†')
plt.plot(k_values, test_scores, 's-', label='æ¸¬è©¦é›†')
plt.xlabel('n_neighbors (k)')
plt.ylabel('æº–ç¢ºç‡')
plt.title('KNN: k å€¼ vs æº–ç¢ºç‡')
plt.legend()
plt.grid(True)
plt.show()
```

è§€å¯Ÿï¼šç•¶ k=1 æ™‚ï¼Œè¨“ç·´é›†æº–ç¢ºç‡æ˜¯å¤šå°‘ï¼Ÿç‚ºä»€éº¼ï¼Ÿ

---

## ä¸‹ä¸€ç« é å‘Š

ä½ å·²ç¶“å­¸æœƒäº† scikit-learn çš„çµ±ä¸€ APIï¼Œä¹ŸçŸ¥é“æ€éº¼å¿«é€Ÿåˆ‡æ›ä¸åŒæ¨¡å‹ã€‚
ä½†åœ¨çœŸå¯¦ä¸–ç•Œä¸­ï¼Œ**å…‰æœ‰æ¨¡å‹æ˜¯ä¸å¤ çš„**ã€‚

ä½ éœ€è¦ï¼š
- å…ˆæŠŠè³‡æ–™**æ¸…æ´—**å’Œ**æ¨™æº–åŒ–**
- ç„¶å¾Œæ‰èƒ½ä¸Ÿé€²æ¨¡å‹

å¦‚æœé€™äº›æ­¥é©Ÿæ²’æœ‰æ­£ç¢ºè™•ç†ï¼Œå°±æœƒç™¼ç”Ÿä¸€å€‹å«åš **ã€Œè³‡æ–™æ´©æ¼ï¼ˆData Leakageï¼‰ã€**
çš„å¯æ€•å•é¡Œâ€”â€”ä½ çš„æ¨¡å‹çœ‹èµ·ä¾†å¾ˆå²å®³ï¼Œä½†å…¶å¯¦åœ¨ä½œå¼Šã€‚

> **ä¸‹ä¸€ç« ï¼šPipeline æ€ç¶­ â€” ML ä¸æ˜¯æ¨¡å‹ï¼Œæ˜¯æµç¨‹** â†’
