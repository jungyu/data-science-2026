# ç¬¬ 15 ç« ï¼šæ¨¡å‹æ²»ç†èˆ‡ MLOps â€” è®“æ¨¡å‹æ´»åœ¨çœŸå¯¦ä¸–ç•Œ

> ã€Œæ¨¡å‹åœ¨ Jupyter Notebook è£¡è·‘å¾—å¾ˆå¥½ã€‚ã€
> ã€Œé‚£éƒ¨ç½²ä¸Šç·šå‘¢ï¼Ÿã€
> ã€Œâ€¦â€¦ä»€éº¼æ˜¯éƒ¨ç½²ï¼Ÿã€

---

## ğŸ¯ æœ¬ç« ç›®æ¨™

è®€å®Œé€™ä¸€ç« ï¼Œä½ å°‡èƒ½å¤ ï¼š

1. ç†è§£ç‚ºä»€éº¼ ML æ¨¡å‹éœ€è¦ **æ²»ç†ï¼ˆGovernanceï¼‰**
2. ä½¿ç”¨ sklearn çš„ **Pipeline** ä½œç‚ºæ²»ç†çš„åŸºç¤
3. ç”¨ **joblib** å„²å­˜å’Œè¼‰å…¥æ¨¡å‹
4. ç†è§£ **DVC** çš„æ¦‚å¿µï¼šç‚ºè³‡æ–™å’Œæ¨¡å‹åšç‰ˆæœ¬æ§åˆ¶
5. èªè­˜ **MLflow** çš„å¯¦é©—è¿½è¹¤åŠŸèƒ½
6. æŒæ¡ **å¯é‡ç¾æ€§ï¼ˆReproducibilityï¼‰** çš„è¦æ±‚
7. ç†è§£ **æ¨¡å‹ç›£æ§** å’Œ **æ¦‚å¿µæ¼‚ç§»ï¼ˆConcept Driftï¼‰**
8. æè¿°å®Œæ•´çš„ **MLOps ç”Ÿå‘½é€±æœŸ**

---

## ç‚ºä»€éº¼éœ€è¦æ¨¡å‹æ²»ç†ï¼Ÿ

### Notebook è‹±é›„ vs ç”Ÿç”¢ç’°å¢ƒ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                           â”‚
â”‚  Notebook è£¡çš„ä¸–ç•Œ          çœŸå¯¦ä¸–ç•Œ                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚
â”‚  è³‡æ–™æ°¸é ä¸è®Š               è³‡æ–™æ¯å¤©éƒ½åœ¨è®Š                  â”‚
â”‚  åªè·‘ä¸€æ¬¡å°±å¥½               è¦ 24/7 æŒçºŒé‹è¡Œ                â”‚
â”‚  è‡ªå·±ä¸€å€‹äººç”¨               æ•´å€‹åœ˜éšŠè¦å”ä½œ                  â”‚
â”‚  å‡ºéŒ¯å°±é‡è·‘                 å‡ºéŒ¯è¦æœ‰äººè¢«å«é†’                â”‚
â”‚  ã€Œæˆ‘è¨˜å¾—ä¸Šæ¬¡æ”¹äº†ä»€éº¼ã€     ã€Œä¸‰å€‹æœˆå‰æ˜¯èª°æ”¹äº†ä»€éº¼ï¼Ÿã€      â”‚
â”‚  éš¨ä¾¿ import               æ¯å€‹å¥—ä»¶éƒ½è¦é–ç‰ˆæœ¬              â”‚
â”‚                                                           â”‚
â”‚  90% çš„æ¨¡å‹æ­»åœ¨å¾ Notebook åˆ°ç”Ÿç”¢ç’°å¢ƒçš„è·¯ä¸Š                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ¨¡å‹æ²»ç†çš„å››å¤§æ”¯æŸ±

```
                    æ¨¡å‹æ²»ç†
                       â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
       â”‚       â”‚       â”‚       â”‚       â”‚
    å¯é‡ç¾æ€§  ç‰ˆæœ¬æ§åˆ¶  ç›£æ§    æ–‡ä»¶åŒ–
       â”‚       â”‚       â”‚       â”‚
   "èƒ½é‡è·‘"  "èƒ½å›æº¯"  "èƒ½é è­¦"  "èƒ½ç†è§£"
```

ğŸ’¡ **é‡é»è§€å¿µ**ï¼šæ¨¡å‹æ²»ç†ä¸æ˜¯å®˜åƒšä½œæ¥­ï¼Œè€Œæ˜¯ç¢ºä¿ä½ çš„æ¨¡å‹èƒ½åœ¨çœŸå¯¦ä¸–ç•Œä¸­
**æŒçºŒã€ç©©å®šã€å¯é ** åœ°é‹ä½œã€‚

---

## Pipeline å°±æ˜¯æ²»ç†çš„èµ·é»

### ç‚ºä»€éº¼ Pipeline å¾ˆé‡è¦ï¼Ÿ

æ²’æœ‰ Pipeline çš„ç¨‹å¼ç¢¼ï¼š

```python
# âŒ æ•£è½å„è™•çš„å‰è™•ç†æ­¥é©Ÿ
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

pca = PCA(n_components=10)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

model = RandomForestClassifier()
model.fit(X_train_pca, y_train)
y_pred = model.predict(X_test_pca)

# éƒ¨ç½²æ™‚è¦è¨˜å¾—ï¼šå…ˆ scale â†’ å† PCA â†’ å† predict
# ä¸‰å€‹æœˆå¾Œä½ æœƒå¿˜è¨˜é †åºçš„ ğŸ™ƒ
```

æœ‰ Pipeline çš„ç¨‹å¼ç¢¼ï¼š

```python
# âœ… ä¸€åˆ‡åŒ…åœ¨ä¸€èµ·
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=10)),
    ('classifier', RandomForestClassifier(random_state=42))
])

# ä¸€è¡Œæå®šè¨“ç·´
pipeline.fit(X_train, y_train)

# ä¸€è¡Œæå®šé æ¸¬ï¼ˆè‡ªå‹•è·‘å®Œæ‰€æœ‰å‰è™•ç†ï¼‰
y_pred = pipeline.predict(X_test)

# éƒ¨ç½²æ™‚åªéœ€è¦é€™ä¸€å€‹ pipeline ç‰©ä»¶
```

### Pipeline çš„æ²»ç†å„ªå‹¢

```
+------------------+------------------------------------------+
| æ²»ç†é¢å‘         | Pipeline å¦‚ä½•å¹«åŠ©                         |
+------------------+------------------------------------------+
| å¯é‡ç¾æ€§         | æ‰€æœ‰æ­¥é©Ÿå°è£åœ¨ä¸€èµ·ï¼Œé †åºå›ºå®š              |
| é˜²æ­¢è³‡æ–™æ´©æ¼     | fit å’Œ transform çš„é‚è¼¯è‡ªå‹•ç®¡ç†           |
| éƒ¨ç½²ç°¡åŒ–         | åªéœ€å„²å­˜/è¼‰å…¥ä¸€å€‹ç‰©ä»¶                     |
| åœ˜éšŠå”ä½œ         | æ˜ç¢ºå®šç¾©çš„è™•ç†æµç¨‹ï¼Œäººäººçœ‹å¾—æ‡‚            |
| ç‰ˆæœ¬æ§åˆ¶         | ä¸€å€‹ç‰©ä»¶ = ä¸€å€‹ç‰ˆæœ¬                       |
+------------------+------------------------------------------+
```

### é€²éšï¼šColumnTransformer + Pipeline

```python
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.ensemble import GradientBoostingClassifier

# å®šç¾©ä¸åŒé¡å‹çš„ç‰¹å¾µ
numeric_features = ['age', 'income', 'credit_score']
categorical_features = ['education', 'employment', 'region']

# ç‚ºä¸åŒç‰¹å¾µé¡å‹è¨­å®šä¸åŒçš„å‰è™•ç†
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ]
)

# å®Œæ•´ Pipeline
full_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', GradientBoostingClassifier(random_state=42))
])

# è¨“ç·´èˆ‡é æ¸¬
full_pipeline.fit(X_train, y_train)
y_pred = full_pipeline.predict(X_test)

# é€™å€‹ pipeline åŒ…å«äº†ã€Œä¸€åˆ‡ã€ï¼š
# - æ•¸å€¼ç‰¹å¾µæ¨™æº–åŒ–
# - é¡åˆ¥ç‰¹å¾µ One-Hot ç·¨ç¢¼
# - æ¨¡å‹æœ¬èº«
```

---

## ğŸ§  å‹•å‹•è…¦

> ç‚ºä»€éº¼ Pipeline å¯ä»¥ã€Œé˜²æ­¢è³‡æ–™æ´©æ¼ã€ï¼Ÿ
>
> æç¤ºï¼šæƒ³æƒ³å¦‚æœä½ å…ˆå° **æ•´å€‹è³‡æ–™é›†** åš StandardScalerï¼Œ
> ç„¶å¾Œæ‰åˆ†æˆ train/testï¼Œæœƒç™¼ç”Ÿä»€éº¼äº‹ï¼Ÿ
> Pipeline + cross_val_score æ˜¯æ€éº¼é¿å…é€™å€‹å•é¡Œçš„ï¼Ÿ

---

## ç”¨ joblib å„²å­˜æ¨¡å‹

è¨“ç·´å¥½çš„æ¨¡å‹éœ€è¦ **æŒä¹…åŒ–ï¼ˆPersistenceï¼‰**ï¼Œæ‰èƒ½åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­ä½¿ç”¨ã€‚

### åŸºæœ¬ç”¨æ³•

```python
import joblib
from datetime import datetime

# === å„²å­˜æ¨¡å‹ ===
model_path = f"models/fraud_detector_{datetime.now():%Y%m%d_%H%M}.joblib"
joblib.dump(full_pipeline, model_path)
print(f"æ¨¡å‹å·²å„²å­˜è‡³: {model_path}")
# æ¨¡å‹å·²å„²å­˜è‡³: models/fraud_detector_20260227_1430.joblib

# === è¼‰å…¥æ¨¡å‹ ===
loaded_pipeline = joblib.load(model_path)

# ç›´æ¥é æ¸¬ï¼ä¸éœ€è¦é‡æ–°è¨“ç·´
y_pred_loaded = loaded_pipeline.predict(X_test)

# é©—è­‰çµæœä¸€è‡´
import numpy as np
assert np.array_equal(y_pred, y_pred_loaded), "é æ¸¬çµæœä¸ä¸€è‡´ï¼"
print("é©—è­‰é€šéï¼šè¼‰å…¥çš„æ¨¡å‹é æ¸¬çµæœèˆ‡åŸå§‹ä¸€è‡´")
```

### joblib vs pickle

```
+------------------+-----------------+------------------+
| ç‰¹æ€§             | joblib          | pickle           |
+------------------+-----------------+------------------+
| NumPy é™£åˆ—æ•ˆç‡   | âœ… å„ªåŒ–          | âŒ ä¸€èˆ¬           |
| å£“ç¸®æ”¯æ´         | âœ… å…§å»º          | âŒ éœ€é¡å¤–è™•ç†      |
| sklearn æ¨è–¦     | âœ… å®˜æ–¹æ¨è–¦      | âš ï¸ å¯ç”¨ä½†ä¸æ¨è–¦   |
| å¤§å‹æ¨¡å‹         | âœ… é«˜æ•ˆ          | âŒ è¼ƒæ…¢           |
| å®‰å…¨æ€§           | âš ï¸ æ³¨æ„ä¾†æº      | âš ï¸ æ³¨æ„ä¾†æº       |
+------------------+-----------------+------------------+
```

### æ¨¡å‹å„²å­˜çš„æœ€ä½³å¯¦è¸

```python
import joblib
import json
from datetime import datetime
from sklearn.metrics import accuracy_score, f1_score

def save_model_with_metadata(pipeline, X_test, y_test,
                              model_name, version):
    """å„²å­˜æ¨¡å‹åŠå…¶å…ƒè³‡æ–™"""

    # 1. å„²å­˜æ¨¡å‹
    model_path = f"models/{model_name}_v{version}.joblib"
    joblib.dump(pipeline, model_path)

    # 2. å„²å­˜å…ƒè³‡æ–™
    y_pred = pipeline.predict(X_test)
    metadata = {
        'model_name': model_name,
        'version': version,
        'created_at': datetime.now().isoformat(),
        'sklearn_version': __import__('sklearn').__version__,
        'python_version': __import__('sys').version,
        'metrics': {
            'accuracy': float(accuracy_score(y_test, y_pred)),
            'f1_score': float(f1_score(y_test, y_pred, average='weighted')),
        },
        'pipeline_steps': [
            step[0] for step in pipeline.steps
        ],
        'n_features': X_test.shape[1],
        'n_test_samples': X_test.shape[0],
    }

    meta_path = f"models/{model_name}_v{version}_metadata.json"
    with open(meta_path, 'w') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)

    print(f"æ¨¡å‹å·²å„²å­˜: {model_path}")
    print(f"å…ƒè³‡æ–™å·²å„²å­˜: {meta_path}")
    return model_path, meta_path

# ä½¿ç”¨ç¯„ä¾‹
save_model_with_metadata(
    full_pipeline, X_test, y_test,
    model_name='fraud_detector',
    version='1.0.0'
)
```

---

## âš ï¸ å¸¸è¦‹é™·é˜±

### é™·é˜± 1ï¼šsklearn ç‰ˆæœ¬ä¸ä¸€è‡´

```python
# âŒ ç”¨ sklearn 1.3 è¨“ç·´çš„æ¨¡å‹ï¼Œåœ¨ sklearn 1.5 ä¸Šè¼‰å…¥
# å¯èƒ½æœƒå‡ºç¾ Warning ç”šè‡³ Error

# âœ… æ°¸é è¨˜éŒ„ sklearn ç‰ˆæœ¬
import sklearn
print(f"sklearn version: {sklearn.__version__}")

# âœ… åœ¨ requirements.txt ä¸­é–å®šç‰ˆæœ¬
# scikit-learn==1.4.2
```

### é™·é˜± 2ï¼šåªå­˜æ¨¡å‹ï¼Œä¸å­˜å‰è™•ç†

```python
# âŒ åªå­˜ modelï¼Œå¿˜äº† scaler å’Œ encoder
joblib.dump(model, 'model.joblib')
# éƒ¨ç½²æ™‚ï¼šã€Œscaler å‘¢ï¼Ÿencoder å‘¢ï¼Ÿã€

# âœ… ç”¨ Pipeline æŠŠä¸€åˆ‡åŒ…åœ¨ä¸€èµ·
joblib.dump(full_pipeline, 'pipeline.joblib')
# éƒ¨ç½²æ™‚ï¼šè¼‰å…¥ä¸€å€‹ç‰©ä»¶å°±æå®š
```

### é™·é˜± 3ï¼šå¾ä¸å—ä¿¡ä»»çš„ä¾†æºè¼‰å…¥æ¨¡å‹

```python
# âš ï¸ joblib.load æœƒåŸ·è¡Œä»»æ„ç¨‹å¼ç¢¼ï¼
# æ°¸é ä¸è¦è¼‰å…¥ä¾†æºä¸æ˜çš„ .joblib æª”æ¡ˆ

# âœ… åªè¼‰å…¥ä½ è‡ªå·±è¨“ç·´ä¸¦å„²å­˜çš„æ¨¡å‹
# âœ… ä½¿ç”¨ hash é©—è­‰æ¨¡å‹å®Œæ•´æ€§
import hashlib

def verify_model_integrity(model_path, expected_hash):
    with open(model_path, 'rb') as f:
        file_hash = hashlib.sha256(f.read()).hexdigest()
    if file_hash != expected_hash:
        raise ValueError(f"æ¨¡å‹æª”æ¡ˆå·²è¢«ç«„æ”¹ï¼")
    return True
```

---

## ç‰ˆæœ¬æ§åˆ¶ï¼šGit + DVC

### Git çš„é™åˆ¶

Git å¾ˆé©åˆè¿½è¹¤ç¨‹å¼ç¢¼ï¼Œä½† **ä¸é©åˆè¿½è¹¤å¤§å‹è³‡æ–™æª”å’Œæ¨¡å‹æª”**ã€‚

```
+-----------------------------------------------------+
|  Git èƒ½è¿½è¹¤çš„              Git ä¸é©åˆè¿½è¹¤çš„            |
|  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          |
|  .py ç¨‹å¼ç¢¼                train.csv (500MB)          |
|  .yaml è¨­å®šæª”              model.joblib (2GB)         |
|  requirements.txt          images/ (10GB)             |
|  Dockerfile                embeddings.npy (5GB)       |
+-----------------------------------------------------+
```

### DVCï¼ˆData Version Controlï¼‰çš„æ¦‚å¿µ

```
DVC çš„æ ¸å¿ƒæ€æƒ³ï¼š
ç”¨ Git è¿½è¹¤ã€ŒæŒ‡æ¨™ã€ï¼Œç”¨é ç«¯å„²å­˜è¿½è¹¤ã€Œå¯¦éš›æª”æ¡ˆã€ã€‚

å°±åƒåœ–æ›¸é¤¨çš„ç´¢å¼•å¡ç‰‡ï¼š
- å¡ç‰‡ï¼ˆ.dvc æª”æ¡ˆï¼‰æ”¾åœ¨ Git è£¡ â†’ å°æª”æ¡ˆï¼Œå¯è¿½è¹¤
- æ›¸æœ¬ï¼ˆå¯¦éš›è³‡æ–™ï¼‰æ”¾åœ¨æ›¸æ¶ä¸Š â†’ å¤§æª”æ¡ˆï¼Œå­˜åœ¨é ç«¯

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Git Repository                           â”‚
â”‚  â”œâ”€â”€ train.csv.dvc    â† æŒ‡æ¨™æª”ï¼ˆå¹¾ KBï¼‰  â”‚
â”‚  â”œâ”€â”€ model.joblib.dvc â† æŒ‡æ¨™æª”ï¼ˆå¹¾ KBï¼‰  â”‚
â”‚  â”œâ”€â”€ src/train.py     â† ç¨‹å¼ç¢¼           â”‚
â”‚  â””â”€â”€ params.yaml      â† è¶…åƒæ•¸è¨­å®š       â”‚
â”‚                                           â”‚
â”‚  Remote Storage (S3/GCS/Azure)            â”‚
â”‚  â”œâ”€â”€ train.csv        â† å¯¦éš›è³‡æ–™ (500MB) â”‚
â”‚  â””â”€â”€ model.joblib     â† å¯¦éš›æ¨¡å‹ (2GB)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### DVC åŸºæœ¬å·¥ä½œæµç¨‹

```bash
# 1. åˆå§‹åŒ– DVC
# dvc init

# 2. è¿½è¹¤å¤§å‹æª”æ¡ˆ
# dvc add data/train.csv
# â†’ ç”¢ç”Ÿ data/train.csv.dvcï¼ˆæŒ‡æ¨™æª”ï¼‰
# â†’ åŸå§‹æª”æ¡ˆåŠ å…¥ .gitignore

# 3. Git è¿½è¹¤æŒ‡æ¨™æª”
# git add data/train.csv.dvc data/.gitignore
# git commit -m "è¿½è¹¤è¨“ç·´è³‡æ–™ v1"

# 4. æ¨é€åˆ°é ç«¯
# dvc push  # è³‡æ–™æ¨åˆ° S3/GCS
# git push  # æŒ‡æ¨™æ¨åˆ° Git

# 5. åœ˜éšŠæˆå“¡å–å¾—è³‡æ–™
# git pull
# dvc pull  # è‡ªå‹•å¾é ç«¯ä¸‹è¼‰å°æ‡‰ç‰ˆæœ¬çš„è³‡æ–™

# 6. åˆ‡æ›åˆ°èˆŠç‰ˆæœ¬
# git checkout v1.0
# dvc checkout  # è‡ªå‹•å–å¾— v1.0 å°æ‡‰çš„è³‡æ–™
```

ğŸ’¡ **é‡é»è§€å¿µ**ï¼šDVC è®“ä½ çš„è³‡æ–™å’Œæ¨¡å‹ä¹Ÿæœ‰ã€Œæ™‚å…‰æ©Ÿã€ã€‚ä»»ä½•æ™‚å€™éƒ½å¯ä»¥
å›åˆ°æŸå€‹æ­·å²ç‰ˆæœ¬ï¼Œå–å¾—ç•¶æ™‚çš„ç¨‹å¼ç¢¼ **å’Œ** è³‡æ–™ã€‚

---

## â“ æ²’æœ‰ç¬¨å•é¡Œ

**Qï¼šæˆ‘å€‘çœŸçš„éœ€è¦ DVC å—ï¼Ÿä¸èƒ½æŠŠè³‡æ–™æ”¾åœ¨å…±ç”¨è³‡æ–™å¤¾ï¼Ÿ**

Aï¼šå¯ä»¥ï¼Œä½†ä½ æœƒé‡åˆ°å•é¡Œï¼šã€Œæ˜¨å¤©é‚£ç‰ˆè³‡æ–™å»å“ªäº†ï¼Ÿã€ã€ã€Œèª°æ”¹äº†è¨“ç·´è³‡æ–™ï¼Ÿã€ã€
ã€Œä¸‰å€‹æœˆå‰çš„æ¨¡å‹æ˜¯ç”¨å“ªç‰ˆè³‡æ–™è¨“çš„ï¼Ÿã€DVC è§£æ±ºçš„å°±æ˜¯é€™äº›è¿½æº¯å•é¡Œã€‚

**Qï¼šDVC å’Œ Git LFS æœ‰ä»€éº¼ä¸åŒï¼Ÿ**

Aï¼šGit LFS æŠŠå¤§æª”æ¡ˆå­˜åœ¨ Git ä¼ºæœå™¨æ—é‚Šï¼Œæ¯å€‹ç‰ˆæœ¬éƒ½ä½”ç©ºé–“ã€‚
DVC æ›´éˆæ´»ï¼Œå¯ä»¥ç”¨ S3ã€GCSã€Azure Blob ç­‰å„ç¨®å„²å­˜å¾Œç«¯ï¼Œ
è€Œä¸”æ”¯æ´ ML Pipeline çš„è¿½è¹¤ã€‚

**Qï¼šå°å°ˆæ¡ˆä¹Ÿéœ€è¦ DVC å—ï¼Ÿ**

Aï¼šå¦‚æœä½ çš„è³‡æ–™ < 100MB ä¸”å¾ˆå°‘è®Šå‹•ï¼ŒGit LFS æˆ–ç”šè‡³ç›´æ¥æ”¾ Git éƒ½è¡Œã€‚
ä½†ä¸€æ—¦è³‡æ–™é–‹å§‹å¢é•·ï¼Œæˆ–æœ‰å¤šäººå”ä½œï¼ŒDVC å°±å€¼å¾—æŠ•è³‡äº†ã€‚

**Qï¼šDVC å…è²»å—ï¼Ÿ**

Aï¼šDVC æœ¬èº«æ˜¯é–‹æºå…è²»çš„ã€‚å„²å­˜å¾Œç«¯ï¼ˆS3ã€GCS ç­‰ï¼‰çš„è²»ç”¨å–æ±ºæ–¼ä½ ç”¨å¤šå°‘ç©ºé–“ã€‚

---

## å¯¦é©—è¿½è¹¤ï¼šMLflow æ¦‚å¿µ

### ç‚ºä»€éº¼éœ€è¦å¯¦é©—è¿½è¹¤ï¼Ÿ

```
ä½ çš„ Notebook æ­·å²ï¼š

å˜—è©¦ #1: RandomForest, n=100, AUC=0.85
å˜—è©¦ #2: RandomForest, n=200, max_depth=10, AUC=0.87
å˜—è©¦ #3: GradientBoosting, lr=0.1, AUC=0.89
å˜—è©¦ #4: æ”¹äº†ä»€éº¼ä¾†è‘—ï¼Ÿå¿˜äº†â€¦ AUC=0.91 â† æœ€å¥½çš„ï¼
å˜—è©¦ #5: æƒ³é‡ç¾ #4ï¼Œä½†è·‘ä¸å‡ºä¸€æ¨£çš„çµæœâ€¦

ä½ ï¼šã€Œæˆ‘åˆ°åº•æ”¹äº†ä»€éº¼æ‰å¾—åˆ° 0.91 çš„ï¼Ÿï¼Ÿï¼Ÿã€
```

### MLflow çš„æ ¸å¿ƒæ¦‚å¿µ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  MLflow                      â”‚
â”‚                                              â”‚
â”‚  Experiment: ã€Œä¿¡ç”¨å¡è©æ¬ºåµæ¸¬ã€              â”‚
â”‚  â”œâ”€â”€ Run #1                                  â”‚
â”‚  â”‚   â”œâ”€â”€ Parameters: {n=100, depth=None}     â”‚
â”‚  â”‚   â”œâ”€â”€ Metrics: {auc=0.85, f1=0.72}       â”‚
â”‚  â”‚   â”œâ”€â”€ Artifacts: model_v1.joblib          â”‚
â”‚  â”‚   â””â”€â”€ Tags: {author=aaron, stage=dev}     â”‚
â”‚  â”œâ”€â”€ Run #2                                  â”‚
â”‚  â”‚   â”œâ”€â”€ Parameters: {n=200, depth=10}       â”‚
â”‚  â”‚   â”œâ”€â”€ Metrics: {auc=0.87, f1=0.76}       â”‚
â”‚  â”‚   â””â”€â”€ ...                                 â”‚
â”‚  â””â”€â”€ Run #3                                  â”‚
â”‚      â”œâ”€â”€ Parameters: {model=GB, lr=0.1}      â”‚
â”‚      â”œâ”€â”€ Metrics: {auc=0.89, f1=0.81}       â”‚
â”‚      â””â”€â”€ ...                                 â”‚
â”‚                                              â”‚
â”‚  â†’ æ‰€æœ‰å˜—è©¦éƒ½è¢«å®Œæ•´è¨˜éŒ„ï¼Œæ°¸é å¯ä»¥å›æº¯       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### MLflow æ¦‚å¿µç¨‹å¼ç¢¼

```python
# å®‰è£ï¼špip install mlflow
# ä»¥ä¸‹ç‚ºæ¦‚å¿µç¤ºç¯„

"""
import mlflow
import mlflow.sklearn

# è¨­å®šå¯¦é©—åç¨±
mlflow.set_experiment("fraud_detection")

# é–‹å§‹ä¸€æ¬¡å¯¦é©—
with mlflow.start_run(run_name="gradient_boosting_v1"):

    # è¨˜éŒ„è¶…åƒæ•¸
    params = {
        'n_estimators': 200,
        'learning_rate': 0.1,
        'max_depth': 5,
        'random_state': 42,
    }
    mlflow.log_params(params)

    # è¨“ç·´æ¨¡å‹
    model = GradientBoostingClassifier(**params)
    model.fit(X_train, y_train)

    # è¨˜éŒ„æŒ‡æ¨™
    y_pred = model.predict(X_test)
    metrics = {
        'accuracy': accuracy_score(y_test, y_pred),
        'f1_score': f1_score(y_test, y_pred),
        'auc_roc': roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]),
    }
    mlflow.log_metrics(metrics)

    # å„²å­˜æ¨¡å‹ä½œç‚º artifact
    mlflow.sklearn.log_model(model, "model")

    # è¨˜éŒ„é¡å¤–è³‡è¨Š
    mlflow.set_tag("author", "aaron")
    mlflow.set_tag("stage", "development")

    print(f"Run ID: {mlflow.active_run().info.run_id}")
    print(f"Metrics: {metrics}")

# å•Ÿå‹• MLflow UI
# mlflow ui --port 5000
# ç„¶å¾Œæ‰“é–‹ http://localhost:5000 æŸ¥çœ‹æ‰€æœ‰å¯¦é©—
"""
```

### ä¸ç”¨ MLflow çš„ç°¡æ˜“æ›¿ä»£æ–¹æ¡ˆ

å¦‚æœä¸æƒ³å®‰è£ MLflowï¼Œå¯ä»¥ç”¨ç°¡å–®çš„ JSON æ—¥èªŒï¼š

```python
import json
from datetime import datetime
import os

class SimpleExperimentTracker:
    """ç°¡æ˜“å¯¦é©—è¿½è¹¤å™¨"""

    def __init__(self, experiment_name, log_dir='experiments'):
        self.experiment_name = experiment_name
        self.log_dir = log_dir
        os.makedirs(log_dir, exist_ok=True)

    def log_run(self, params, metrics, notes=""):
        """è¨˜éŒ„ä¸€æ¬¡å¯¦é©—"""
        run = {
            'experiment': self.experiment_name,
            'timestamp': datetime.now().isoformat(),
            'params': params,
            'metrics': metrics,
            'notes': notes,
        }

        # å­˜åˆ° JSON æª”
        log_file = os.path.join(self.log_dir,
                                f"{self.experiment_name}.jsonl")
        with open(log_file, 'a') as f:
            f.write(json.dumps(run, ensure_ascii=False) + '\n')

        print(f"å·²è¨˜éŒ„å¯¦é©—: {metrics}")
        return run

    def get_best_run(self, metric='accuracy'):
        """æ‰¾å‡ºæœ€ä½³å¯¦é©—"""
        log_file = os.path.join(self.log_dir,
                                f"{self.experiment_name}.jsonl")
        best = None
        with open(log_file, 'r') as f:
            for line in f:
                run = json.loads(line)
                if best is None or run['metrics'][metric] > best['metrics'][metric]:
                    best = run
        return best

# ä½¿ç”¨ç¯„ä¾‹
tracker = SimpleExperimentTracker("fraud_detection")

tracker.log_run(
    params={'model': 'RandomForest', 'n_estimators': 100},
    metrics={'accuracy': 0.95, 'f1': 0.72, 'auc': 0.85},
    notes='åŸºç·šæ¨¡å‹'
)

tracker.log_run(
    params={'model': 'GradientBoosting', 'n_estimators': 200, 'lr': 0.1},
    metrics={'accuracy': 0.96, 'f1': 0.81, 'auc': 0.89},
    notes='èª¿æ•´å­¸ç¿’ç‡å¾Œæ•ˆæœæå‡'
)

best = tracker.get_best_run(metric='auc')
print(f"\næœ€ä½³å¯¦é©—: AUC={best['metrics']['auc']}, "
      f"æ¨¡å‹={best['params']['model']}")
```

---

## å¯é‡ç¾æ€§ï¼ˆReproducibilityï¼‰

### å¯é‡ç¾æ€§æ¸…å–®

```
ä½ çš„å¯¦é©—èƒ½è¢«é‡ç¾å—ï¼Ÿæª¢æŸ¥ä»¥ä¸‹é …ç›®ï¼š

+------+----------------------------------+---------------+
| ç·¨è™Ÿ | é …ç›®                             | å¦‚ä½•ç¢ºä¿       |
+------+----------------------------------+---------------+
|  1   | Python ç‰ˆæœ¬                      | pyenv / conda |
|  2   | å¥—ä»¶ç‰ˆæœ¬                         | requirements  |
|  3   | éš¨æ©Ÿç¨®å­                         | random_state  |
|  4   | è³‡æ–™ç‰ˆæœ¬                         | DVC / hash    |
|  5   | è¶…åƒæ•¸                           | config æª”     |
|  6   | å‰è™•ç†æ­¥é©Ÿ                       | Pipeline      |
|  7   | ç‰¹å¾µå·¥ç¨‹                         | ç¨‹å¼ç¢¼ç‰ˆæœ¬     |
|  8   | è¨“ç·´/æ¸¬è©¦åˆ†å‰²                    | random_state  |
|  9   | ç¡¬é«”ç’°å¢ƒ                         | Docker        |
| 10   | ä½œæ¥­ç³»çµ±                         | Docker        |
+------+----------------------------------+---------------+
```

### å¯¦ä½œå¯é‡ç¾æ€§

```python
"""
å¯é‡ç¾æ€§ç¯„æœ¬
"""
import random
import numpy as np
from sklearn.model_selection import train_test_split

# ===== 1. å›ºå®šæ‰€æœ‰éš¨æ©Ÿç¨®å­ =====
RANDOM_SEED = 42

random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

# ===== 2. è¨˜éŒ„ç’°å¢ƒè³‡è¨Š =====
def log_environment():
    import sys, sklearn, platform
    env_info = {
        'python': sys.version,
        'sklearn': sklearn.__version__,
        'numpy': np.__version__,
        'platform': platform.platform(),
        'random_seed': RANDOM_SEED,
    }
    print("=== ç’°å¢ƒè³‡è¨Š ===")
    for k, v in env_info.items():
        print(f"  {k}: {v}")
    return env_info

log_environment()

# ===== 3. è³‡æ–™è¼‰å…¥èˆ‡åˆ†å‰²ï¼ˆå›ºå®šç¨®å­ï¼‰=====
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.3,
    random_state=RANDOM_SEED,  # æ¯æ¬¡åˆ†å‰²éƒ½ä¸€æ¨£
    stratify=y                  # ä¿æŒé¡åˆ¥æ¯”ä¾‹
)

# ===== 4. å»ºç«‹ Pipelineï¼ˆæ‰€æœ‰æ­¥é©Ÿå°è£ï¼‰=====
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingClassifier

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', GradientBoostingClassifier(
        n_estimators=200,
        learning_rate=0.1,
        max_depth=5,
        random_state=RANDOM_SEED  # æ¨¡å‹ä¹Ÿè¦å›ºå®šç¨®å­
    ))
])

# ===== 5. è¨“ç·´èˆ‡è©•ä¼° =====
pipeline.fit(X_train, y_train)

# ===== 6. å„²å­˜ä¸€åˆ‡ =====
import joblib, json

joblib.dump(pipeline, 'models/pipeline_v1.joblib')
json.dump({
    'random_seed': RANDOM_SEED,
    'test_size': 0.3,
    'pipeline_params': pipeline.get_params(),
}, open('models/pipeline_v1_config.json', 'w'), indent=2)
```

---

## ğŸ§  å‹•å‹•è…¦

> ä½ çš„åŒäº‹èªªï¼šã€Œæˆ‘è·‘äº†ä¸€æ¨£çš„ç¨‹å¼ç¢¼ï¼Œä½†çµæœä¸ä¸€æ¨£ï¼ã€
>
> åˆ—å‡ºè‡³å°‘ 5 å€‹å¯èƒ½çš„åŸå› ã€‚
>
> æç¤ºï¼šæƒ³æƒ³éš¨æ©Ÿç¨®å­ã€å¥—ä»¶ç‰ˆæœ¬ã€è³‡æ–™ã€ç¡¬é«”â€¦â€¦

---

## æ¨¡å‹ç›£æ§èˆ‡æ¦‚å¿µæ¼‚ç§»

### ä»€éº¼æ˜¯æ¦‚å¿µæ¼‚ç§»ï¼ˆConcept Driftï¼‰ï¼Ÿ

```
æ¨¡å‹æ˜¯åœ¨ã€Œéå»çš„è³‡æ–™ã€ä¸Šè¨“ç·´çš„ã€‚
ä½†ä¸–ç•Œæœƒè®Šã€‚

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                    â”‚
â”‚  è¨“ç·´æ™‚æœŸï¼ˆ2024ï¼‰         éƒ¨ç½²å¾Œï¼ˆ2026ï¼‰            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”‚
â”‚  è©æ¬ºæ‰‹æ³•ï¼šç›œåˆ·          è©æ¬ºæ‰‹æ³•ï¼šAI æ·±å½èªéŸ³     â”‚
â”‚  å®¢æˆ¶è¡Œç‚ºï¼šå¯¦é«”åº—æ¶ˆè²»     å®¢æˆ¶è¡Œç‚ºï¼šå…¨é¢ç·šä¸Šæ”¯ä»˜    â”‚
â”‚  ç¶“æ¿Ÿç’°å¢ƒï¼šä½åˆ©ç‡         ç¶“æ¿Ÿç’°å¢ƒï¼šé«˜é€šè†¨          â”‚
â”‚                                                    â”‚
â”‚  æ¨¡å‹å­¸åˆ°çš„ã€Œè©æ¬ºé•·ä»€éº¼æ¨£ã€å·²ç¶“éæ™‚äº†ï¼            â”‚
â”‚                                                    â”‚
â”‚  é€™å°±æ˜¯æ¦‚å¿µæ¼‚ç§»ã€‚                                  â”‚
â”‚                                                    â”‚
â”‚  é æ¸¬æº–ç¢ºåº¦                                        â”‚
â”‚    ^                                               â”‚
â”‚    |â”€â”€â”€â”€â”€â”€\                                        â”‚
â”‚    |       \                                       â”‚
â”‚    |        \________                              â”‚
â”‚    |                 \________                     â”‚
â”‚    +â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> æ™‚é–“            â”‚
â”‚    éƒ¨ç½²   3å€‹æœˆ    6å€‹æœˆ    12å€‹æœˆ                  â”‚
â”‚                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ¦‚å¿µæ¼‚ç§»çš„ç¨®é¡

```
+------------------+------------------------+-----------------------+
| é¡å‹             | æè¿°                   | ä¾‹å­                  |
+------------------+------------------------+-----------------------+
| çªç„¶æ¼‚ç§»         | è³‡æ–™åˆ†ä½ˆçªç„¶æ”¹è®Š       | COVID-19 æ”¹è®Šæ¶ˆè²»è¡Œç‚º |
| æ¼¸é€²æ¼‚ç§»         | æ…¢æ…¢è®ŠåŒ–               | å®¢æˆ¶åå¥½é€å¹´æ”¹è®Š      |
| é€±æœŸæ¼‚ç§»         | å­£ç¯€æ€§æˆ–é€±æœŸæ€§è®ŠåŒ–     | è–èª•ç¯€æ¶ˆè²»æš´å¢        |
| é‡ç¾æ¼‚ç§»         | èˆŠæ¨¡å¼é‡æ–°å‡ºç¾         | ç¶“æ¿Ÿé€±æœŸ              |
+------------------+------------------------+-----------------------+
```

### ç›£æ§ç­–ç•¥

```python
"""
æ¨¡å‹ç›£æ§ï¼šåµæ¸¬æ¦‚å¿µæ¼‚ç§»
"""
from sklearn.metrics import accuracy_score, f1_score
import numpy as np

class ModelMonitor:
    """ç°¡æ˜“æ¨¡å‹ç›£æ§å™¨"""

    def __init__(self, baseline_metrics, alert_threshold=0.05):
        """
        baseline_metrics: æ¨¡å‹éƒ¨ç½²æ™‚çš„åŸºæº–æŒ‡æ¨™
        alert_threshold: æŒ‡æ¨™ä¸‹é™å¤šå°‘æ™‚ç™¼å‡ºè­¦å ±
        """
        self.baseline = baseline_metrics
        self.threshold = alert_threshold
        self.history = []

    def check(self, y_true, y_pred, period_name=""):
        """æª¢æŸ¥ç•¶å‰æ•ˆèƒ½æ˜¯å¦æ¼‚ç§»"""
        current_metrics = {
            'accuracy': accuracy_score(y_true, y_pred),
            'f1_score': f1_score(y_true, y_pred, average='weighted'),
        }

        alerts = []
        for metric, value in current_metrics.items():
            baseline_value = self.baseline[metric]
            drop = baseline_value - value

            if drop > self.threshold:
                alerts.append({
                    'metric': metric,
                    'baseline': baseline_value,
                    'current': value,
                    'drop': drop,
                    'severity': 'HIGH' if drop > self.threshold * 2 else 'MEDIUM'
                })

        result = {
            'period': period_name,
            'metrics': current_metrics,
            'alerts': alerts,
            'status': 'ALERT' if alerts else 'OK'
        }
        self.history.append(result)

        # è¼¸å‡ºå ±å‘Š
        print(f"\n{'='*50}")
        print(f"æ¨¡å‹ç›£æ§å ±å‘Š - {period_name}")
        print(f"{'='*50}")
        for metric, value in current_metrics.items():
            baseline = self.baseline[metric]
            diff = value - baseline
            symbol = '+' if diff >= 0 else ''
            status = 'OK' if abs(diff) <= self.threshold else 'ALERT'
            print(f"  {metric}: {value:.4f} "
                  f"(åŸºæº–: {baseline:.4f}, {symbol}{diff:.4f}) "
                  f"[{status}]")

        if alerts:
            print(f"\n  *** è­¦å ± ***")
            for alert in alerts:
                print(f"  [{alert['severity']}] {alert['metric']} "
                      f"ä¸‹é™ {alert['drop']:.4f}")
            print(f"  å»ºè­°ï¼šè€ƒæ…®é‡æ–°è¨“ç·´æ¨¡å‹")
        else:
            print(f"\n  ç‹€æ…‹: æ­£å¸¸é‹ä½œä¸­")

        return result

# ä½¿ç”¨ç¯„ä¾‹
monitor = ModelMonitor(
    baseline_metrics={'accuracy': 0.95, 'f1_score': 0.88},
    alert_threshold=0.05
)

# æ¨¡æ“¬æ¯æœˆç›£æ§
# monitor.check(y_true_jan, y_pred_jan, "2026-01")
# monitor.check(y_true_feb, y_pred_feb, "2026-02")
```

### è³‡æ–™æ¼‚ç§»åµæ¸¬

```python
def detect_data_drift(reference_data, current_data,
                       feature_names, threshold=0.1):
    """
    ç°¡æ˜“è³‡æ–™æ¼‚ç§»åµæ¸¬ï¼šæ¯”è¼ƒç‰¹å¾µåˆ†ä½ˆ
    ä½¿ç”¨ KS æª¢å®šï¼ˆKolmogorov-Smirnov testï¼‰
    """
    from scipy import stats

    drift_report = []

    for i, name in enumerate(feature_names):
        ref = reference_data[:, i]
        cur = current_data[:, i]

        # KS æª¢å®š
        statistic, p_value = stats.ks_2samp(ref, cur)

        is_drift = p_value < threshold
        drift_report.append({
            'feature': name,
            'ks_statistic': statistic,
            'p_value': p_value,
            'drift_detected': is_drift
        })

    # è¼¸å‡ºå ±å‘Š
    print(f"{'ç‰¹å¾µ':>20s} | {'KSçµ±è¨ˆé‡':>8s} | {'p-value':>8s} | æ¼‚ç§»")
    print("-" * 55)
    for r in sorted(drift_report, key=lambda x: x['ks_statistic'],
                     reverse=True):
        flag = 'YES' if r['drift_detected'] else 'no'
        print(f"{r['feature']:>20s} | {r['ks_statistic']:>8.4f} | "
              f"{r['p_value']:>8.4f} | {flag}")

    n_drift = sum(1 for r in drift_report if r['drift_detected'])
    print(f"\nåµæ¸¬åˆ° {n_drift}/{len(feature_names)} å€‹ç‰¹å¾µæœ‰æ¼‚ç§»")

    return drift_report
```

---

## MLOps ç”Ÿå‘½é€±æœŸ

### å®Œæ•´æµç¨‹

```
MLOps ç”Ÿå‘½é€±æœŸï¼š

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚          â”‚    â”‚          â”‚    â”‚          â”‚    â”‚          â”‚
  â”‚  è¨“ç·´    â”‚â”€â”€â”€â–ºâ”‚  é©—è­‰    â”‚â”€â”€â”€â–ºâ”‚  éƒ¨ç½²    â”‚â”€â”€â”€â–ºâ”‚  ç›£æ§    â”‚
  â”‚  Train   â”‚    â”‚ Validate â”‚    â”‚  Deploy  â”‚    â”‚ Monitor  â”‚
  â”‚          â”‚    â”‚          â”‚    â”‚          â”‚    â”‚          â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â–²                                               â”‚
       â”‚                                               â”‚
       â”‚              æ¼‚ç§»åµæ¸¬ / æ•ˆèƒ½ä¸‹é™               â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      é‡æ–°è¨“ç·´ï¼ˆRe-trainï¼‰

æ¯å€‹éšæ®µçš„é—œéµæ´»å‹•ï¼š

è¨“ç·´ï¼ˆTrainï¼‰:
  - è³‡æ–™å‰è™•ç†èˆ‡ç‰¹å¾µå·¥ç¨‹
  - æ¨¡å‹é¸æ“‡èˆ‡è¶…åƒæ•¸èª¿æ•´
  - äº¤å‰é©—è­‰
  - å¯¦é©—è¿½è¹¤ï¼ˆMLflowï¼‰

é©—è­‰ï¼ˆValidateï¼‰:
  - æ¸¬è©¦é›†æ•ˆèƒ½è©•ä¼°
  - å•†æ¥­æŒ‡æ¨™è¨ˆç®—ï¼ˆç¬¬ 13 ç« ï¼‰
  - å¯è§£é‡‹æ€§å ±å‘Šï¼ˆç¬¬ 14 ç« ï¼‰
  - A/B æ¸¬è©¦è¨­è¨ˆ

éƒ¨ç½²ï¼ˆDeployï¼‰:
  - æ¨¡å‹åºåˆ—åŒ–ï¼ˆjoblibï¼‰
  - API åŒ…è£ï¼ˆFlask/FastAPIï¼‰
  - å®¹å™¨åŒ–ï¼ˆDockerï¼‰
  - æ¼¸é€²å¼ä¸Šç·šï¼ˆCanary Releaseï¼‰

ç›£æ§ï¼ˆMonitorï¼‰:
  - æ•ˆèƒ½æŒ‡æ¨™è¿½è¹¤
  - è³‡æ–™æ¼‚ç§»åµæ¸¬
  - å•†æ¥­æŒ‡æ¨™ç›£æ§
  - è­¦å ±èˆ‡é€šçŸ¥
```

### MLOps æˆç†Ÿåº¦ç­‰ç´š

```
+--------+------------------+------------------------------------+
| ç­‰ç´š   | åç¨±             | æè¿°                               |
+--------+------------------+------------------------------------+
| Level 0| æ‰‹å‹•æµç¨‹         | Notebook è¨“ç·´ï¼Œæ‰‹å‹•éƒ¨ç½²             |
| Level 1| ML Pipeline      | è‡ªå‹•åŒ–è¨“ç·´ Pipelineï¼Œæ‰‹å‹•éƒ¨ç½²       |
| Level 2| CI/CD for ML     | è‡ªå‹•åŒ–è¨“ç·´ + è‡ªå‹•åŒ–éƒ¨ç½²             |
| Level 3| Full MLOps       | è‡ªå‹•åŒ–è¨“ç·´ + éƒ¨ç½² + ç›£æ§ + é‡è¨“    |
+--------+------------------+------------------------------------+

å¤§éƒ¨åˆ†åœ˜éšŠåœ¨ Level 0-1ã€‚
é”åˆ° Level 2 å°±å·²ç¶“å¾ˆå²å®³äº†ã€‚
Level 3 é€šå¸¸æ˜¯å¤§å‹ç§‘æŠ€å…¬å¸çš„ç›®æ¨™ã€‚
```

---

## æ–‡ä»¶åŒ–è¦æ±‚

### æ¨¡å‹å¡ç‰‡ï¼ˆModel Cardï¼‰

Google æå‡ºçš„æ¨¡å‹æ–‡ä»¶æ¨™æº–ï¼š

```python
MODEL_CARD_TEMPLATE = """
# æ¨¡å‹å¡ç‰‡ï¼š{model_name}

## æ¨¡å‹æ¦‚è¿°
- **æ¨¡å‹é¡å‹**: {model_type}
- **ç‰ˆæœ¬**: {version}
- **è¨“ç·´æ—¥æœŸ**: {training_date}
- **è² è²¬äºº**: {owner}

## é æœŸç”¨é€”
- **ä¸»è¦ç”¨é€”**: {primary_use}
- **ä¸é©ç”¨å ´æ™¯**: {out_of_scope}
- **ç›®æ¨™ä½¿ç”¨è€…**: {target_users}

## è¨“ç·´è³‡æ–™
- **è³‡æ–™ä¾†æº**: {data_source}
- **è³‡æ–™é‡**: {data_size}
- **æ™‚é–“ç¯„åœ**: {date_range}
- **è³‡æ–™ç‰ˆæœ¬**: {data_version}

## æ•ˆèƒ½æŒ‡æ¨™
| æŒ‡æ¨™       | æ•´é«”  | å­ç¾¤é«” A | å­ç¾¤é«” B |
|-----------|-------|---------|---------|
| Accuracy  | {acc} | {acc_a} | {acc_b} |
| F1 Score  | {f1}  | {f1_a}  | {f1_b}  |
| AUC-ROC   | {auc} | {auc_a} | {auc_b} |

## é™åˆ¶èˆ‡åå·®
- {limitation_1}
- {limitation_2}
- {bias_analysis}

## å€«ç†è€ƒé‡
- {ethical_consideration_1}
- {ethical_consideration_2}

## ç›£æ§è¨ˆç•«
- **ç›£æ§æŒ‡æ¨™**: {monitoring_metrics}
- **é‡è¨“è§¸ç™¼æ¢ä»¶**: {retrain_trigger}
- **ç›£æ§é »ç‡**: {monitoring_frequency}
"""

def generate_model_card(model_info):
    """ç”¢ç”Ÿæ¨¡å‹å¡ç‰‡"""
    return MODEL_CARD_TEMPLATE.format(**model_info)
```

### æœ€ä½æ–‡ä»¶è¦æ±‚

```
+------+----------------------------------+--------------------+
| å„ªå…ˆ | æ–‡ä»¶                             | æ ¼å¼               |
+------+----------------------------------+--------------------+
| å¿…è¦ | æ¨¡å‹å¡ç‰‡ï¼ˆModel Cardï¼‰           | Markdown           |
| å¿…è¦ | è¶…åƒæ•¸èˆ‡è¨­å®š                     | YAML/JSON          |
| å¿…è¦ | å¥—ä»¶ç‰ˆæœ¬                         | requirements.txt   |
| å¿…è¦ | æ•ˆèƒ½æŒ‡æ¨™                         | JSON               |
| å»ºè­° | è¨“ç·´æµç¨‹æ–‡ä»¶                     | Markdown           |
| å»ºè­° | è³‡æ–™å­—å…¸                         | CSV/Markdown       |
| å»ºè­° | æ¼‚ç§»ç›£æ§å ±å‘Š                     | è‡ªå‹•ç”¢ç”Ÿ           |
| é€²éš | å…¬å¹³æ€§åˆ†æå ±å‘Š                   | Markdown/HTML      |
+------+----------------------------------+--------------------+
```

---

## å®Œæ•´ç¯„ä¾‹ï¼šå¾è¨“ç·´åˆ°æ²»ç†

```python
"""
å®Œæ•´ MLOps æµç¨‹ç¯„ä¾‹
"""
import json
import joblib
import numpy as np
from datetime import datetime
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
from sklearn.datasets import make_classification

# ===== è¨­å®š =====
CONFIG = {
    'random_seed': 42,
    'test_size': 0.3,
    'model_params': {
        'n_estimators': 200,
        'learning_rate': 0.1,
        'max_depth': 5,
    },
    'version': '1.0.0',
    'author': 'data-science-team',
}

# ===== Step 1: è³‡æ–™ =====
X, y = make_classification(n_samples=10000, n_features=20,
                            random_state=CONFIG['random_seed'])
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=CONFIG['test_size'],
    random_state=CONFIG['random_seed'], stratify=y
)

# ===== Step 2: Pipeline =====
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', GradientBoostingClassifier(
        **CONFIG['model_params'],
        random_state=CONFIG['random_seed']
    ))
])

# ===== Step 3: è¨“ç·´èˆ‡äº¤å‰é©—è­‰ =====
cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='f1')
print(f"CV F1: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")

pipeline.fit(X_train, y_train)

# ===== Step 4: è©•ä¼° =====
y_pred = pipeline.predict(X_test)
y_proba = pipeline.predict_proba(X_test)[:, 1]

metrics = {
    'accuracy': float(accuracy_score(y_test, y_pred)),
    'f1_score': float(f1_score(y_test, y_pred)),
    'auc_roc': float(roc_auc_score(y_test, y_proba)),
    'cv_f1_mean': float(cv_scores.mean()),
    'cv_f1_std': float(cv_scores.std()),
}

# ===== Step 5: å„²å­˜ =====
model_name = f"model_v{CONFIG['version']}"
joblib.dump(pipeline, f"models/{model_name}.joblib")

# å„²å­˜å®Œæ•´çš„å…ƒè³‡æ–™
metadata = {
    'config': CONFIG,
    'metrics': metrics,
    'created_at': datetime.now().isoformat(),
    'environment': {
        'sklearn': __import__('sklearn').__version__,
        'numpy': np.__version__,
        'python': __import__('sys').version,
    },
    'data_info': {
        'n_train': len(X_train),
        'n_test': len(X_test),
        'n_features': X_train.shape[1],
    }
}

with open(f"models/{model_name}_metadata.json", 'w') as f:
    json.dump(metadata, f, indent=2, ensure_ascii=False)

print(f"\næ¨¡å‹å·²å„²å­˜: models/{model_name}.joblib")
print(f"å…ƒè³‡æ–™å·²å„²å­˜: models/{model_name}_metadata.json")
print(f"æ•ˆèƒ½æŒ‡æ¨™: {metrics}")
```

---

## æœ¬ç« ç¸½çµ

```
+----------------------------------------------------------+
|              æ¨¡å‹æ²»ç†èˆ‡ MLOps é€ŸæŸ¥è¡¨                       |
+----------------------------------------------------------+
|                                                            |
|  æ²»ç†å››å¤§æ”¯æŸ±ï¼š                                            |
|                                                            |
|  1. å¯é‡ç¾æ€§                                               |
|     â†’ Pipeline + random_state + requirements.txt           |
|                                                            |
|  2. ç‰ˆæœ¬æ§åˆ¶                                               |
|     â†’ Gitï¼ˆç¨‹å¼ç¢¼ï¼‰+ DVCï¼ˆè³‡æ–™/æ¨¡å‹ï¼‰                      |
|                                                            |
|  3. ç›£æ§                                                   |
|     â†’ æ•ˆèƒ½ç›£æ§ + è³‡æ–™æ¼‚ç§»åµæ¸¬ + è­¦å ±                       |
|                                                            |
|  4. æ–‡ä»¶åŒ–                                                 |
|     â†’ Model Card + å…ƒè³‡æ–™ + å¯¦é©—æ—¥èªŒ                       |
|                                                            |
|  MLOps ç”Ÿå‘½é€±æœŸï¼š                                          |
|     è¨“ç·´ â†’ é©—è­‰ â†’ éƒ¨ç½² â†’ ç›£æ§ â†’ (é‡è¨“)                    |
|                                                            |
|  è¨˜ä½ï¼šç”Ÿç”¢ç’°å¢ƒä¸­çš„æ¨¡å‹éœ€è¦æŒçºŒç…§é¡§ï¼Œ                       |
|        å°±åƒèŠ±åœ’éœ€è¦å®šæœŸæ¾†æ°´å’Œä¿®å‰ªã€‚                         |
+----------------------------------------------------------+
```

---

## ğŸ“ èª²å¾Œç·´ç¿’

### ç·´ç¿’ 1ï¼šå»ºç«‹å®Œæ•´çš„ ML Pipeline

ä½¿ç”¨ sklearn çš„ä»»ä½•åˆ†é¡è³‡æ–™é›†ï¼š
1. å»ºç«‹åŒ…å«å‰è™•ç†å’Œæ¨¡å‹çš„ Pipeline
2. ä½¿ç”¨ cross_val_score è©•ä¼°
3. ç”¨ joblib å„²å­˜ Pipeline å’Œå…ƒè³‡æ–™
4. è¼‰å…¥ Pipeline ä¸¦é©—è­‰é æ¸¬çµæœä¸€è‡´

### ç·´ç¿’ 2ï¼šå¯¦é©—è¿½è¹¤

ä½¿ç”¨æœ¬ç« çš„ `SimpleExperimentTracker`ï¼š
1. å°åŒä¸€å€‹å•é¡Œå˜—è©¦è‡³å°‘ 3 ç¨®ä¸åŒçš„æ¨¡å‹/è¶…åƒæ•¸
2. è¨˜éŒ„æ¯æ¬¡å˜—è©¦çš„åƒæ•¸å’ŒæŒ‡æ¨™
3. æ‰¾å‡ºæœ€ä½³çµ„åˆ
4. å›ç­”ï¼šã€Œå¦‚æœä¸‰å€‹æœˆå¾Œè¦é‡ç¾æœ€ä½³çµæœï¼Œéœ€è¦è¨˜éŒ„ä»€éº¼ï¼Ÿã€

### ç·´ç¿’ 3ï¼šæ¨¡å‹ç›£æ§æ¨¡æ“¬

1. è¨“ç·´ä¸€å€‹æ¨¡å‹ä¸¦è¨˜éŒ„åŸºæº–æŒ‡æ¨™
2. æ¨¡æ“¬è³‡æ–™æ¼‚ç§»ï¼ˆä¾‹å¦‚å°æ¸¬è©¦é›†åŠ å…¥é›œè¨Šæˆ–æ”¹è®Šåˆ†ä½ˆï¼‰
3. ä½¿ç”¨ `ModelMonitor` åµæ¸¬æ•ˆèƒ½ä¸‹é™
4. ä½¿ç”¨ `detect_data_drift` æ‰¾å‡ºå“ªäº›ç‰¹å¾µæ¼‚ç§»äº†
5. é‡æ–°è¨“ç·´æ¨¡å‹ä¸¦æ¯”è¼ƒæ•ˆæœ

### ç·´ç¿’ 4ï¼ˆé€²éšï¼‰ï¼šç«¯åˆ°ç«¯ MLOps

å»ºç«‹ä¸€å€‹å®Œæ•´çš„ ML å°ˆæ¡ˆï¼ŒåŒ…å«ï¼š
1. `data/` - è³‡æ–™ï¼ˆåŠ ä¸Š .dvc è¿½è¹¤ï¼‰
2. `src/train.py` - è¨“ç·´è…³æœ¬
3. `src/predict.py` - é æ¸¬è…³æœ¬
4. `src/monitor.py` - ç›£æ§è…³æœ¬
5. `models/` - æ¨¡å‹å’Œå…ƒè³‡æ–™
6. `experiments/` - å¯¦é©—æ—¥èªŒ
7. `docs/model_card.md` - æ¨¡å‹å¡ç‰‡

---

> ğŸ“Œ **æ­å–œä½ ï¼** ä½ å·²ç¶“èµ°å®Œäº†å¾ã€Œæº–ç¢ºç‡è¿·æ€ã€åˆ°ã€Œç”Ÿç”¢ç´šæ¨¡å‹æ²»ç†ã€çš„æ—…ç¨‹ã€‚
> è¨˜ä½ï¼šå¥½çš„è³‡æ–™ç§‘å­¸ä¸åªæ˜¯å»ºæ¨¡å‹ï¼Œè€Œæ˜¯å»ºç«‹ **å¯ä¿¡è³´çš„æ±ºç­–ç³»çµ±**ã€‚
