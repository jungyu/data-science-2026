# Chapter 8: Random Forest â€” ä¸‰å€‹è‡­çš®åŒ å‹éä¸€å€‹è«¸è‘›äº®

> **ç¬¬ 8 é€±ï½œRandom Forest & Ensemble Learning**

---

## ğŸ¯ æœ¬ç« ç›®æ¨™

è®€å®Œé€™ä¸€ç« ï¼Œä½ å°‡èƒ½å¤ ï¼š

1. ç†è§£ Ensemble Learningï¼ˆé›†æˆå­¸ç¿’ï¼‰çš„æ ¸å¿ƒæ€æƒ³
2. ææ‡‚ Baggingï¼ˆBootstrap Aggregatingï¼‰åœ¨åšä»€éº¼
3. ç”¨ `RandomForestClassifier` è¨“ç·´æ¨¡å‹
4. ç†è§£ Random Forest ç‚ºä»€éº¼æ¯”å–®æ£µæ±ºç­–æ¨¹å¼·
5. å­¸æœƒçœ‹ Feature Importanceï¼ˆç‰¹å¾µé‡è¦æ€§ï¼‰
6. èª¿æ•´ `n_estimators`ã€`max_features` ç­‰è¶…åƒæ•¸
7. ç†è§£ Out-of-Bag (OOB) Score çš„å¦™ç”¨
8. é«”æœƒç‚ºä»€éº¼ã€Œ80% çš„å¯¦å‹™å•é¡Œï¼ŒRandom Forest å°±å¤ ç”¨äº†ã€

---

## å¾ã€Œå•ä¸€å€‹äººã€åˆ°ã€Œå•ä¸€ç¾¤äººã€

ä¸Šä¸€ç« æˆ‘å€‘å­¸äº†æ±ºç­–æ¨¹ã€‚å®ƒå¾ˆå²å®³ï¼Œä½†æœ‰ä¸€å€‹è‡´å‘½çš„ç¼ºé»ï¼š**ä¸ç©©å®š**ã€‚
è³‡æ–™ç¨å¾®è®Šä¸€ä¸‹ï¼Œæ•´æ£µæ¨¹å¯èƒ½é•·å¾—å®Œå…¨ä¸ä¸€æ¨£ã€‚

é€™å°±åƒä½ å•ä¸€å€‹æœ‹å‹ï¼šã€Œé€™é–“é¤å»³å¥½åƒå—ï¼Ÿã€
ä»–å¯èƒ½å› ç‚ºé‚£å¤©å¿ƒæƒ…ä¸å¥½ï¼Œå°±èªªã€Œé›£åƒæ­»äº†ã€ã€‚

ä½†å¦‚æœä½ å• **100 å€‹æœ‹å‹**ï¼Œç„¶å¾Œçœ‹**å¤šæ•¸äººæ€éº¼èªª**å‘¢ï¼Ÿ
å°±ç®—æœ‰å¹¾å€‹äººåˆ¤æ–·å¤±æº–ï¼Œå¤šæ•¸æ±ºçš„çµæœé€šå¸¸é‚„æ˜¯å¯é çš„ã€‚

```
        å•ä¸€å€‹äºº                    å• 100 å€‹äºº

     ğŸ§‘ "é›£åƒï¼"              ğŸ§‘ğŸ§‘ğŸ§‘ğŸ§‘ğŸ§‘ğŸ§‘ğŸ§‘  "å¥½åƒï¼" (72äºº)
                               ğŸ§‘ğŸ§‘ğŸ§‘         "é›£åƒï¼" (28äºº)
     çµè«–ï¼šé›£åƒ
     ï¼ˆå¯èƒ½ä¸æº–ï¼‰               çµè«–ï¼šå¥½åƒï¼
                               ï¼ˆå¤šæ•¸æ±ºï¼Œæ¯”è¼ƒå¯é ï¼‰
```

**é€™å°±æ˜¯ Ensemble Learningï¼ˆé›†æˆå­¸ç¿’ï¼‰çš„æ ¸å¿ƒæ€æƒ³ï¼š**
> æŠŠå¾ˆå¤šå€‹ã€Œé‚„è¡Œã€çš„æ¨¡å‹çµ„åˆèµ·ä¾†ï¼Œå¾—åˆ°ä¸€å€‹ã€Œå¾ˆå¼·ã€çš„æ¨¡å‹ã€‚

---

## ğŸ’¡ é‡é»è§€å¿µï¼šEnsemble çš„ä¸‰å¤§æ™ºæ…§

```
+------------------------------------------------------------+
|  æ™ºæ…§ 1ï¼šå¤šæ¨£æ€§ (Diversity)                                |
|  â†’ æ¯æ£µæ¨¹çœ‹åˆ°çš„è³‡æ–™å’Œç‰¹å¾µä¸åŒï¼ŒçŠ¯çš„éŒ¯ä¹Ÿä¸åŒ               |
|                                                            |
|  æ™ºæ…§ 2ï¼šå¤šæ•¸æ±º (Majority Voting)                          |
|  â†’ å°‘æ•¸éŒ¯èª¤æœƒè¢«å¤šæ•¸æ­£ç¢ºçš„çµæœè“‹é                          |
|                                                            |
|  æ™ºæ…§ 3ï¼šç©©å®šæ€§ (Stability)                                |
|  â†’ å–®æ£µæ¨¹ä¸ç©©å®šï¼Œä½†ä¸€ç¾¤æ¨¹çš„ã€Œå¹³å‡æ„è¦‹ã€éå¸¸ç©©å®š           |
+------------------------------------------------------------+
```

---

## Baggingï¼šBootstrap Aggregating

Random Forest çš„åŸºç¤æŠ€è¡“å«åš **Bagging**ï¼Œå…¨åæ˜¯ Bootstrap Aggregatingã€‚
é€™å€‹åå­—è½èµ·ä¾†å¾ˆåš‡äººï¼Œä½†å…¶å¯¦å¾ˆç°¡å–®ï¼š

### Step 1ï¼šBootstrapï¼ˆè‡ªåŠ©æŠ½æ¨£ï¼‰

å¾åŸå§‹è³‡æ–™ä¸­**æœ‰æ”¾å›åœ°éš¨æ©ŸæŠ½æ¨£**ï¼Œè£½é€ å‡ºå¤šä»½ã€Œç¨æœ‰ä¸åŒã€çš„è¨“ç·´é›†ã€‚

```
åŸå§‹è³‡æ–™: [A, B, C, D, E, F, G, H, I, J]  (10 ç­†)

æŠ½æ¨£ 1:   [A, A, C, D, F, F, G, H, I, J]  â† æœ‰é‡è¤‡ï¼(A, F å‡ºç¾å…©æ¬¡ï¼ŒB, E æ²’è¢«é¸åˆ°)
æŠ½æ¨£ 2:   [B, C, C, D, E, G, G, H, I, I]  â† ä¸åŒçš„é‡è¤‡
æŠ½æ¨£ 3:   [A, B, D, D, E, F, H, H, J, J]  â† åˆä¸ä¸€æ¨£
...
```

æ¯æ¬¡æŠ½ N ç­†ï¼ˆè·ŸåŸå§‹è³‡æ–™ä¸€æ¨£å¤šï¼‰ï¼Œä½†å› ç‚ºæœ‰æ”¾å›ï¼Œ
å¤§ç´„ **63.2%** çš„åŸå§‹è³‡æ–™æœƒè¢«é¸ä¸­ï¼ˆæ•¸å­¸ä¸Šæ˜¯ 1 - 1/eï¼‰ï¼Œ
å‰©ä¸‹çš„ **36.8%** æ²’è¢«é¸åˆ°ï¼ˆé€™äº›å«åš Out-of-Bag è³‡æ–™ï¼Œç­‰ç­‰æœƒç”¨åˆ°ï¼‰ã€‚

### Step 2ï¼šç”¨æ¯ä»½è³‡æ–™å„è¨“ç·´ä¸€æ£µæ±ºç­–æ¨¹

```
æŠ½æ¨£ 1 â†’ è¨“ç·´ â†’ ğŸŒ² æ¨¹ 1
æŠ½æ¨£ 2 â†’ è¨“ç·´ â†’ ğŸŒ² æ¨¹ 2
æŠ½æ¨£ 3 â†’ è¨“ç·´ â†’ ğŸŒ² æ¨¹ 3
...
æŠ½æ¨£ N â†’ è¨“ç·´ â†’ ğŸŒ² æ¨¹ N
```

### Step 3ï¼šAggregatingï¼ˆèšåˆï¼‰â€” å¤šæ•¸æ±º

```
æ–°è³‡æ–™ x é€²ä¾†ï¼š

ğŸŒ² æ¨¹ 1: "æ˜¯è²“ï¼"
ğŸŒ² æ¨¹ 2: "æ˜¯ç‹—ï¼"
ğŸŒ² æ¨¹ 3: "æ˜¯è²“ï¼"        çµ±è¨ˆï¼šè²“=7ç¥¨, ç‹—=3ç¥¨
ğŸŒ² æ¨¹ 4: "æ˜¯è²“ï¼"
ğŸŒ² æ¨¹ 5: "æ˜¯è²“ï¼"        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
ğŸŒ² æ¨¹ 6: "æ˜¯ç‹—ï¼"        â•‘ æœ€çµ‚ç­”æ¡ˆï¼šè²“ï¼ â•‘
ğŸŒ² æ¨¹ 7: "æ˜¯è²“ï¼"        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸŒ² æ¨¹ 8: "æ˜¯è²“ï¼"
ğŸŒ² æ¨¹ 9: "æ˜¯ç‹—ï¼"
ğŸŒ² æ¨¹10: "æ˜¯è²“ï¼"
```

---

## Random Forest = Bagging + éš¨æ©Ÿç‰¹å¾µé¸æ“‡

Random Forest åœ¨ Bagging çš„åŸºç¤ä¸Šåˆå¤šåŠ äº†ä¸€æ‹›ï¼š

> **æ¯æ¬¡åˆ†è£‚æ™‚ï¼Œä¸æ˜¯è€ƒæ…®æ‰€æœ‰ç‰¹å¾µï¼Œè€Œæ˜¯éš¨æ©ŸæŠ½ä¸€éƒ¨åˆ†ç‰¹å¾µä¾†é¸ã€‚**

ç‚ºä»€éº¼ï¼Ÿå› ç‚ºå¦‚æœæœ‰ä¸€å€‹ç‰¹å¾µç‰¹åˆ¥å¼·ï¼ˆæ¯”å¦‚éµé”å°¼è™Ÿçš„ã€Œæ€§åˆ¥ã€ï¼‰ï¼Œ
æ¯æ£µæ¨¹éƒ½æœƒå…ˆç”¨é€™å€‹ç‰¹å¾µä¾†åˆ†è£‚ï¼Œçµæœæ‰€æœ‰æ¨¹éƒ½é•·å¾—å·®ä¸å¤š â€” **å¤±å»äº†å¤šæ¨£æ€§**ã€‚

éš¨æ©Ÿé¸ç‰¹å¾µå°±æ˜¯å¼·è¿«æ¯æ£µæ¨¹ã€Œå¾ä¸åŒè§’åº¦æ€è€ƒã€ï¼š

```
æ‰€æœ‰ç‰¹å¾µ: [èº«é«˜, é«”é‡, å¹´é½¡, æ”¶å…¥, å­¸æ­·, æ€§åˆ¥, è·æ¥­, å±…ä½åœ°]

ğŸŒ² æ¨¹ 1 çš„ç¬¬ä¸€æ¬¡åˆ†è£‚åªèƒ½å¾ [é«”é‡, æ”¶å…¥, æ€§åˆ¥] ä¸­é¸
ğŸŒ² æ¨¹ 2 çš„ç¬¬ä¸€æ¬¡åˆ†è£‚åªèƒ½å¾ [èº«é«˜, å¹´é½¡, è·æ¥­] ä¸­é¸
ğŸŒ² æ¨¹ 3 çš„ç¬¬ä¸€æ¬¡åˆ†è£‚åªèƒ½å¾ [å­¸æ­·, æ€§åˆ¥, å±…ä½åœ°] ä¸­é¸
...

é€™æ¨£æ¯æ£µæ¨¹å°±è¢«è¿«ç”¨ä¸åŒçš„ç‰¹å¾µçµ„åˆä¾†åšåˆ¤æ–·ï¼
```

---

## ç”¨ scikit-learn è¨“ç·´ Random Forest

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# è¼‰å…¥è³‡æ–™
iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# è¨“ç·´ Random Forestï¼ˆ100 æ£µæ¨¹ï¼‰
rf = RandomForestClassifier(
    n_estimators=100,      # 100 æ£µæ¨¹
    random_state=42
)
rf.fit(X_train, y_train)

# é æ¸¬
y_pred = rf.predict(X_test)
print(f"Random Forest æº–ç¢ºç‡: {accuracy_score(y_test, y_pred):.4f}")
```

å°±é€™éº¼ç°¡å–®ã€‚ä¸‰è¡Œæ ¸å¿ƒç¨‹å¼ç¢¼ï¼Œä½ å°±æ“æœ‰äº† 100 æ£µæ¨¹çµ„æˆçš„æ£®æ—ã€‚

---

## ğŸ§  å‹•å‹•è…¦

Random Forest è£¡çš„æ¯æ£µæ¨¹éƒ½æ˜¯ã€Œæ•…æ„è¨“ç·´å¾—ä¸å®Œç¾ã€çš„ã€‚
ç‚ºä»€éº¼ã€Œä¸å®Œç¾çš„æ¨¹ã€çµ„åˆèµ·ä¾†åè€Œæ¯”ã€Œå®Œç¾çš„æ¨¹ã€æ›´å¼·ï¼Ÿ

ï¼ˆæç¤ºï¼šæƒ³æƒ³ã€Œä¸€ç¾¤å„æœ‰å°ˆé•·ä½†ç•¥æœ‰ç¼ºé™·çš„å°ˆå®¶ã€vsã€Œä¸€ç¾¤ä¸€æ¨¡ä¸€æ¨£çš„å°ˆå®¶ã€ï¼‰

---

## å–®æ£µæ¨¹ vs. Random Forestï¼šç›´æ¥æ¯”è¼ƒ

```python
from sklearn.tree import DecisionTreeClassifier

# å–®æ£µæ±ºç­–æ¨¹
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)

# Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

print("=" * 50)
print(f"{'æ¨¡å‹':<20} {'è¨“ç·´æº–ç¢ºç‡':>12} {'æ¸¬è©¦æº–ç¢ºç‡':>12}")
print("=" * 50)
print(f"{'Decision Tree':<20} {dt.score(X_train, y_train):>12.4f} {dt.score(X_test, y_test):>12.4f}")
print(f"{'Random Forest':<20} {rf.score(X_train, y_train):>12.4f} {rf.score(X_test, y_test):>12.4f}")
print("=" * 50)
```

å…¸å‹çš„çµæœï¼š

```
==================================================
æ¨¡å‹                     è¨“ç·´æº–ç¢ºç‡     æ¸¬è©¦æº–ç¢ºç‡
==================================================
Decision Tree              1.0000       0.9556
Random Forest              1.0000       0.9778
==================================================
```

æ³¨æ„çœ‹æ¸¬è©¦æº–ç¢ºç‡ â€” Random Forest é€šå¸¸æ›´é«˜ã€æ›´ç©©å®šã€‚

```
ç©©å®šæ€§æ¯”è¼ƒï¼šè·‘ 10 æ¬¡ä¸åŒçš„ random_state

Decision Tree:  [0.93, 0.98, 0.91, 0.96, 0.93, 0.98, 0.91, 0.96, 0.93, 0.98]
                å¹³å‡ = 0.947, æ¨™æº–å·® = 0.028  â† æ³¢å‹•å¤§

Random Forest:  [0.96, 0.98, 0.96, 0.98, 0.96, 0.98, 0.96, 0.97, 0.96, 0.98]
                å¹³å‡ = 0.969, æ¨™æº–å·® = 0.010  â† ç©©å®šå¤šäº†ï¼
```

---

## Feature Importanceï¼šå“ªäº›ç‰¹å¾µæœ€é‡è¦ï¼Ÿ

Random Forest çš„ä¸€å€‹è¶…å¯¦ç”¨åŠŸèƒ½ï¼šå®ƒæœƒå‘Šè¨´ä½ **å“ªäº›ç‰¹å¾µå°é æ¸¬æœ€æœ‰å¹«åŠ©**ã€‚

```python
import numpy as np

# å–å¾—ç‰¹å¾µé‡è¦æ€§
importances = rf.feature_importances_
feature_names = iris.feature_names

# æ’åºä¸¦é¡¯ç¤º
indices = np.argsort(importances)[::-1]

print("ç‰¹å¾µé‡è¦æ€§æ’åï¼š")
print("-" * 50)
for rank, idx in enumerate(indices, 1):
    bar = 'â–ˆ' * int(importances[idx] * 40)
    print(f"  {rank}. {feature_names[idx]:<20} {bar} {importances[idx]:.4f}")
```

è¼¸å‡ºï¼š

```
ç‰¹å¾µé‡è¦æ€§æ’åï¼š
--------------------------------------------------
  1. petal length (cm)   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.4424
  2. petal width (cm)    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.4231
  3. sepal length (cm)   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                             0.0987
  4. sepal width (cm)    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                0.0358
```

ğŸ’¡ **Feature Importance æ˜¯ Random Forest æœ€å¯¦ç”¨çš„å‰¯ç”¢å“ä¹‹ä¸€ã€‚**
åœ¨çœŸå¯¦å°ˆæ¡ˆä¸­ï¼Œå®ƒèƒ½å¹«ä½ ï¼š
- æ‰¾å‡ºå“ªäº›ç‰¹å¾µå€¼å¾—æ·±å…¥ç ”ç©¶
- ç§»é™¤ä¸é‡è¦çš„ç‰¹å¾µï¼Œç°¡åŒ–æ¨¡å‹
- å‘éæŠ€è¡“äººå“¡è§£é‡‹æ¨¡å‹

---

## è¶…åƒæ•¸èª¿æ•´

### 1. `n_estimators` â€” æ£®æ—è£¡æœ‰å¹¾æ£µæ¨¹

```python
# æ¯”è¼ƒä¸åŒæ•¸é‡çš„æ¨¹
for n_trees in [1, 10, 50, 100, 200, 500]:
    rf = RandomForestClassifier(n_estimators=n_trees, random_state=42)
    rf.fit(X_train, y_train)
    acc = rf.score(X_test, y_test)
    bar = 'â–ˆ' * int(acc * 40)
    print(f"n_estimators={n_trees:>3}: {bar} {acc:.4f}")
```

```
n_estimators çš„æ•ˆæœï¼š

æº–ç¢ºç‡
  ^
  |           ___________________________
  |         /
  |       /
  |     /
  |   /
  | /
  +-----------------------------------------> n_estimators
  1    10    50   100   200   500   1000

  â†’ è¶Šå¤šè¶Šå¥½ï¼Œä½†åˆ°ä¸€å®šæ•¸é‡å¾Œæ”¶ç›Šéæ¸›
  â†’ é€šå¸¸ 100-500 å°±å¤ äº†
  â†’ æ›´å¤šçš„æ¨¹ = æ›´å¤šçš„è¨“ç·´æ™‚é–“
```

### 2. `max_features` â€” æ¯æ¬¡åˆ†è£‚è€ƒæ…®å¹¾å€‹ç‰¹å¾µ

```python
# max_features çš„é¸é …
# 'sqrt': sqrt(n_features) â€” åˆ†é¡å•é¡Œçš„æ¨è–¦å€¼
# 'log2': log2(n_features)
# None:   n_featuresï¼ˆè€ƒæ…®æ‰€æœ‰ç‰¹å¾µï¼Œå¤±å»éš¨æ©Ÿæ€§ï¼‰
# int:    æŒ‡å®šæ•¸é‡
# float:  æŒ‡å®šæ¯”ä¾‹

rf = RandomForestClassifier(
    n_estimators=100,
    max_features='sqrt',   # é è¨­å€¼ï¼Œæ¨è–¦
    random_state=42
)
```

```
max_features å°ç…§è¡¨ï¼š
+------------------+--------------------+------------------+
| å€¼               | æ•ˆæœ               | é©ç”¨å ´æ™¯         |
+------------------+--------------------+------------------+
| 'sqrt'           | é«˜å¤šæ¨£æ€§ï¼Œæ¨è–¦     | åˆ†é¡å•é¡Œï¼ˆé è¨­ï¼‰ |
| 'log2'           | æ›´é«˜å¤šæ¨£æ€§         | ç‰¹å¾µå¾ˆå¤šæ™‚       |
| None (å…¨éƒ¨)      | ä½å¤šæ¨£æ€§           | ç‰¹å¾µå¾ˆå°‘æ™‚       |
| 0.5 (50%)        | ä¸­ç­‰å¤šæ¨£æ€§         | è‡ªè¨‚             |
+------------------+--------------------+------------------+
```

### 3. å…¶ä»–å¸¸ç”¨è¶…åƒæ•¸

```python
rf = RandomForestClassifier(
    n_estimators=200,       # 200 æ£µæ¨¹
    max_features='sqrt',    # æ¯æ¬¡åˆ†è£‚è€ƒæ…® sqrt(n) å€‹ç‰¹å¾µ
    max_depth=10,           # é™åˆ¶æ¯æ£µæ¨¹çš„æ·±åº¦
    min_samples_leaf=5,     # è‘‰ç¯€é»æœ€å°‘ 5 å€‹æ¨£æœ¬
    n_jobs=-1,              # ç”¨æ‰€æœ‰ CPU æ ¸å¿ƒå¹³è¡Œè¨“ç·´ï¼
    random_state=42
)
```

ğŸ’¡ **`n_jobs=-1` æ˜¯ Random Forest çš„éš±è—æŠ€èƒ½ã€‚**
å› ç‚ºæ¯æ£µæ¨¹æ˜¯ç¨ç«‹è¨“ç·´çš„ï¼Œæ‰€ä»¥å¯ä»¥å®Œç¾åœ°å¹³è¡ŒåŒ–è™•ç†ã€‚
è¨­å®š `n_jobs=-1` æœƒç”¨ä½ é›»è…¦çš„æ‰€æœ‰ CPU æ ¸å¿ƒï¼Œå¤§å¹…åŠ é€Ÿè¨“ç·´ï¼

---

## Out-of-Bag (OOB) Scoreï¼šå…è²»çš„é©—è­‰é›†

é‚„è¨˜å¾— Bootstrap æŠ½æ¨£æ™‚ï¼Œå¤§ç´„ 36.8% çš„è³‡æ–™æ²’è¢«é¸åˆ°å—ï¼Ÿ
é€™äº›ã€Œæ²’è¢«é¸åˆ°çš„è³‡æ–™ã€å«åš **Out-of-Bag (OOB) æ¨£æœ¬**ã€‚

å¦™çš„æ˜¯ï¼š**æ¯æ£µæ¨¹éƒ½å¯ä»¥ç”¨å®ƒçš„ OOB æ¨£æœ¬ä¾†åšé©—è­‰ã€‚**

```
ğŸŒ² æ¨¹ 1: è¨“ç·´ç”¨ [A,C,D,F,G,H]    OOB: [B,E,I,J]  â†’ ç”¨ B,E,I,J é©—è­‰æ¨¹ 1
ğŸŒ² æ¨¹ 2: è¨“ç·´ç”¨ [B,C,E,G,H,I]    OOB: [A,D,F,J]  â†’ ç”¨ A,D,F,J é©—è­‰æ¨¹ 2
ğŸŒ² æ¨¹ 3: è¨“ç·´ç”¨ [A,B,D,F,H,J]    OOB: [C,E,G,I]  â†’ ç”¨ C,E,G,I é©—è­‰æ¨¹ 3

å°æ–¼æ¨£æœ¬ Aï¼šå®ƒæ˜¯ æ¨¹ 2 å’Œ æ¨¹ 3 ä»¥å¤–çš„ OOB
           â†’ ç”¨ æ¨¹ 2 å’Œæ¨¹ 3ï¼ˆæ²’è¦‹é A çš„æ¨¹ï¼‰ä¾†é æ¸¬ A
           â†’ é€™å°±ç­‰æ–¼æ˜¯ã€Œæ¸¬è©¦é›†ã€çš„æ•ˆæœï¼
```

```python
# å•Ÿç”¨ OOB Score
rf_oob = RandomForestClassifier(
    n_estimators=100,
    oob_score=True,    # å•Ÿç”¨ OOB è©•ä¼°
    random_state=42
)
rf_oob.fit(X_train, y_train)

print(f"OOB Score:   {rf_oob.oob_score_:.4f}")
print(f"æ¸¬è©¦é›†æº–ç¢ºç‡: {rf_oob.score(X_test, y_test):.4f}")
```

```
OOB Score çš„å¥½è™•ï¼š
+--------------------------------------+
| 1. ä¸éœ€è¦é¡å¤–åˆ‡é©—è­‰é›†                |
| 2. è‡ªå‹•è¨ˆç®—ï¼Œä¸æµªè²»ä»»ä½•è³‡æ–™          |
| 3. é€šå¸¸è·Ÿäº¤å‰é©—è­‰çš„çµæœå¾ˆæ¥è¿‘        |
| 4. è³‡æ–™é‡å°‘çš„æ™‚å€™ç‰¹åˆ¥æœ‰ç”¨            |
+--------------------------------------+
```

---

## âš ï¸ å¸¸è¦‹é™·é˜±

### é™·é˜± 1ï¼šRandom Forest ä¸æ˜¯è¬èƒ½çš„

é›–ç„¶ RF å¾ˆå¼·ï¼Œä½†å®ƒä¸æ“…é•·ï¼š
- **é«˜ç¶­åº¦ç¨€ç–è³‡æ–™**ï¼ˆå¦‚æ–‡æœ¬åˆ†é¡ï¼‰â€” ç”¨ SVM æˆ–æ·±åº¦å­¸ç¿’å¯èƒ½æ›´å¥½
- **éœ€è¦ç·šæ€§å¤–æ¨çš„å•é¡Œ** â€” æ¨¹æ¨¡å‹åªèƒ½é æ¸¬è¨“ç·´è³‡æ–™ç¯„åœå…§çš„å€¼
- **å³æ™‚é æ¸¬è¦æ±‚æ¥µé«˜**çš„å ´æ™¯ â€” 100 æ£µæ¨¹çš„é æ¸¬æ¯” 1 æ£µæ…¢ 100 å€

### é™·é˜± 2ï¼šFeature Importance å¯èƒ½æœƒèª¤å°ä½ 

```
æ³¨æ„ï¼Feature Importance çš„é™·é˜±ï¼š

å¦‚æœå…©å€‹ç‰¹å¾µé«˜åº¦ç›¸é—œï¼ˆä¾‹å¦‚ã€Œèº«é«˜(cm)ã€å’Œã€Œèº«é«˜(inch)ã€ï¼‰ï¼Œ
Random Forest æœƒæŠŠé‡è¦æ€§ã€Œåˆ†æ•£ã€åˆ°å…©å€‹ç‰¹å¾µä¸Šï¼Œ
è®“å…©å€‹éƒ½çœ‹èµ·ä¾†ã€Œä¸å¤ªé‡è¦ã€ã€‚

      å¯¦éš›é‡è¦æ€§          RF é¡¯ç¤ºçš„é‡è¦æ€§
    èº«é«˜(cm): 0.40        èº«é«˜(cm): 0.22
                          èº«é«˜(inch): 0.18
                          â† è¢«æ‹†æ•£äº†ï¼
```

è§£æ±ºæ–¹æ³•ï¼šä½¿ç”¨ **Permutation Importance** æˆ–å…ˆåšç‰¹å¾µç¯©é¸ã€‚

### é™·é˜± 3ï¼šn_estimators ä¸æ˜¯è¶Šå¤§è¶Šå¥½

æ›´å¤šçš„æ¨¹ = æ›´é•·çš„è¨“ç·´å’Œé æ¸¬æ™‚é–“ï¼Œä½†æ•ˆæœæå‡æœ‰é™ï¼š

```
n_estimators    æº–ç¢ºç‡    è¨“ç·´æ™‚é–“
     10         0.945     0.1s
     50         0.967     0.3s
    100         0.978     0.6s
    500         0.980     2.8s    â† æ•ˆæœå¹¾ä¹æ²’è®Š
   1000         0.980     5.5s    â† æµªè²»æ™‚é–“
```

---

## ğŸ§  å‹•å‹•è…¦

å¦‚æœä½ çš„ Random Forest æœ‰ 1000 æ£µæ¨¹ï¼Œæ¯æ£µæ¨¹çš„æº–ç¢ºç‡éƒ½æ˜¯ 60%ã€‚
é‚£æ•´å€‹æ£®æ—çš„æº–ç¢ºç‡æœƒæ˜¯å¤šå°‘ï¼Ÿæœƒæ¯” 60% é«˜é‚„æ˜¯ä½ï¼Ÿ

ï¼ˆæç¤ºï¼šæƒ³æƒ³é¸èˆ‰ä¸­ã€Œå¤šæ•¸æ±ºã€çš„çµ±è¨ˆå­¸ã€‚
å¦‚æœæ¯å€‹äººåšå°çš„æ©Ÿç‡ > 50%ï¼Œäººè¶Šå¤šï¼Œå¤šæ•¸æ±ºè¶Šæº–ã€‚
é€™å°±æ˜¯ **Condorcet é™ªå¯©åœ˜å®šç†**ã€‚ï¼‰

---

## ç‚ºä»€éº¼èªªã€Œ80% çš„å•é¡Œ RF å°±å¤ ç”¨äº†ã€

åœ¨è³‡æ–™ç§‘å­¸çš„å¯¦å‹™å·¥ä½œä¸­ï¼Œæœ‰ä¸€å¥å»£ç‚ºæµå‚³çš„è©±ï¼š

> **ã€Œ80% çš„å¯¦å‹™å•é¡Œï¼ŒRandom Forest å°±å¤ ç”¨äº†ã€‚ã€**

ç‚ºä»€éº¼ï¼Ÿ

```
+------------------------------------------------------------+
| åŸå›  1ï¼šå¹¾ä¹ä¸éœ€è¦è³‡æ–™é è™•ç†                               |
|   â†’ ä¸éœ€è¦ç‰¹å¾µç¸®æ”¾                                         |
|   â†’ å¯ä»¥è™•ç†ç¼ºå¤±å€¼ï¼ˆæŸäº›å¯¦ä½œä¸­ï¼‰                           |
|   â†’ å°ç•°å¸¸å€¼ä¸æ•æ„Ÿ                                         |
+------------------------------------------------------------+
| åŸå›  2ï¼šé è¨­åƒæ•¸å°±å¾ˆèƒ½æ‰“                                   |
|   â†’ ä¸åƒ SVM é‚£æ¨£å°è¶…åƒæ•¸æ¥µåº¦æ•æ„Ÿ                          |
|   â†’ éš¨ä¾¿è¨­å€‹ n_estimators=100 å°±æœ‰ä¸éŒ¯çš„æ•ˆæœ               |
+------------------------------------------------------------+
| åŸå›  3ï¼šä¸å®¹æ˜“ overfit                                     |
|   â†’ Bagging + éš¨æ©Ÿç‰¹å¾µé¸æ“‡ = å¤©ç„¶çš„æ­£å‰‡åŒ–                  |
|   â†’ æ¯”å–®æ£µæ±ºç­–æ¨¹ç©©å®šå¾—å¤š                                   |
+------------------------------------------------------------+
| åŸå›  4ï¼šè¨“ç·´é€Ÿåº¦åˆç†                                       |
|   â†’ å¯ä»¥å¹³è¡ŒåŒ–ï¼ˆn_jobs=-1ï¼‰                                |
|   â†’ æ¯” Gradient Boosting å¿«å¾—å¤š                            |
+------------------------------------------------------------+
| åŸå›  5ï¼šæä¾› Feature Importance                            |
|   â†’ å¯ä»¥ç†è§£æ¨¡å‹åœ¨çœ‹ä»€éº¼                                   |
|   â†’ å¯ä»¥ç”¨ä¾†åšç‰¹å¾µç¯©é¸                                     |
+------------------------------------------------------------+
```

ç•¶ç„¶ï¼Œå¦å¤– 20% çš„å•é¡Œï¼ˆä¾‹å¦‚å½±åƒè¾¨è­˜ã€NLPã€è¶…å¤§è¦æ¨¡è³‡æ–™ï¼‰
å¯èƒ½éœ€è¦ XGBoostã€LightGBM æˆ–æ·±åº¦å­¸ç¿’ã€‚

ä½†åœ¨å¾ˆå¤š Kaggle ç«¶è³½å’Œä¼æ¥­å°ˆæ¡ˆä¸­ï¼Œ
**Random Forest æ˜¯æœ€å¥½çš„ã€Œç¬¬ä¸€å€‹å˜—è©¦ã€ã€‚**

---

## å¯¦æˆ°ï¼šå®Œæ•´çš„ Random Forest å·¥ä½œæµç¨‹

```python
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from sklearn.datasets import load_wine

# 1. è¼‰å…¥è³‡æ–™
wine = load_wine()
X, y = wine.data, wine.target
feature_names = wine.feature_names

# 2. åˆ‡åˆ†è³‡æ–™
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# 3. è¨“ç·´ Random Forest
rf = RandomForestClassifier(
    n_estimators=200,
    max_features='sqrt',
    max_depth=None,         # ä¸é™åˆ¶æ·±åº¦ï¼ˆRF ä¸å¤ªå®¹æ˜“ overfitï¼‰
    min_samples_leaf=2,
    oob_score=True,
    n_jobs=-1,
    random_state=42
)
rf.fit(X_train, y_train)

# 4. è©•ä¼°
y_pred = rf.predict(X_test)

print("=== Random Forest è©•ä¼°å ±å‘Š ===\n")
print(f"OOB Score:      {rf.oob_score_:.4f}")
print(f"æ¸¬è©¦é›†æº–ç¢ºç‡:   {accuracy_score(y_test, y_pred):.4f}\n")
print(classification_report(
    y_test, y_pred,
    target_names=wine.target_names
))

# 5. ç‰¹å¾µé‡è¦æ€§
print("\n=== ç‰¹å¾µé‡è¦æ€§ Top 5 ===\n")
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1][:5]

for rank, idx in enumerate(indices, 1):
    bar = 'â–ˆ' * int(importances[idx] * 50)
    print(f"  {rank}. {feature_names[idx]:<25} {bar} {importances[idx]:.4f}")
```

---

## â“ æ²’æœ‰ç¬¨å•é¡Œ

**Q: Random Forest å¯ä»¥åšè¿´æ­¸å—ï¼Ÿ**
A: å¯ä»¥ï¼ç”¨ `RandomForestRegressor`ã€‚åˆ†é¡æ™‚ç”¨å¤šæ•¸æ±ºï¼Œè¿´æ­¸æ™‚ç”¨**å¹³å‡å€¼**ä¾†èšåˆã€‚

**Q: Random Forest è·Ÿ Gradient Boosting æœ‰ä»€éº¼ä¸åŒï¼Ÿ**
A: Random Forest çš„æ¯æ£µæ¨¹æ˜¯**ç¨ç«‹è¨“ç·´**çš„ï¼ˆå¯ä»¥å¹³è¡ŒåŒ–ï¼‰ï¼Œ
Gradient Boosting çš„æ¯æ£µæ¨¹æ˜¯**æ¥åŠ›è¨“ç·´**çš„ï¼ˆå¾Œé¢çš„æ¨¹ä¿®æ­£å‰é¢çš„éŒ¯èª¤ï¼‰ã€‚
Gradient Boosting é€šå¸¸æ›´æº–ï¼Œä½†æ›´æ…¢ã€æ›´å®¹æ˜“ overfitã€æ›´é›£èª¿åƒã€‚

**Q: è¦æ€éº¼æ±ºå®š n_estimatorsï¼Ÿ**
A: å¾ 100 é–‹å§‹ï¼Œå¦‚æœæ™‚é–“å…è¨±å°±åŠ åˆ° 500ã€‚
ç”¨ OOB Score æˆ–äº¤å‰é©—è­‰ä¾†ç¢ºèªå¢åŠ æ¨¹çš„æ•¸é‡æ˜¯å¦é‚„æœ‰å¹«åŠ©ã€‚
é€šå¸¸è¶…é 500 ä¹‹å¾Œæ”¹å–„å°±å¾ˆå°äº†ã€‚

**Q: Random Forest éœ€è¦åšç‰¹å¾µç¸®æ”¾å—ï¼Ÿ**
A: ä¸éœ€è¦ï¼è·Ÿæ±ºç­–æ¨¹ä¸€æ¨£ï¼ŒRandom Forest åŸºæ–¼åˆ†è£‚æ“ä½œï¼Œ
ä¸æœƒå—åˆ°ç‰¹å¾µå°ºåº¦çš„å½±éŸ¿ã€‚

**Q: Feature Importance æ˜¯æ€éº¼è¨ˆç®—çš„ï¼Ÿ**
A: é è¨­çš„æ–¹å¼ï¼ˆMean Decrease in Impurityï¼‰æ˜¯çœ‹æ¯å€‹ç‰¹å¾µåœ¨æ‰€æœ‰æ¨¹ä¸­
ã€Œé™ä½äº†å¤šå°‘ Gini Impurityã€çš„å¹³å‡å€¼ã€‚ä½†é€™å€‹æ–¹æ³•åå¥½é«˜åŸºæ•¸çš„ç‰¹å¾µï¼Œ
æ›´ç©©å¥çš„æ–¹å¼æ˜¯ç”¨ `sklearn.inspection.permutation_importance`ã€‚

**Q: ä»€éº¼æ™‚å€™ä¸è©²ç”¨ Random Forestï¼Ÿ**
A: (1) éœ€è¦å³æ™‚é æ¸¬ã€æ¨¡å‹å¿…é ˆéå¸¸å°çš„æ™‚å€™ï¼ˆåµŒå…¥å¼è¨­å‚™ï¼‰
(2) è³‡æ–™æœ‰å¾ˆå¼·çš„ç·šæ€§çµæ§‹æ™‚ï¼ˆLogistic Regression å¯èƒ½æ›´å¥½ï¼‰
(3) éœ€è¦å¤–æ¨èƒ½åŠ›æ™‚ï¼ˆæ¨¹æ¨¡å‹ç„¡æ³•é æ¸¬è¶…å‡ºè¨“ç·´ç¯„åœçš„å€¼ï¼‰

---

## ğŸ“ èª²å¾Œç·´ç¿’

### ç·´ç¿’ 1ï¼šBagging çš„å¨åŠ›

```python
from sklearn.datasets import make_moons

# ç”¢ç”Ÿæœˆäº®å½¢è³‡æ–™
X, y = make_moons(n_samples=500, noise=0.3, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# æ¯”è¼ƒï¼š
# 1. å–®æ£µ DecisionTreeClassifierï¼ˆä¸è¨­é™ï¼‰
# 2. RandomForestClassifierï¼ˆn_estimators=10ï¼‰
# 3. RandomForestClassifierï¼ˆn_estimators=100ï¼‰
# 4. RandomForestClassifierï¼ˆn_estimators=500ï¼‰
#
# è¨˜éŒ„æ¯å€‹æ¨¡å‹çš„è¨“ç·´æº–ç¢ºç‡å’Œæ¸¬è©¦æº–ç¢ºç‡
# è§€å¯Ÿ Random Forest å¦‚ä½•éš¨è‘—æ¨¹çš„æ•¸é‡æå‡è¡¨ç¾
```

### ç·´ç¿’ 2ï¼šFeature Importance å¯¦æˆ°

```python
from sklearn.datasets import load_breast_cancer

# è¼‰å…¥ä¹³ç™Œè³‡æ–™é›†ï¼ˆ30 å€‹ç‰¹å¾µï¼‰
cancer = load_breast_cancer()

# 1. è¨“ç·´ä¸€å€‹ RandomForestClassifier
# 2. å°å‡º Feature Importance æ’å
# 3. åªä¿ç•™ Top 10 çš„ç‰¹å¾µï¼Œé‡æ–°è¨“ç·´
# 4. æ¯”è¼ƒç”¨å…¨éƒ¨ç‰¹å¾µ vs. Top 10 ç‰¹å¾µçš„æº–ç¢ºç‡
#    ï¼ˆä½ æœƒç™¼ç¾å·®ç•°å¾ˆå°ï¼Œä½†æ¨¡å‹ç°¡å–®å¤šäº†ï¼ï¼‰
```

### ç·´ç¿’ 3ï¼šOOB Score vs. æ¸¬è©¦é›†æº–ç¢ºç‡

```python
# è¨“ç·´ Random Forestï¼ˆè¨˜å¾—è¨­ oob_score=Trueï¼‰
# æ¯”è¼ƒ OOB Score å’Œæ¸¬è©¦é›†æº–ç¢ºç‡
# å®ƒå€‘æ¥è¿‘å—ï¼Ÿå¦‚æœä¸æ¥è¿‘ï¼Œå¯èƒ½çš„åŸå› æ˜¯ä»€éº¼ï¼Ÿ
```

---

## æœ¬ç« ç¸½çµ

```
+--------------------------------------------------+
|         Random Forest æ ¸å¿ƒè§€å¿µ                   |
+--------------------------------------------------+
| 1. é›†æˆå­¸ç¿’ = å¤šæ•¸æ±ºï¼Œã€Œç¾¤çœ¾æ™ºæ…§ã€               |
| 2. Bagging = æœ‰æ”¾å›æŠ½æ¨£ + ç¨ç«‹è¨“ç·´ + å¤šæ•¸æ±º     |
| 3. RF = Bagging + éš¨æ©Ÿç‰¹å¾µé¸æ“‡                   |
| 4. æ¯”å–®æ£µæ±ºç­–æ¨¹æ›´æº–ã€æ›´ç©©å®š                      |
| 5. Feature Importance æ˜¯å¯¦ç”¨çš„å‰¯ç”¢å“             |
| 6. OOB Score æ˜¯å…è²»çš„é©—è­‰æ©Ÿåˆ¶                    |
| 7. n_jobs=-1 å¯ä»¥å¹³è¡ŒåŠ é€Ÿè¨“ç·´                    |
| 8. 80% çš„å•é¡Œï¼ŒRF å°±å¤ ç”¨äº†ï¼                     |
+--------------------------------------------------+

    å–®æ£µæ±ºç­–æ¨¹          Random Forest

       ğŸŒ²                ğŸŒ²ğŸŒ²ğŸŒ²ğŸŒ²ğŸŒ²
                         ğŸŒ²ğŸŒ²ğŸŒ²ğŸŒ²ğŸŒ²
    ä¸ç©©å®š               ç©©å®šã€å¼·å¤§
    å®¹æ˜“ overfit         ä¸å®¹æ˜“ overfit
    é€Ÿåº¦å¿«               é€Ÿåº¦åˆç†ï¼ˆå¯å¹³è¡Œï¼‰
    å¯è§£é‡‹               ç‰¹å¾µé‡è¦æ€§

ä¸‹ä¸€ç« ï¼Œæˆ‘å€‘è¦å­¸ç¿’å¦‚ä½•**å…¬å¹³åœ°æ¯”è¼ƒ**ä¸åŒçš„æ¨¡å‹ï¼Œ
ç¢ºä¿ä½ é¸å‡ºçš„æ¨¡å‹ä¸æ˜¯å› ç‚ºã€Œé‹æ°£å¥½ã€æ‰è¡¨ç¾å¥½ â€”
äº¤å‰é©—è­‰èˆ‡æ¨¡å‹è©•ä¼°ï¼
```
