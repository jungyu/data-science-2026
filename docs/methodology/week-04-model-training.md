# ç¬¬ 4 é€±ï¼šæ¨¡å‹è¨“ç·´èˆ‡è³‡æ–™åˆ‡åˆ†

> æ¨¡å‹åœ¨è¨“ç·´é›†ä¸Šè¡¨ç¾å¥½ï¼Œå°±å¤ äº†å—ï¼Ÿ

---

## ğŸ¯ å­¸ç¿’æ ¸å¿ƒ

- ç‚ºä»€éº¼å¿…é ˆåˆ‡åˆ†è³‡æ–™
- éæ“¬åˆï¼ˆoverfittingï¼‰èˆ‡æ¬ æ“¬åˆï¼ˆunderfittingï¼‰
- äº¤å‰é©—è­‰ï¼ˆcross-validationï¼‰çš„åŸç†èˆ‡å¯¦ä½œ
- è³‡æ–™æ´©æ¼ï¼ˆdata leakageï¼‰çš„å±éšª

---

## ç‚ºä»€éº¼é€™ä»¶äº‹é‡è¦ï¼Ÿ

ä½ è¨“ç·´äº†ä¸€å€‹æ¨¡å‹ï¼Œåœ¨è¨“ç·´è³‡æ–™ä¸Šæº–ç¢ºç‡ 99%ã€‚ä½ èˆˆå¥®åœ°å‘ä¸»ç®¡å ±å‘Šã€‚

ä¸»ç®¡å•ï¼šã€Œä¸Šç·šå¾Œæ•ˆæœæ€æ¨£ï¼Ÿã€

ç­”æ¡ˆï¼šæº–ç¢ºç‡æ‰åˆ° 60%ã€‚

**é€™å°±æ˜¯éæ“¬åˆ** â€” æ¨¡å‹ã€ŒèƒŒã€äº†è¨“ç·´è³‡æ–™ï¼Œä½†æ²’æœ‰å­¸åˆ°çœŸæ­£çš„è¦å¾‹ã€‚å°±åƒä¸€å€‹å­¸ç”ŸèƒŒäº†æ‰€æœ‰è€ƒå¤é¡Œï¼Œä½†é‡åˆ°æ–°é¡Œç›®å°±ä¸æœƒåšã€‚

æ›´å¯æ€•çš„æ˜¯**è³‡æ–™æ´©æ¼** â€” ä½ çš„æ¨¡å‹åœ¨è¨“ç·´æ™‚ã€Œå·çœ‹ã€äº†å®ƒä¸æ‡‰è©²çŸ¥é“çš„è³‡è¨Šï¼Œå°è‡´è©•ä¼°çµæœè™›å‡åœ°å¥½ã€‚ä¸Šç·šå¾Œæ‰ç™¼ç¾ä¸€åˆ‡éƒ½æ˜¯å‡è±¡ã€‚

---

## æ ¸å¿ƒæ¦‚å¿µ

### 1ï¸âƒ£ Train / Validation / Test ä¸‰åˆ†æ³•

![è³‡æ–™åˆ‡åˆ†](https://framerusercontent.com/images/qHCBprH2QBdww2TTap3IKNBTkUM.png?height=886&width=1240)

```
å…¨éƒ¨è³‡æ–™
â”œâ”€â”€ Training Setï¼ˆ60-70%ï¼‰
â”‚   â””â”€â”€ ç”¨ä¾†è¨“ç·´æ¨¡å‹
â”œâ”€â”€ Validation Setï¼ˆ15-20%ï¼‰
â”‚   â””â”€â”€ ç”¨ä¾†èª¿æ•´è¶…åƒæ•¸ã€é¸æ“‡æ¨¡å‹
â””â”€â”€ Test Setï¼ˆ15-20%ï¼‰
    â””â”€â”€ æœ€å¾Œä¸€æ¬¡è©•ä¼°ï¼Œåªèƒ½ç”¨ä¸€æ¬¡
```

**é—œéµè¦å‰‡**ï¼š

| è³‡æ–™é›† | ç”¨é€” | ä½¿ç”¨é »ç‡ | æ³¨æ„äº‹é … |
|--------|------|---------|---------|
| Training | å­¸ç¿’æ¨¡å¼ | æ¯æ¬¡è¨“ç·´éƒ½ç”¨ | æ¨¡å‹æœƒã€Œçœ‹åˆ°ã€é€™äº›è³‡æ–™ |
| Validation | èª¿åƒæ•¸ã€é¸æ¨¡å‹ | åè¦†ä½¿ç”¨ | ä¸èƒ½è®“æ¨¡å‹ç›´æ¥å­¸ç¿’ |
| Test | æœ€çµ‚è©•ä¼° | **åªç”¨ä¸€æ¬¡** | ä»£è¡¨ã€Œæœªä¾†çš„çœŸå¯¦ä¸–ç•Œã€ |

> âš ï¸ **å¸¸è¦‹éŒ¯èª¤**ï¼šåè¦†ç”¨ Test Set ä¾†èª¿æ•´æ¨¡å‹ï¼Œç­‰æ–¼æŠŠ Test Set è®Šæˆäº†å¦ä¸€å€‹ Validation Setã€‚ä½ éœ€è¦ä¸€å€‹ã€ŒçœŸæ­£æ²’çœ‹éçš„ã€è³‡æ–™ä¾†åšæœ€çµ‚è©•ä¼°ã€‚

---

### 2ï¸âƒ£ Bias-Variance Tradeoff

![Bias-Variance](https://miro.medium.com/1*_7OPgojau8hkiPUiHoGK_w.png)

æ¯å€‹æ¨¡å‹çš„èª¤å·®å¯ä»¥åˆ†è§£ç‚ºä¸‰å€‹éƒ¨åˆ†ï¼š

```
Total Error = BiasÂ² + Variance + Irreducible Noise
```

| | é«˜ Bias | ä½ Bias |
|--|--------|---------|
| **é«˜ Variance** | æœ€ç³Ÿï¼šåˆååˆä¸ç©©å®š | éæ“¬åˆï¼šå¤ªè¤‡é›œ |
| **ä½ Variance** | æ¬ æ“¬åˆï¼šå¤ªç°¡å–® | æœ€ä½³ï¼šåˆæº–åˆç©©å®š |

**æ¨¡å‹å¤ªç°¡å–®ï¼ˆæ¬ æ“¬åˆï¼‰**ï¼š
- è¨“ç·´èª¤å·®é«˜ï¼Œæ¸¬è©¦èª¤å·®ä¹Ÿé«˜
- æ¨¡å‹æ²’æœ‰æ•æ‰åˆ°è³‡æ–™ä¸­çš„è¦å¾‹
- ä¾‹ï¼šç”¨ç·šæ€§æ¨¡å‹æ“¬åˆéç·šæ€§è³‡æ–™

**æ¨¡å‹å¤ªè¤‡é›œï¼ˆéæ“¬åˆï¼‰**ï¼š
- è¨“ç·´èª¤å·®ä½ï¼Œä½†æ¸¬è©¦èª¤å·®é«˜
- æ¨¡å‹è¨˜ä½äº†é›œè¨Šï¼ˆnoiseï¼‰ï¼Œè€Œä¸æ˜¯ä¿¡è™Ÿï¼ˆsignalï¼‰
- ä¾‹ï¼šç”¨ 100 æ¬¡å¤šé …å¼æ“¬åˆ 10 å€‹è³‡æ–™é»

```
æ¨¡å‹è¤‡é›œåº¦ â†’

    â†‘ èª¤å·®
    â”‚   â•²  è¨“ç·´èª¤å·®
    â”‚    â•²___________
    â”‚
    â”‚        ___________â•±  æ¸¬è©¦èª¤å·®
    â”‚       â•±
    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
          Sweet Spotï¼ˆæœ€ä½³è¤‡é›œåº¦ï¼‰
```

---

### 3ï¸âƒ£ äº¤å‰é©—è­‰ï¼ˆCross-Validationï¼‰

![K-Fold CV](https://www.researchgate.net/publication/332370436/figure/fig1/AS%3A746775958806528%401555056671117/Diagram-of-k-fold-cross-validation-with-k-10-Image-from-Karl-Rosaen-Log.ppm)

ç•¶è³‡æ–™é‡æœ‰é™æ™‚ï¼Œå–®ä¸€çš„ train/validation split å¯èƒ½ä¸å¤ ç©©å®šã€‚äº¤å‰é©—è­‰çš„åšæ³•ï¼š

**K-Fold Cross-Validation**ï¼š
1. å°‡è³‡æ–™åˆ†æˆ K ç­‰ä»½ï¼ˆé€šå¸¸ K = 5 æˆ– 10ï¼‰
2. æ¯æ¬¡ç”¨ K-1 ä»½è¨“ç·´ï¼Œ1 ä»½é©—è­‰
3. é‡è¤‡ K æ¬¡ï¼Œæ¯ä»½éƒ½ç•¶éé©—è­‰é›†
4. å– K æ¬¡çµæœçš„å¹³å‡

**å„ªé»**ï¼š
- æ¯ç­†è³‡æ–™éƒ½è¢«é©—è­‰éä¸€æ¬¡
- çµæœæ›´ç©©å®šã€ä¸ä¾è³´ç‰¹å®šçš„åˆ†å‰²æ–¹å¼
- å……åˆ†åˆ©ç”¨æœ‰é™çš„è³‡æ–™

**ç‰¹æ®Šè®Šé«”**ï¼š

| æ–¹æ³• | K å€¼ | é©ç”¨å ´æ™¯ |
|------|------|---------|
| 5-Fold | 5 | ä¸€èˆ¬ç”¨é€”ï¼ˆæœ€å¸¸ç”¨ï¼‰ |
| 10-Fold | 10 | è³‡æ–™é‡ä¸­ç­‰ï¼Œæƒ³æ›´ç©©å®š |
| LOOCV | n | è³‡æ–™é‡æ¥µå°‘ï¼ˆ<100ï¼‰ |
| Stratified K-Fold | ä»»æ„ | é¡åˆ¥ä¸å¹³è¡¡æ™‚ |
| Time Series Split | ä»»æ„ | æ™‚é–“åºåˆ—è³‡æ–™ |

---

### 4ï¸âƒ£ è³‡æ–™æ´©æ¼ï¼ˆData Leakageï¼‰

![Data Leakage](https://codingnomads.com/images/f8c470c9-c714-48e6-0397-7a9a4e57b500/public)

**è³‡æ–™æ´©æ¼**æ˜¯æŒ‡è¨“ç·´éç¨‹ä¸­ï¼Œæ¨¡å‹æ¥è§¸åˆ°äº†å®ƒåœ¨å¯¦éš›æ‡‰ç”¨ä¸­ä¸å¯èƒ½çŸ¥é“çš„è³‡è¨Šã€‚

**å¸¸è¦‹æ´©æ¼å ´æ™¯**ï¼š

| å ´æ™¯ | å•é¡Œ | è§£æ³• |
|------|------|------|
| å…ˆåšç‰¹å¾µå·¥ç¨‹å†åˆ‡è³‡æ–™ | æ¨™æº–åŒ–ç”¨äº†å…¨éƒ¨è³‡æ–™çš„å‡å€¼ | å…ˆåˆ‡è³‡æ–™å†åšç‰¹å¾µå·¥ç¨‹ |
| ç”¨æœªä¾†çš„è³‡æ–™é æ¸¬éå» | æ™‚é–“åºåˆ—ä¸­ç”¨äº†å¾ŒçºŒçš„è§€æ¸¬å€¼ | ç”¨æ™‚é–“åˆ‡åˆ†ï¼ˆTime Splitï¼‰ |
| ç‰¹å¾µåŒ…å«ç›®æ¨™çš„è¡ç”Ÿè³‡è¨Š | ä¾‹å¦‚ç”¨ã€Œé€€è²¨é‡‘é¡ã€é æ¸¬ã€Œæ˜¯å¦é€€è²¨ã€ | ä»”ç´°å¯©æŸ¥æ¯å€‹ç‰¹å¾µçš„å› æœæ–¹å‘ |
| é‡è¤‡çš„è³‡æ–™è·¨è¶Š train/test | åŒä¸€ç­†è³‡æ–™å‡ºç¾åœ¨å…©å€‹é›†åˆ | å»é‡å¾Œå†åˆ‡åˆ† |

> ğŸš¨ **è³‡æ–™æ´©æ¼æ˜¯æœ€éš±è”½çš„éŒ¯èª¤**ï¼Œå› ç‚ºä½ çš„æŒ‡æ¨™çœ‹èµ·ä¾†å¾ˆå¥½ï¼Œä½†ä¸Šç·šå¾Œæœƒå®Œå…¨å´©æ½°ã€‚

---

## ğŸ§ª å¯¦ä½œä»»å‹™

### ä»»å‹™ Aï¼šè¦ªçœ¼çœ‹è¦‹éæ“¬åˆ

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

np.random.seed(42)

# ç”¢ç”ŸçœŸå¯¦é—œä¿‚ï¼šy = sin(x) + noise
n = 30
X = np.sort(np.random.uniform(0, 2 * np.pi, n))
y = np.sin(X) + np.random.normal(0, 0.3, n)

X_test = np.linspace(0, 2 * np.pi, 200)
y_test_true = np.sin(X_test)

# å˜—è©¦ä¸åŒè¤‡é›œåº¦çš„å¤šé …å¼
degrees = [1, 3, 5, 15]
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

for ax, degree in zip(axes.flat, degrees):
    poly = PolynomialFeatures(degree)
    X_poly = poly.fit_transform(X.reshape(-1, 1))
    X_test_poly = poly.transform(X_test.reshape(-1, 1))

    model = LinearRegression()
    model.fit(X_poly, y)

    y_pred_train = model.predict(X_poly)
    y_pred_test = model.predict(X_test_poly)

    train_mse = mean_squared_error(y, y_pred_train)

    ax.scatter(X, y, color='blue', alpha=0.6, label='è¨“ç·´è³‡æ–™')
    ax.plot(X_test, y_test_true, 'g--', label='çœŸå¯¦å‡½æ•¸', alpha=0.7)
    ax.plot(X_test, y_pred_test, 'r-', label=f'æ¨¡å‹ (degree={degree})')
    ax.set_title(f'å¤šé …å¼ degree={degree}\nTrain MSE={train_mse:.4f}')
    ax.legend(fontsize=8)
    ax.set_ylim(-2, 2)

plt.tight_layout()
plt.show()
```

**è§€å¯Ÿ**ï¼š
- å“ªå€‹ degree çš„è¨“ç·´èª¤å·®æœ€ä½ï¼Ÿ
- å“ªå€‹ degree æœ€æ¥è¿‘çœŸå¯¦å‡½æ•¸ï¼Ÿ
- ç‚ºä»€éº¼è¨“ç·´èª¤å·®æœ€ä½çš„æ¨¡å‹ä¸ä¸€å®šæœ€å¥½ï¼Ÿ

### ä»»å‹™ Bï¼šäº¤å‰é©—è­‰å¯¦ä½œ

```python
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import make_pipeline

# ç”¨äº¤å‰é©—è­‰é¸æ“‡æœ€ä½³çš„å¤šé …å¼åº¦æ•¸
degrees = range(1, 16)
cv_means = []
cv_stds = []

for d in degrees:
    model = make_pipeline(
        PolynomialFeatures(d),
        LinearRegression()
    )
    scores = cross_val_score(model, X.reshape(-1, 1), y,
                             cv=5, scoring='neg_mean_squared_error')
    cv_means.append(-scores.mean())
    cv_stds.append(scores.std())

# ç•«å‡ºçµæœ
plt.figure(figsize=(10, 6))
plt.errorbar(degrees, cv_means, yerr=cv_stds, marker='o', capsize=5)
plt.xlabel('å¤šé …å¼åº¦æ•¸')
plt.ylabel('CV å¹³å‡ MSE')
plt.title('äº¤å‰é©—è­‰ï¼šé¸æ“‡æœ€ä½³æ¨¡å‹è¤‡é›œåº¦')
plt.axvline(x=degrees[np.argmin(cv_means)], color='r',
            linestyle='--', label=f'æœ€ä½³ degree = {degrees[np.argmin(cv_means)]}')
plt.legend()
plt.show()

print(f"äº¤å‰é©—è­‰å»ºè­°çš„æœ€ä½³åº¦æ•¸ï¼š{degrees[np.argmin(cv_means)]}")
```

### ä»»å‹™ Cï¼šè³‡æ–™æ´©æ¼åµæ¸¬

```python
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification

# ç”¢ç”Ÿåˆ†é¡è³‡æ–™
X_data, y_data = make_classification(n_samples=500, n_features=20,
                                      n_informative=5, random_state=42)

# âŒ éŒ¯èª¤åšæ³•ï¼šå…ˆæ¨™æº–åŒ–å†åˆ‡åˆ†ï¼ˆæ´©æ¼ï¼ï¼‰
scaler_wrong = StandardScaler()
X_scaled_wrong = scaler_wrong.fit_transform(X_data)  # ç”¨äº†å…¨éƒ¨è³‡æ–™çš„å‡å€¼/æ¨™æº–å·®
X_tr_w, X_te_w, y_tr_w, y_te_w = train_test_split(X_scaled_wrong, y_data,
                                                     test_size=0.2, random_state=42)
model_wrong = LogisticRegression(max_iter=1000)
model_wrong.fit(X_tr_w, y_tr_w)
score_wrong = model_wrong.score(X_te_w, y_te_w)

# âœ… æ­£ç¢ºåšæ³•ï¼šå…ˆåˆ‡åˆ†å†æ¨™æº–åŒ–
X_tr, X_te, y_tr, y_te = train_test_split(X_data, y_data,
                                            test_size=0.2, random_state=42)
scaler_right = StandardScaler()
X_tr_scaled = scaler_right.fit_transform(X_tr)  # åªç”¨è¨“ç·´é›†çš„å‡å€¼/æ¨™æº–å·®
X_te_scaled = scaler_right.transform(X_te)       # ç”¨è¨“ç·´é›†çš„åƒæ•¸è½‰æ›æ¸¬è©¦é›†
model_right = LogisticRegression(max_iter=1000)
model_right.fit(X_tr_scaled, y_tr)
score_right = model_right.score(X_te_scaled, y_te)

print(f"æ´©æ¼ç‰ˆæœ¬çš„æº–ç¢ºç‡ï¼š{score_wrong:.4f}")
print(f"æ­£ç¢ºç‰ˆæœ¬çš„æº–ç¢ºç‡ï¼š{score_right:.4f}")
print(f"å·®ç•°ï¼š{score_wrong - score_right:.4f}")
print("\nåœ¨é€™å€‹ä¾‹å­ä¸­å·®ç•°å¯èƒ½å¾ˆå°ï¼Œä½†åœ¨çœŸå¯¦å ´æ™¯ä¸­å¯èƒ½å·®å¾ˆå¤šã€‚")
print("é‡é»ä¸æ˜¯å·®å¤šå°‘ï¼Œè€Œæ˜¯é¤Šæˆæ­£ç¢ºçš„ç¿’æ…£ã€‚")
```

---

## ğŸ§  åæ€å•é¡Œ

1. **ç‚ºä»€éº¼æ¸¬è©¦é›†åªèƒ½ç”¨ä¸€æ¬¡ï¼Ÿ** å¦‚æœä½ åè¦†ç”¨æ¸¬è©¦é›†ä¾†é¸æ¨¡å‹ï¼Œæœƒç™¼ç”Ÿä»€éº¼ï¼Ÿ

2. **åœ¨æ™‚é–“åºåˆ—å•é¡Œä¸­ï¼Œç‚ºä»€éº¼ä¸èƒ½ç”¨éš¨æ©Ÿåˆ‡åˆ†ï¼Ÿ** èˆ‰ä¸€å€‹å…·é«”çš„ä¾‹å­èªªæ˜å¯èƒ½çš„æ´©æ¼ã€‚

3. **ã€Œæ¨¡å‹åœ¨è¨“ç·´é›†ä¸Šè¡¨ç¾å¾ˆå·®ã€ä¸€å®šæ˜¯å£äº‹å—ï¼Ÿ** ä»€éº¼æƒ…æ³ä¸‹é€™å¯èƒ½æ˜¯åˆç†çš„ï¼Ÿ

4. **ä½ åœ¨å¯¦éš›å·¥ä½œä¸­ï¼Œå¦‚ä½•æª¢æŸ¥æ˜¯å¦æœ‰è³‡æ–™æ´©æ¼ï¼Ÿ** åˆ—å‡º 3 å€‹å…·é«”çš„æª¢æŸ¥æ­¥é©Ÿã€‚

---

## å»¶ä¼¸é–±è®€

- [Scikit-learn: Cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html) â€” å®˜æ–¹æ–‡ä»¶
- Hastie, Tibshirani & Friedman, *The Elements of Statistical Learning* â€” ç¬¬ 7 ç« 
- [Kaggle: Data Leakage](https://www.kaggle.com/alexisbcook/data-leakage) â€” å¯¦å‹™æ¡ˆä¾‹

---

## æœ¬é€± Checklist

- [ ] å®Œæˆä»»å‹™ Aï¼šè¦–è¦ºåŒ–éæ“¬åˆ
- [ ] å®Œæˆä»»å‹™ Bï¼šäº¤å‰é©—è­‰é¸æ¨¡å‹
- [ ] å®Œæˆä»»å‹™ Cï¼šè³‡æ–™æ´©æ¼åµæ¸¬
- [ ] å›ç­”å…¨éƒ¨åæ€å•é¡Œ
- [ ] å°‡ç¨‹å¼ç¢¼èˆ‡ç­†è¨˜æ¨é€è‡³ GitHub

---

[â† ä¸Šä¸€é€±ï¼šå› æœæ¨è«–èˆ‡å¯¦é©—è¨­è¨ˆ](week-03-causal-inference.md) ï½œ [â†’ ä¸‹ä¸€é€±ï¼šæ¨¡å‹è©•ä¼°èˆ‡æŒ‡æ¨™é¸æ“‡](week-05-model-evaluation.md)
